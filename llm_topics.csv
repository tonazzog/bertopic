Topic,Count,Name,Representation,Representative_Docs
-1,5989,-1_and_the_of_to,"['and', 'the', 'of', 'to', 'in', 'we', 'for', 'llms', 'that', 'models']","['Fine-tuning text-to-image diffusion models with human feedback is an\neffective method for aligning model behavior with human intentions. However,\nthis alignment process often suffers from slow convergence due to the large\nsize and noise present in human feedback datasets. In this work, we propose\nFiFA, a novel automated data filtering algorithm designed to enhance the\nfine-tuning of diffusion models using human feedback datasets with direct\npreference optimization (DPO). Specifically, our approach selects data by\nsolving an optimization problem to maximize three components: preference\nmargin, text quality, and text diversity. The concept of preference margin is\nused to identify samples that are highly informative in addressing the noisy\nnature of feedback dataset, which is calculated using a proxy reward model.\nAdditionally, we incorporate text quality, assessed by large language models to\nprevent harmful contents, and consider text diversity through a k-nearest\nneighbor entropy estimator to improve generalization. Finally, we integrate all\nthese components into an optimization process, with approximating the solution\nby assigning importance score to each data pair and selecting the most\nimportant ones. As a result, our method efficiently filters data automatically,\nwithout the need for manual intervention, and can be applied to any large-scale\ndataset. Experimental results show that FiFA significantly enhances training\nstability and achieves better performance, being preferred by humans 17% more,\nwhile using less than 0.5% of the full data and thus 1% of the GPU hours\ncompared to utilizing full human feedback datasets.', 'This paper explores the advancements in making large language models (LLMs)\nmore human-like. We focus on techniques that enhance natural language\nunderstanding, conversational coherence, and emotional intelligence in AI\nsystems. The study evaluates various approaches, including fine-tuning with\ndiverse datasets, incorporating psychological principles, and designing models\nthat better mimic human reasoning patterns. Our findings demonstrate that these\nenhancements not only improve user interactions but also open new possibilities\nfor AI applications across different domains. Future work will address the\nethical implications and potential biases introduced by these human-like\nattributes.', 'With the rapid development of Large Language Models (LLMs), a large number of\nmachine learning models have been developed to assist programming tasks\nincluding the generation of program code from natural language input. However,\nhow to evaluate such LLMs for this task is still an open problem despite of the\ngreat amount of research efforts that have been made and reported to evaluate\nand compare them. This paper provides a critical review of the existing work on\nthe testing and evaluation of these tools with a focus on two key aspects: the\nbenchmarks and the metrics used in the evaluations. Based on the review,\nfurther research directions are discussed.']"
0,1108,0_visual_multimodal_image_mllms,"['visual', 'multimodal', 'image', 'mllms', 'images', 'vision', 'mllm', 'models', 'visionlanguage', 'text']","['Multimodal large language models (MLLMs) improve performance on\nvision-language tasks by integrating visual features from pre-trained vision\nencoders into large language models (LLMs). However, how MLLMs process and\nutilize visual information remains unclear. In this paper, a shift in the\ndominant flow of visual information is uncovered: (1) in shallow layers, strong\ninteractions are observed between image tokens and instruction tokens, where\nmost visual information is injected into instruction tokens to form cross-modal\nsemantic representations; (2) in deeper layers, image tokens primarily interact\nwith each other, aggregating the remaining visual information to optimize\nsemantic representations within visual modality. Based on these insights, we\npropose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference\nacceleration method that dynamically prunes image tokens at specific layers,\nreducing computational costs by approximately 65% without sacrificing\nperformance. Our findings offer a new understanding of visual information\nprocessing in MLLMs and provide a state-of-the-art solution for efficient\ninference.', 'Large Language Models (LLMs) have demonstrated impressive capabilities in\nanswering questions, but they lack domain-specific knowledge and are prone to\nhallucinations. Retrieval Augmented Generation (RAG) is one approach to address\nthese challenges, while multimodal models are emerging as promising AI\nassistants for processing both text and images. In this paper we describe a\nseries of experiments aimed at determining how to best integrate multimodal\nmodels into RAG systems for the industrial domain. The purpose of the\nexperiments is to determine whether including images alongside text from\ndocuments within the industrial domain increases RAG performance and to find\nthe optimal configuration for such a multimodal RAG system. Our experiments\ninclude two approaches for image processing and retrieval, as well as two LLMs\n(GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies\ninvolve the use of multimodal embeddings and the generation of textual\nsummaries from images. We evaluate our experiments with an LLM-as-a-Judge\napproach. Our results reveal that multimodal RAG can outperform single-modality\nRAG settings, although image retrieval poses a greater challenge than text\nretrieval. Additionally, leveraging textual summaries from images presents a\nmore promising approach compared to the use of multimodal embeddings, providing\nmore opportunities for future advancements.', 'With the advancement of large-scale language modeling techniques, large\nmultimodal models combining visual encoders with large language models have\ndemonstrated exceptional performance in various visual tasks. Most of the\ncurrent large-scale multimodal models achieve this by mapping visual features\nobtained from the visual encoder into a large language model and using them as\ninputs alongside text for downstream tasks. Therefore, the number of visual\ntokens directly affects the training and inference speed of the model. There\nhas been significant work on token pruning for visual transformers, but for\nlarge multimodal models, only relying on visual information for token pruning\nor compression may lead to significant loss of important information. On the\nother hand, the textual input in the form of a question may contain valuable\ninformation that can aid in answering the question, providing additional\nknowledge to the model. To address the potential oversimplification and\nexcessive pruning that can occur with most purely visual token pruning methods,\nwe propose a text information-guided dynamic visual token recovery mechanism\nthat does not require training. This mechanism leverages the similarity between\nthe question text and visual tokens to recover visually meaningful tokens with\nimportant text information while merging other less important tokens.\nExperimental results demonstrate that our proposed method achieves comparable\nperformance to the original approach while compressing the visual tokens to an\naverage of 10% of the original quantity. Our source code will be made publicly\navailable following acceptance.']"
1,695,1_reasoning_cot_mathematical_problems,"['reasoning', 'cot', 'mathematical', 'problems', 'logical', 'math', 'steps', 'chainofthought', 'llms', 'capabilities']","['Large Language Models (LLMs) have succeeded remarkably in various natural\nlanguage processing (NLP) tasks, yet their reasoning capabilities remain a\nfundamental challenge. While LLMs exhibit impressive fluency and factual\nrecall, their ability to perform complex reasoning-spanning logical deduction,\nmathematical problem-solving, commonsense inference, and multi-step\nreasoning-often falls short of human expectations. This survey provides a\ncomprehensive review of emerging techniques enhancing reasoning in LLMs. We\ncategorize existing methods into key approaches, including prompting strategies\n(e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought\nreasoning), architectural innovations (e.g., retrieval-augmented models,\nmodular reasoning networks, and neuro-symbolic integration), and learning\nparadigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement\nlearning, and self-supervised reasoning objectives). Additionally, we explore\nevaluation frameworks used to assess reasoning in LLMs and highlight open\nchallenges, such as hallucinations, robustness, and reasoning generalization\nacross diverse tasks. By synthesizing recent advancements, this survey aims to\nprovide insights into promising directions for future research and practical\napplications of reasoning-augmented LLMs.', 'Pretrained large language models (LLMs) are increasingly utilized across a\nwide range of natural language processing (NLP) tasks due to their impressive\ncapabilities as few-shot learners. Recent techniques, such as chain-of-thought\n(CoT) prompting, have significantly advanced multi-step reasoning by\nintroducing step-by-step decomposition, achieving state-of-the-art results on\ncomplex reasoning benchmarks. However, these approaches often rely on static\nprompting templates that do not adapt to task complexity or errors during the\nreasoning process. In this work, we introduce Adaptive Prompting, a dynamic and\niterative framework designed to enhance reasoning by incorporating real-time\nadjustments to prompt structures and validation mechanisms.Experimental results\ndemonstrate that Adaptive Prompting significantly improves performance on\ndiverse reasoning benchmarks, including arithmetic reasoning (GSM8K,\nMultiArith), logical reasoning and commonsense tasks, achieving substantial\naccuracy gains compared to static prompting baselines. By integrating guided\nprompts, intermediate validation, and self-corrective steps, our approach\nenables smaller models to achieve competitive performance with larger\ncounterparts, such as GPT-4, while maintaining computational efficiency. The\nframework achieves this without requiring fine-tuning or task-specific training\ndata, highlighting the untapped potential of iterative reasoning methods.', 'Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the ""overthinking phenomenon"". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.']"
2,614,2_medical_clinical_healthcare_patient,"['medical', 'clinical', 'healthcare', 'patient', 'biomedical', 'patients', 'notes', 'health', 'and', 'in']","[""Accurate and efficient question-answering systems are essential for\ndelivering high-quality patient care in the medical field. While Large Language\nModels (LLMs) have made remarkable strides across various domains, they\ncontinue to face significant challenges in medical question answering,\nparticularly in understanding domain-specific terminologies and performing\ncomplex reasoning. These limitations undermine their effectiveness in critical\nmedical applications. To address these issues, we propose a novel approach\nincorporating similar case generation within a multi-agent medical\nquestion-answering (MedQA) system. Specifically, we leverage the Llama3.1:70B\nmodel, a state-of-the-art LLM, in a multi-agent architecture to enhance\nperformance on the MedQA dataset using zero-shot learning. Our method\ncapitalizes on the model's inherent medical knowledge and reasoning\ncapabilities, eliminating the need for additional training data. Experimental\nresults show substantial performance gains over existing benchmark models, with\nimprovements of 7% in both accuracy and F1-score across various medical QA\ntasks. Furthermore, we examine the model's interpretability and reliability in\naddressing complex medical queries. This research not only offers a robust\nsolution for medical question answering but also establishes a foundation for\nbroader applications of LLMs in the medical domain."", 'With the proliferation of Large Language Models (LLMs) in diverse domains,\nthere is a particular need for unified evaluation standards in clinical medical\nscenarios, where models need to be examined very thoroughly. We present\nCliMedBench, a comprehensive benchmark with 14 expert-guided core clinical\nscenarios specifically designed to assess the medical ability of LLMs across 7\npivot dimensions. It comprises 33,735 questions derived from real-world medical\nreports of top-tier tertiary hospitals and authentic examination exercises. The\nreliability of this benchmark has been confirmed in several ways. Subsequent\nexperiments with existing LLMs have led to the following findings: (i) Chinese\nmedical LLMs underperform on this benchmark, especially where medical reasoning\nand factual consistency are vital, underscoring the need for advances in\nclinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs\ndemonstrate substantial potential in medical clinics, while the limited input\ncapacity of many medical LLMs hinders their practical use. These findings\nreveal both the strengths and limitations of LLMs in clinical scenarios and\noffer critical insights for medical research.', 'Since the inception of the Transformer architecture in 2017, Large Language\nModels (LLMs) such as GPT and BERT have evolved significantly, impacting\nvarious industries with their advanced capabilities in language understanding\nand generation. These models have shown potential to transform the medical\nfield, highlighting the necessity for specialized evaluation frameworks to\nensure their effective and ethical deployment. This comprehensive survey\ndelineates the extensive application and requisite evaluation of LLMs within\nhealthcare, emphasizing the critical need for empirical validation to fully\nexploit their capabilities in enhancing healthcare outcomes. Our survey is\nstructured to provide an in-depth analysis of LLM applications across clinical\nsettings, medical text data processing, research, education, and public health\nawareness. We begin by exploring the roles of LLMs in various medical\napplications, detailing their evaluation based on performance in tasks such as\nclinical diagnosis, medical text data processing, information retrieval, data\nanalysis, and educational content generation. The subsequent sections offer a\ncomprehensive discussion on the evaluation methods and metrics employed,\nincluding models, evaluators, and comparative experiments. We further examine\nthe benchmarks and datasets utilized in these evaluations, providing a\ncategorized description of benchmarks for tasks like question answering,\nsummarization, information extraction, bioinformatics, information retrieval\nand general comprehensive benchmarks. This structure ensures a thorough\nunderstanding of how LLMs are assessed for their effectiveness, accuracy,\nusability, and ethical alignment in the medical domain. ...']"
3,488,3_rag_retrieval_retrievalaugmented_documents,"['rag', 'retrieval', 'retrievalaugmented', 'documents', 'query', 'retrieved', 'generation', 'knowledge', 'queries', 'information']","['Retrieval-augmented generation (RAG) has emerged as a critical mechanism in\ncontemporary NLP to support Large Language Models(LLMs) in systematically\naccessing richer factual context. However, the integration of RAG mechanisms\nbrings its inherent challenges, as LLMs need to deal with potentially noisy\ncontexts. Recent studies have shown that LLMs still struggle to critically\nanalyse RAG-based in-context information, a limitation that may lead to\nincorrect inferences and hallucinations. In this paper, we investigate how to\nelicit critical reasoning in RAG via contrastive explanations. In particular,\nwe propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant\ndocuments given a query, (ii) selects and exemplifies relevant passages, and\n(iii) generates explanations that explicitly contrast the relevance of the\npassages to (iv) support the final answer. We show the impact of C-RAG building\ncontrastive reasoning demonstrations from LLMs to instruct smaller models for\nretrieval-augmented tasks. Extensive experiments demonstrate that C-RAG\nimproves state-of-the-art RAG models while (a) requiring significantly fewer\nprompts and demonstrations and (b) being robust to perturbations in the\nretrieved documents.', 'Retrieval-Augmented Generation (RAG) has recently demonstrated the\nperformance of Large Language Models (LLMs) in the knowledge-intensive tasks\nsuch as Question-Answering (QA). RAG expands the query context by incorporating\nexternal knowledge bases to enhance the response accuracy. However, it would be\ninefficient to access LLMs multiple times for each query and unreliable to\nretrieve all the relevant documents by a single query. We have found that even\nthough there is low relevance between some critical documents and query, it is\npossible to retrieve the remaining documents by combining parts of the\ndocuments with the query. To mine the relevance, a two-stage retrieval\nframework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is\nproposed to improve document retrieval recall and the accuracy of answers while\nmaintaining efficiency. Additionally, a compact classifier is applied to two\ndifferent selection strategies to determine the contribution of the retrieved\ndocuments to answering the query and retrieve the relatively relevant\ndocuments. Meanwhile, DR-RAG call the LLMs only once, which significantly\nimproves the efficiency of the experiment. The experimental results on\nmulti-hop QA datasets show that DR-RAG can significantly improve the accuracy\nof the answers and achieve new progress in QA systems.', 'Large Language Models (LLMs) are becoming essential tools for various natural\nlanguage processing tasks but often suffer from generating outdated or\nincorrect information. Retrieval-Augmented Generation (RAG) addresses this\nissue by incorporating external, real-time information retrieval to ground LLM\nresponses. However, the existing RAG systems frequently struggle with the\nquality of retrieval documents, as irrelevant or noisy documents degrade\nperformance, increase computational overhead, and undermine response\nreliability. To tackle this problem, we propose Multi-Agent Filtering\nRetrieval-Augmented Generation (MAIN-RAG), a training-free RAG framework that\nleverages multiple LLM agents to collaboratively filter and score retrieved\ndocuments. Specifically, MAIN-RAG introduces an adaptive filtering mechanism\nthat dynamically adjusts the relevance filtering threshold based on score\ndistributions, effectively minimizing noise while maintaining high recall of\nrelevant documents. The proposed approach leverages inter-agent consensus to\nensure robust document selection without requiring additional training data or\nfine-tuning. Experimental results across four QA benchmarks demonstrate that\nMAIN-RAG consistently outperforms traditional RAG approaches, achieving a 2-11%\nimprovement in answer accuracy while reducing the number of irrelevant\nretrieved documents. Quantitative analysis further reveals that our approach\nachieves superior response consistency and answer accuracy over baseline\nmethods, offering a competitive and practical alternative to training-based\nsolutions.']"
4,462,4_preference_reward_rlhf_alignment,"['preference', 'reward', 'rlhf', 'alignment', 'preferences', 'dpo', 'human', 'feedback', 'optimization', 'reinforcement']","[""Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal\ntool for aligning large language models (LLMs) with human preferences. Direct\nPreference Optimization (DPO), one of the most popular approaches, formulates\nRLHF as a policy optimization problem without explicitly estimating the reward\nfunction. It overcomes the stability and efficiency issues of two-step\napproaches, which typically involve first estimating the reward function and\nthen optimizing the policy via proximal policy optimization (PPO). Since RLHF\nis essentially an optimization problem, and it is well-known that momentum\ntechniques can accelerate optimization both theoretically and empirically, a\nnatural question arises: Can RLHF be accelerated by momentum? This paper\nanswers this question in the affirmative. In detail, we first show that the\niterative preference optimization method can be viewed as a proximal point\nmethod. Based on this observation, we propose a general Accelerated Preference\nOptimization (APO) framework, which unifies many existing preference\noptimization algorithms and employs Nesterov's momentum technique to speed up\nthe alignment of LLMs. Theoretically, we demonstrate that APO can achieve a\nfaster convergence rate than the standard iterative preference optimization\nmethods, including DPO and Self-Play Preference Optimization (SPPO).\nEmpirically, we show the superiority of APO over DPO, iterative DPO, and other\nstrong baselines for RLHF on the AlpacaEval 2.0 benchmark."", 'Reward inference (learning a reward model from human preferences) is a\ncritical intermediate step in the Reinforcement Learning from Human Feedback\n(RLHF) pipeline for fine-tuning Large Language Models (LLMs). In practice, RLHF\nfaces fundamental challenges such as distribution shift, reward model\noverfitting, and problem misspecification. An alternative approach is direct\npolicy optimization without reward inference, such as Direct Preference\nOptimization (DPO), which provides a much simpler pipeline and has shown\nempirical success in LLM applications. However, DPO utilizes the closed-form\nexpression between the optimal policy and the reward function, which is only\nsuitable under the bandit setting or deterministic MDPs. This paper develops\ntwo RLHF algorithms without reward inference for general RL problems beyond\nbandits and deterministic MDPs, and general preference models beyond the\nBradley-Terry model. The key idea is to estimate the local value function\ndifference from human preferences and then approximate the policy gradient with\na zeroth-order gradient approximator. For both algorithms, we establish\npolynomial convergence rates in terms of the number of policy gradient\niterations, the number of trajectory samples, and human preference queries per\niteration. Numerical experiments in stochastic environments validate the\nperformance of our proposed algorithms, outperforming popular RLHF baselines\nsuch as DPO and PPO. Our paper shows there exist provably efficient methods to\nsolve general RLHF problems without reward inference.', 'Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. While\nit enables LLMs to achieve human-level alignment, it often incurs significant\ncomputational and financial costs due to its reliance on training external\nreward models or human-labeled preferences. In this work, we propose Implicit\nPreference Optimization (IPO), an alternative approach that leverages\ngenerative LLMs as preference classifiers, thereby reducing the dependence on\nexternal human feedback or reward models to obtain preferences. We conduct a\ncomprehensive evaluation on the preference classification ability of LLMs using\nRewardBench, assessing models across different sizes, architectures, and\ntraining levels to validate our hypothesis. Furthermore, we investigate the\nself-improvement capabilities of LLMs by generating multiple responses for a\ngiven instruction and employing the model itself as a preference classifier for\nDirect Preference Optimization (DPO)-based training. Our findings demonstrate\nthat models trained through IPO achieve performance comparable to those\nutilizing state-of-the-art reward models for obtaining preferences.']"
5,403,5_robot_robots_planning_robotic,"['robot', 'robots', 'planning', 'robotic', 'navigation', 'embodied', 'task', 'environments', 'manipulation', 'environment']","[""Embodied robots which can interact with their environment and neighbours are\nincreasingly being used as a test case to develop Artificial Intelligence. This\ncreates a need for multimodal robot controllers that can operate across\ndifferent types of information, including text. Large Language Models are able\nto process and generate textual as well as audiovisual data and, more recently,\nrobot actions. Language Models are increasingly being applied to robotic\nsystems; these Language-Based robots leverage the power of language models in a\nvariety of ways. Additionally, the use of language opens up multiple forms of\ninformation exchange between members of a human-robot team. This survey\nmotivates the use of language models in robotics, and then delineates works\nbased on the part of the overall control flow in which language is\nincorporated. Language can be used by human to task a robot, by a robot to\ninform a human, between robots as a human-like communication medium, and\ninternally for a robot's planning and control. Applications of language-based\nrobots are explored, and numerous limitations and challenges are discussed to\nprovide a summary of the development needed for the future of language-based\nrobotics."", ""The integration of large language models (LLMs) with robotics has\nsignificantly advanced robots' abilities in perception, cognition, and task\nplanning. The use of natural language interfaces offers a unified approach for\nexpressing the capability differences of heterogeneous robots, facilitating\ncommunication between them, and enabling seamless task allocation and\ncollaboration. Currently, the utilization of LLMs to achieve decentralized\nmulti-heterogeneous robot collaborative tasks remains an under-explored area of\nresearch. In this paper, we introduce a novel framework that utilizes LLMs to\nachieve decentralized collaboration among multiple heterogeneous robots. Our\nframework supports three robot categories, mobile robots, manipulation robots,\nand mobile manipulation robots, working together to complete tasks such as\nexploration, transportation, and organization. We developed a rich set of\ntextual feedback mechanisms and chain-of-thought (CoT) prompts to enhance task\nplanning efficiency and overall system performance. The mobile manipulation\nrobot can adjust its base position flexibly, ensuring optimal conditions for\ngrasping tasks. The manipulation robot can comprehend task requirements, seek\nassistance when necessary, and handle objects appropriately. Meanwhile, the\nmobile robot can explore the environment extensively, map object locations, and\ncommunicate this information to the mobile manipulation robot, thus improving\ntask execution efficiency. We evaluated the framework using PyBullet, creating\nscenarios with three different room layouts and three distinct operational\ntasks. We tested various LLM models and conducted ablation studies to assess\nthe contributions of different modules. The experimental results confirm the\neffectiveness and necessity of our proposed framework."", 'In recent years, research in the area of human-robot interaction has focused\non developing robots capable of understanding complex human instructions and\nperforming tasks in dynamic and diverse environments. These systems have a wide\nrange of applications, from personal assistance to industrial robotics,\nemphasizing the importance of robots interacting flexibly, naturally and safely\nwith humans. This paper presents an advanced architecture for robotic action\nplanning that integrates communication, perception, and planning with Large\nLanguage Models (LLMs). Our system is designed to translate commands expressed\nin natural language into executable robot actions, incorporating environmental\ninformation and dynamically updating plans based on real-time feedback. The\nPlanner Module is the core of the system where LLMs embedded in a modified\nReAct framework are employed to interpret and carry out user commands. By\nleveraging their extensive pre-trained knowledge, LLMs can effectively process\nuser requests without the need to introduce new knowledge on the changing\nenvironment. The modified ReAct framework further enhances the execution space\nby providing real-time environmental perception and the outcomes of physical\nactions. By combining robust and dynamic semantic map representations as graphs\nwith control components and failure explanations, this architecture enhances a\nrobot adaptability, task execution, and seamless collaboration with human users\nin shared and dynamic environments. Through the integration of continuous\nfeedback loops with the environment the system can dynamically adjusts the plan\nto accommodate unexpected changes, optimizing the robot ability to perform\ntasks. Using a dataset of previous experience is possible to provide detailed\nfeedback about the failure. Updating the LLMs context of the next iteration\nwith suggestion on how to overcame the issue.']"
6,396,6_code_software_generation_programming,"['code', 'software', 'generation', 'programming', 'coding', 'python', 'developers', 'llms', 'completion', 'development']","['Large Language Models (LLMs), particularly Code LLMs, have demonstrated\nimpressive performance in code generation. Current research primarily focuses\non the correctness of generated code, while efficiency remains less explored.\nRecent works have focused on modifying the initial version of the code to\nimprove its efficiency. However, such refinements are limited by the\nalgorithmic design and overall logic of the initial code, resulting in only\nincremental improvements. In contrast, when human developers write high-quality\ncode, they typically begin by designing several potential solutions at the\nlogical level, evaluating various algorithms and their complexities, and then\nproceeding to implement and optimize the solution. In this study, we introduce\n\\tool: \\uline{L}arge \\uline{L}anguage \\uline{M}odel for Code\n\\uline{Effi}ciency, a novel framework that enables LLMs to generate code that\nbalances both efficiency and correctness. Specifically, \\tool divides the\nefficiency optimization process into two domains: algorithmic exploration in\nthe logic domain and implementation optimization in the code domain. The\ncorrectness of the code is then guaranteed through a synthetic test case\nrefinement process. This approach, which prioritizes efficiency before ensuring\ncorrectness, offers a new paradigm for efficient code generation. Experiments\ndemonstrate that \\tool consistently improves both efficiency and correctness,\nachieving new state-of-the-art performance in code efficiency benchmarks across\nvarious LLM backbones.', 'Context. Nowadays, 83% of software developers use Large Language Models\n(LLMs) to generate code. LLMs recently became essential to increase the\nproductivity of software developers and decrease the time and cost of software\ndevelopment. Developers ranging from novices to experts use LLM tools not only\nto detect and patch bugs, but also to integrate generated code into their\nsoftware. However, as of today there is no objective assessment of the energy\nefficiency of the source code generated by LLM tools. Released in August 2023,\nCode Llama is one of the most recent LLM tools.\n  Goal. In this paper, we present an empirical study that assesses the energy\nefficiency of Code Llama with respect to human-written source code.\n  Method. We design an experiment involving three human-written benchmarks\nimplemented in C++, JavaScript, and Python. We ask Code Llama to generate the\ncode of the benchmarks using different prompts and temperatures. Therefore, we\nexecute both implementations and profile their energy efficiency.\n  Results. Our study shows that the energy efficiency of code generated by Code\nLlama is heavily-dependent on the chosen programming language and the specific\ncode problem at hand. Also, human implementations tend to be more energy\nefficient overall, with generated JavaScript code outperforming its human\ncounterpart. Moreover, explicitly asking Code Llama to generate\nenergy-efficient code results in an equal or worse energy efficiency, as well\nas using different temperatures seems not to affect the energy efficiency of\ngenerated code.\n  Conclusions. According to our results, code generated using Code Llama does\nnot guarantee energy efficiency, even when prompted to do so. Therefore,\nsoftware developers should evaluate the energy efficiency of generated code\nbefore integrating it into the software system under development.', ""In this paper, we present a novel approach to improving software quality and\nefficiency through a Large Language Model (LLM)-based model designed to review\ncode and identify potential issues. Our proposed LLM-based AI agent model is\ntrained on large code repositories. This training includes code reviews, bug\nreports, and documentation of best practices. It aims to detect code smells,\nidentify potential bugs, provide suggestions for improvement, and optimize the\ncode. Unlike traditional static code analysis tools, our LLM-based AI agent has\nthe ability to predict future potential risks in the code. This supports a dual\ngoal of improving code quality and enhancing developer education by encouraging\na deeper understanding of best practices and efficient coding techniques.\nFurthermore, we explore the model's effectiveness in suggesting improvements\nthat significantly reduce post-release bugs and enhance code review processes,\nas evidenced by an analysis of developer sentiment toward LLM feedback. For\nfuture work, we aim to assess the accuracy and efficiency of LLM-generated\ndocumentation updates in comparison to manual methods. This will involve an\nempirical study focusing on manually conducted code reviews to identify code\nsmells and bugs, alongside an evaluation of best practice documentation,\naugmented by insights from developer discussions and code reviews. Our goal is\nto not only refine the accuracy of our LLM-based tool but also to underscore\nits potential in streamlining the software development lifecycle through\nproactive code improvement and education.""]"
7,376,7_bias_biases_gender_political,"['bias', 'biases', 'gender', 'political', 'social', 'fairness', 'demographic', 'stereotypes', 'llms', 'in']","['Large Language Models (LLMs) have been shown to exhibit various biases and\nstereotypes in their generated content. While extensive research has\ninvestigated bias in LLMs, prior work has predominantly focused on explicit\nbias, leaving the more nuanced implicit biases largely unexplored. This paper\npresents a systematic framework grounded in social psychology theories to\ninvestigate and compare explicit and implicit biases in LLMs. We propose a\nnovel ""self-reflection"" based evaluation framework that operates in two phases:\nfirst measuring implicit bias through simulated psychological assessment\nmethods, then evaluating explicit bias by prompting LLMs to analyze their own\ngenerated content. Through extensive experiments on state-of-the-art LLMs\nacross multiple social dimensions, we demonstrate that LLMs exhibit a\nsubstantial inconsistency between explicit and implicit biases, where explicit\nbiases manifest as mild stereotypes while implicit biases show strong\nstereotypes. Furthermore, we investigate the underlying factors contributing to\nthis explicit-implicit bias inconsistency. Our experiments examine the effects\nof training data scale, model parameters, and alignment techniques. Results\nindicate that while explicit bias diminishes with increased training data and\nmodel size, implicit bias exhibits a contrasting upward trend. Notably,\ncontemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit\nbias but show limited efficacy in mitigating implicit bias. These findings\nsuggest that while scaling up models and alignment training can address\nexplicit bias, the challenge of implicit bias requires novel approaches beyond\ncurrent methodologies.', ""Gender bias research has been pivotal in revealing undesirable behaviors in\nlarge language models, exposing serious gender stereotypes associated with\noccupations, and emotions. A key observation in prior work is that models\nreinforce stereotypes as a consequence of the gendered correlations that are\npresent in the training data. In this paper, we focus on bias where the effect\nfrom training data is unclear, and instead address the question: Do language\nmodels still exhibit gender bias in non-stereotypical settings? To do so, we\nintroduce UnStereoEval (USE), a novel framework tailored for investigating\ngender bias in stereotype-free scenarios. USE defines a sentence-level score\nbased on pretraining data statistics to determine if the sentence contain\nminimal word-gender associations. To systematically benchmark the fairness of\npopular language models in stereotype-free scenarios, we utilize USE to\nautomatically generate benchmarks without any gender-related language. By\nleveraging USE's sentence-level score, we also repurpose prior gender bias\nbenchmarks (Winobias and Winogender) for non-stereotypical evaluation.\nSurprisingly, we find low fairness across all 28 tested models. Concretely,\nmodels demonstrate fair behavior in only 9%-41% of stereotype-free sentences,\nsuggesting that bias does not solely stem from the presence of gender-related\nwords. These results raise important questions about where underlying model\nbiases come from and highlight the need for more systematic and comprehensive\nbias evaluation. We release the full dataset and code at\nhttps://ucinlp.github.io/unstereo-eval."", 'Recent advancements in Artificial Intelligence, particularly in Large\nLanguage Models (LLMs), have transformed natural language processing by\nimproving generative capabilities. However, detecting biases embedded within\nthese models remains a challenge. Subtle biases can propagate misinformation,\ninfluence decision-making, and reinforce stereotypes, raising ethical concerns.\nThis study presents a detection framework to identify nuanced biases in LLMs.\nThe approach integrates contextual analysis, interpretability via attention\nmechanisms, and counterfactual data augmentation to capture hidden biases\nacross linguistic contexts. The methodology employs contrastive prompts and\nsynthetic datasets to analyze model behaviour across cultural, ideological, and\ndemographic scenarios.\n  Quantitative analysis using benchmark datasets and qualitative assessments\nthrough expert reviews validate the effectiveness of the framework. Results\nshow improvements in detecting subtle biases compared to conventional methods,\nwhich often fail to highlight disparities in model responses to race, gender,\nand socio-political contexts. The framework also identifies biases arising from\nimbalances in training data and model architectures. Continuous user feedback\nensures adaptability and refinement. This research underscores the importance\nof proactive bias mitigation strategies and calls for collaboration between\npolicymakers, AI developers, and regulators. The proposed detection mechanisms\nenhance model transparency and support responsible LLM deployment in sensitive\napplications such as education, legal systems, and healthcare. Future work will\nfocus on real-time bias monitoring and cross-linguistic generalization to\nimprove fairness and inclusivity in AI-driven communication tools.']"
8,253,8_video_videos_temporal_frames,"['video', 'videos', 'temporal', 'frames', 'understanding', 'long', 'visual', 'videollms', 'frame', 'multimodal']","['Building on the advances of language models, Large Multimodal Models (LMMs)\nhave contributed significant improvements in video understanding. While the\ncurrent video LMMs utilize advanced Large Language Models (LLMs), they rely on\neither image or video encoders to process visual inputs, each of which has its\nown limitations. Image encoders excel at capturing rich spatial details from\nframe sequences but lack explicit temporal context, which can be important in\nvideos with intricate action sequences. On the other hand, video encoders\nprovide temporal context but are often limited by computational constraints\nthat lead to processing only sparse frames at lower resolutions, resulting in\nreduced contextual and spatial understanding. To this end, we introduce\nVideoGPT+, which combines the complementary benefits of the image encoder (for\ndetailed spatial understanding) and the video encoder (for global temporal\ncontext modeling). The model processes videos by dividing them into smaller\nsegments and applies an adaptive pooling strategy on features extracted by both\nimage and video encoders. Our architecture showcases improved performance\nacross multiple video benchmarks, including VCGBench, MVBench and Zero-shot\nquestion-answering. Further, we develop 112K video-instruction set using a\nnovel semi-automatic annotation pipeline which further improves the model\nperformance. Additionally, to comprehensively evaluate video LMMs, we present\nVCGBench-Diverse, covering 18 broad video categories such as lifestyle, sports,\nscience, gaming, and surveillance videos. This benchmark with 4,354\nquestion-answer pairs evaluates the generalization of existing LMMs on dense\nvideo captioning, spatial and temporal understanding, and complex reasoning,\nensuring comprehensive assessment across diverse video types and dynamics.\nCode: https://github.com/mbzuai-oryx/VideoGPT-plus.', ""Large language models (LLMs) have revolutionized video-based computer vision\napplications, including action recognition, anomaly detection, and video\nsummarization. Videos inherently pose unique challenges, combining spatial\ncomplexity with temporal dynamics that are absent in static images or textual\ndata. Current approaches to video understanding with LLMs often rely on\npretrained video encoders to extract spatiotemporal features and text encoders\nto capture semantic meaning. These representations are integrated within LLM\nframeworks, enabling multimodal reasoning across diverse video tasks. However,\nthe critical question persists: Can LLMs truly understand the concept of time,\nand how effectively can they reason about temporal relationships in videos?\nThis work critically examines the role of LLMs in video processing, with a\nspecific focus on their temporal reasoning capabilities. We identify key\nlimitations in the interaction between LLMs and pretrained encoders, revealing\ngaps in their ability to model long-term dependencies and abstract temporal\nconcepts such as causality and event progression. Furthermore, we analyze\nchallenges posed by existing video datasets, including biases, lack of temporal\nannotations, and domain-specific limitations that constrain the temporal\nunderstanding of LLMs. To address these gaps, we explore promising future\ndirections, including the co-evolution of LLMs and encoders, the development of\nenriched datasets with explicit temporal labels, and innovative architectures\nfor integrating spatial, temporal, and semantic reasoning. By addressing these\nchallenges, we aim to advance the temporal comprehension of LLMs, unlocking\ntheir full potential in video analysis and beyond. Our paper's GitHub\nrepository can be found at https://github.com/Darcyddx/Video-LLM."", 'Advancements in Large Language Models (LLMs) inspire various strategies for\nintegrating video modalities. A key approach is Video-LLMs, which incorporate\nan optimizable interface linking sophisticated video encoders to LLMs. However,\ndue to computation and data limitations, these Video-LLMs are typically\npre-trained to process only short videos, limiting their broader application\nfor understanding longer video content. Additionally, fine-tuning Video-LLMs to\nhandle longer videos is cost-prohibitive. Consequently, it becomes essential to\nexplore the interpolation of Video-LLMs under a completely training-free\nsetting. In this paper, we first identify the primary challenges in\ninterpolating Video-LLMs: (1) the video encoder and modality alignment\nprojector are fixed, preventing the integration of additional frames into\nVideo-LLMs, and (2) the LLM backbone is limited in its content length\ncapabilities, which complicates the processing of an increased number of video\ntokens. To address these challenges, we propose a specific INTerPolation method\nfor Video-LLMs (INTP-Video-LLMs). We introduce an alternative video token\nrearrangement technique that circumvents limitations imposed by the fixed video\nencoder and alignment projector. Furthermore, we introduce a training-free LLM\ncontext window extension method to enable Video-LLMs to understand a\ncorrespondingly increased number of visual tokens.']"
9,248,9_recommendation_recommender_user_recommendations,"['recommendation', 'recommender', 'user', 'recommendations', 'item', 'items', 'systems', 'collaborative', 'users', 'sequential']","['The explainability of recommendation systems is crucial for enhancing user\ntrust and satisfaction. Leveraging large language models (LLMs) offers new\nopportunities for comprehensive recommendation logic generation. However, in\nexisting related studies, fine-tuning LLM models for recommendation tasks\nincurs high computational costs and alignment issues with existing systems,\nlimiting the application potential of proven proprietary/closed-source LLM\nmodels, such as GPT-4. In this work, our proposed effective strategy LANE\naligns LLMs with online recommendation systems without additional LLMs tuning,\nreducing costs and improving explainability. This innovative approach addresses\nkey challenges in integrating language models with recommendation systems while\nfully utilizing the capabilities of powerful proprietary models. Specifically,\nour strategy operates through several key components: semantic embedding, user\nmulti-preference extraction using zero-shot prompting, semantic alignment, and\nexplainable recommendation generation using Chain of Thought (CoT) prompting.\nBy embedding item titles instead of IDs and utilizing multi-head attention\nmechanisms, our approach aligns the semantic features of user preferences with\nthose of candidate items, ensuring coherent and user-aligned recommendations.\nSufficient experimental results including performance comparison, questionnaire\nvoting, and visualization cases prove that our method can not only ensure\nrecommendation performance, but also provide easy-to-understand and reasonable\nrecommendation logic.', ""With the advent of the information explosion era, the importance of\nrecommendation systems in various applications is increasingly significant.\nTraditional collaborative filtering algorithms are widely used due to their\neffectiveness in capturing user behavior patterns, but they encounter\nlimitations when dealing with cold start problems and data sparsity. Large\nLanguage Models (LLMs), with their strong natural language understanding and\ngeneration capabilities, provide a new breakthrough for recommendation systems.\nThis study proposes an enhanced recommendation method that combines\ncollaborative filtering and LLMs, aiming to leverage collaborative filtering's\nadvantage in modeling user preferences while enhancing the understanding of\ntextual information about users and items through LLMs to improve\nrecommendation accuracy and diversity. This paper first introduces the\nfundamental theories of collaborative filtering and LLMs, then designs a\nrecommendation system architecture that integrates both, and validates the\nsystem's effectiveness through experiments. The results show that the hybrid\nmodel based on collaborative filtering and LLMs significantly improves\nprecision, recall, and user satisfaction, demonstrating its potential in\ncomplex recommendation scenarios."", ""Large Language Models (LLMs) have exhibited remarkable performance across a\nwide range of domains, motivating research into their potential for\nrecommendation systems. Early efforts have leveraged LLMs' rich knowledge and\nstrong generalization capabilities via in-context learning, where\nrecommendation tasks are framed as prompts. However, LLM performance in\nrecommendation scenarios remains limited due to the mismatch between their\npretraining objectives and recommendation tasks, as well as the lack of\nrecommendation-specific data during pretraining. To address these challenges,\nwe propose DPO4Rec, a novel framework that integrates Direct Preference\nOptimization (DPO) into LLM-enhanced recommendation systems. First, we prompt\nthe LLM to infer user preferences from historical interactions, which are then\nused to augment traditional ID-based sequential recommendation models. Next, we\ntrain a reward model based on knowledge-augmented recommendation architectures\nto assess the quality of LLM-generated reasoning. Using this, we select the\nhighest- and lowest-ranked responses from N samples to construct a dataset for\nLLM fine-tuning. Finally, we apply a structure alignment strategy via DPO to\nalign the LLM's outputs with desirable recommendation behavior. Extensive\nexperiments show that DPO4Rec significantly improves re-ranking performance\nover strong baselines, demonstrating enhanced instruction-following\ncapabilities of LLMs in recommendation tasks.""]"
10,240,10_driving_traffic_autonomous_transportation,"['driving', 'traffic', 'autonomous', 'transportation', 'urban', 'mobility', 'vehicles', 'scenarios', 'vehicle', 'trajectory']","[""Autonomous driving (AD) technology promises to revolutionize daily\ntransportation by making it safer, more efficient, and more comfortable. Their\nrole in reducing traffic accidents and improving mobility will be vital to the\nfuture of intelligent transportation systems. Autonomous driving in harsh\nenvironmental conditions presents significant challenges that demand robust and\nadaptive solutions and require more investigation. In this context, we present\nin this paper a comprehensive performance analysis of an autonomous driving\nagent leveraging the capabilities of a Multi-modal Large Language Model (MLLM)\nusing GPT-4o within the LimSim++ framework that offers close loop interaction\nwith the CARLA driving simulator. We call it MLLM-AD-4o. Our study evaluates\nthe agent's decision-making, perception, and control under adverse conditions,\nincluding bad weather, poor visibility, and complex traffic scenarios. Our\nresults demonstrate the AD agent's ability to maintain high levels of safety\nand efficiency, even in challenging environments, underscoring the potential of\nGPT-4o to enhance autonomous driving systems (ADS) in any environment\ncondition. Moreover, we evaluate the performance of MLLM-AD-4o when different\nperception entities are used including either front cameras only, front and\nrear cameras, and when combined with LiDAR. The results of this work provide\nvaluable insights into integrating MLLMs with AD frameworks, paving the way for\nfuture advancements in this field."", 'Autonomous driving, particularly navigating complex and unanticipated\nscenarios, demands sophisticated reasoning and planning capabilities. While\nMulti-modal Large Language Models (MLLMs) offer a promising avenue for this,\ntheir use has been largely confined to understanding complex environmental\ncontexts or generating high-level driving commands, with few studies extending\ntheir application to end-to-end path planning. A major research bottleneck is\nthe lack of large-scale annotated datasets encompassing vision, language, and\naction. To address this issue, we propose CoVLA (Comprehensive\nVision-Language-Action) Dataset, an extensive dataset comprising real-world\ndriving videos spanning more than 80 hours. This dataset leverages a novel,\nscalable approach based on automated data processing and a caption generation\npipeline to generate accurate driving trajectories paired with detailed natural\nlanguage descriptions of driving environments and maneuvers. This approach\nutilizes raw in-vehicle sensor data, allowing it to surpass existing datasets\nin scale and annotation richness. Using CoVLA, we investigate the driving\ncapabilities of MLLMs that can handle vision, language, and action in a variety\nof driving scenarios. Our results illustrate the strong proficiency of our\nmodel in generating coherent language and action outputs, emphasizing the\npotential of Vision-Language-Action (VLA) models in the field of autonomous\ndriving. This dataset establishes a framework for robust, interpretable, and\ndata-driven autonomous driving systems by providing a comprehensive platform\nfor training and evaluating VLA models, contributing to safer and more reliable\nself-driving vehicles. The dataset is released for academic purpose.', ""Multimodal large language models (MLLMs) hold the potential to enhance\nautonomous driving by combining domain-independent world knowledge with\ncontext-specific language guidance. Their integration into autonomous driving\nsystems shows promising results in isolated proof-of-concept applications,\nwhile their performance is evaluated on selective singular aspects of\nperception, reasoning, or planning. To leverage their full potential a\nsystematic framework for evaluating MLLMs in the context of autonomous driving\nis required. This paper proposes a holistic framework for a capability-driven\nevaluation of MLLMs in autonomous driving. The framework structures scenario\nunderstanding along the four core capability dimensions semantic, spatial,\ntemporal, and physical. They are derived from the general requirements of\nautonomous driving systems, human driver cognition, and language-based\nreasoning. It further organises the domain into context layers, processing\nmodalities, and downstream tasks such as language-based interaction and\ndecision-making. To illustrate the framework's applicability, two exemplary\ntraffic scenarios are analysed, grounding the proposed dimensions in realistic\ndriving situations. The framework provides a foundation for the structured\nevaluation of MLLMs' potential for scenario understanding in autonomous\ndriving.""]"
11,224,11_planning_agents_agent_web,"['planning', 'agents', 'agent', 'web', 'multiagent', 'tasks', 'environments', 'pddl', 'world', 'actions']","['The emergence of large language models (LLMs) has increasingly drawn\nattention to the use of LLMs for human-like planning. Existing work on\nLLM-based planning either focuses on leveraging the inherent language\ngeneration capabilities of LLMs to produce free-style plans, or employs\nreinforcement learning approaches to learn decision-making for a limited set of\nactions within restricted environments. However, both approaches exhibit\nsignificant discrepancies from the open and executable requirements in\nreal-world planning. In this paper, we propose a new planning task--open\ngrounded planning. The primary objective of open grounded planning is to ask\nthe model to generate an executable plan based on a variable action set,\nthereby ensuring the executability of the produced plan. To this end, we\nestablishes a benchmark for open grounded planning spanning a wide range of\ndomains. Then we test current state-of-the-art LLMs along with five planning\napproaches, revealing that existing LLMs and methods still struggle to address\nthe challenges posed by grounded planning in open domains. The outcomes of this\npaper define and establish a foundational dataset for open grounded planning,\nand shed light on the potential challenges and future directions of LLM-based\nplanning.', 'Recent studies have highlighted their proficiency in some simple tasks like\nwriting and coding through various reasoning strategies. However, LLM agents\nstill struggle with tasks that require comprehensive planning, a process that\nchallenges current models and remains a critical research issue. In this study,\nwe concentrate on travel planning, a Multi-Phases planning problem, that\ninvolves multiple interconnected stages, such as outlining, information\ngathering, and planning, often characterized by the need to manage various\nconstraints and uncertainties. Existing reasoning approaches have struggled to\neffectively address this complex task. Our research aims to address this\nchallenge by developing a human-like planning framework for LLM agents, i.e.,\nguiding the LLM agent to simulate various steps that humans take when solving\nMulti-Phases problems. Specifically, we implement several strategies to enable\nLLM agents to generate a coherent outline for each travel query, mirroring\nhuman planning patterns. Additionally, we integrate Strategy Block and\nKnowledge Block into our framework: Strategy Block facilitates information\ncollection, while Knowledge Block provides essential information for detailed\nplanning. Through our extensive experiments, we demonstrate that our framework\nsignificantly improves the planning capabilities of LLM agents, enabling them\nto tackle the travel planning task with improved efficiency and effectiveness.\nOur experimental results showcase the exceptional performance of the proposed\nframework; when combined with GPT-4-Turbo, it attains $10\\times$ the\nperformance gains in comparison to the baseline framework deployed on\nGPT-4-Turbo.', 'Autonomous planning has been an ongoing pursuit since the inception of\nartificial intelligence. Based on curated problem solvers, early planning\nagents could deliver precise solutions for specific tasks but lacked\ngeneralization. The emergence of large language models (LLMs) and their\npowerful reasoning capabilities has reignited interest in autonomous planning\nby automatically generating reasonable solutions for given tasks. However,\nprior research and our experiments show that current language agents still lack\nhuman-level planning abilities. Even the state-of-the-art reasoning model,\nOpenAI o1, achieves only 15.6% on one of the complex real-world planning\nbenchmarks. This highlights a critical question: What hinders language agents\nfrom achieving human-level planning? Although existing studies have highlighted\nweak performance in agent planning, the deeper underlying issues and the\nmechanisms and limitations of the strategies proposed to address them remain\ninsufficiently understood. In this work, we apply the feature attribution study\nand identify two key factors that hinder agent planning: the limited role of\nconstraints and the diminishing influence of questions. We also find that\nalthough current strategies help mitigate these challenges, they do not fully\nresolve them, indicating that agents still have a long way to go before\nreaching human-level intelligence.']"
12,214,12_kgs_knowledge_kg_graph,"['kgs', 'knowledge', 'kg', 'graph', 'graphs', 'ontology', 'reasoning', 'retrieval', 'ontologies', 'information']","[""This survey investigates the synergistic relationship between Large Language\nModels (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's\ncapabilities in understanding, reasoning, and language processing. It aims to\naddress gaps in current research by exploring areas such as KG Question\nAnswering, ontology generation, KG validation, and the enhancement of KG\naccuracy and consistency through LLMs. The paper further examines the roles of\nLLMs in generating descriptive texts and natural language queries for KGs.\nThrough a structured analysis that includes categorizing LLM-KG interactions,\nexamining methodologies, and investigating collaborative uses and potential\nbiases, this study seeks to provide new insights into the combined potential of\nLLMs and KGs. It highlights the importance of their interaction for improving\nAI applications and outlines future research directions."", 'Knowledge Graphs (KGs) are foundational structures in many AI applications,\nrepresenting entities and their interrelations through triples. However,\ntriple-based KGs lack the contextual information of relational knowledge, like\ntemporal dynamics and provenance details, which are crucial for comprehensive\nknowledge representation and effective reasoning. Instead, \\textbf{Context\nGraphs} (CGs) expand upon the conventional structure by incorporating\nadditional information such as time validity, geographic location, and source\nprovenance. This integration provides a more nuanced and accurate understanding\nof knowledge, enabling KGs to offer richer insights and support more\nsophisticated reasoning processes. In this work, we first discuss the inherent\nlimitations of triple-based KGs and introduce the concept of CGs, highlighting\ntheir advantages in knowledge representation and reasoning. We then present a\ncontext graph reasoning \\textbf{CGR$^3$} paradigm that leverages large language\nmodels (LLMs) to retrieve candidate entities and related contexts, rank them\nbased on the retrieved information, and reason whether sufficient information\nhas been obtained to answer a query. Our experimental results demonstrate that\nCGR$^3$ significantly improves performance on KG completion (KGC) and KG\nquestion answering (KGQA) tasks, validating the effectiveness of incorporating\ncontextual information on KG representation and reasoning.', 'Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation. However, they often struggle\nwith complex reasoning tasks and are prone to hallucination. Recent research\nhas shown promising results in leveraging knowledge graphs (KGs) to enhance LLM\nperformance. KGs provide a structured representation of entities and their\nrelationships, offering a rich source of information that can enhance the\nreasoning capabilities of LLMs. For this work, we have developed different\ntechniques that tightly integrate KG structures and semantics into LLM\nrepresentations. Our results show that we are able to significantly improve the\nperformance of LLMs in complex reasoning scenarios, and ground the reasoning\nprocess with KGs. We are the first to represent KGs with programming language\nand fine-tune pretrained LLMs with KGs. This integration facilitates more\naccurate and interpretable reasoning processes, paving the way for more\nadvanced reasoning capabilities of LLMs.']"
13,208,13_gpu_memory_serving_gpus,"['gpu', 'memory', 'serving', 'gpus', 'inference', 'parallelism', 'throughput', 'scheduling', 'hardware', 'requests']","['Large language models have been widely adopted across different tasks, but\ntheir auto-regressive generation nature often leads to inefficient resource\nutilization during inference. While batching is commonly used to increase\nthroughput, performance gains plateau beyond a certain batch size, especially\nwith smaller models, a phenomenon that existing literature typically explains\nas a shift to the compute-bound regime. In this paper, through an in-depth\nGPU-level analysis, we reveal that large-batch inference remains memory-bound,\nwith most GPU compute capabilities underutilized due to DRAM bandwidth\nsaturation as the primary bottleneck. To address this, we propose a Batching\nConfiguration Advisor (BCA) that optimizes memory allocation, reducing GPU\nmemory requirements with minimal impact on throughput. The freed memory and\nunderutilized GPU compute capabilities can then be leveraged by concurrent\nworkloads. Specifically, we use model replication to improve serving throughput\nand GPU utilization. Our findings challenge conventional assumptions about LLM\ninference, offering new insights and practical strategies for improving\nresource utilization, particularly for smaller language models.', 'Recent advancements in Large Language Models (LLMs) have led to increasingly\ndiverse requests, accompanied with varying resource (compute and memory)\ndemands to serve them. However, this in turn degrades the cost-efficiency of\nLLM serving as common practices primarily rely on homogeneous GPU resources. In\nresponse to this problem, this work conducts a thorough study about serving\nLLMs over heterogeneous GPU resources on cloud platforms. The rationale is that\ndifferent GPU types exhibit distinct compute and memory characteristics,\naligning well with the divergent resource demands of diverse requests.\nParticularly, through comprehensive benchmarking, we discover that the\ncost-efficiency of LLM serving can be substantially optimized by meticulously\ndetermining GPU composition, deployment configurations, and workload\nassignments. Subsequently, we design a scheduling algorithm via mixed-integer\nlinear programming, aiming at deducing the most cost-efficient serving plan\nunder the constraints of price budget and real-time GPU availability.\nRemarkably, our approach effectively outperforms homogeneous and heterogeneous\nbaselines under a wide array of scenarios, covering diverse workload traces,\nvarying GPU availablilities, and multi-model serving. This casts new light on\nmore accessible and efficient LLM serving over heterogeneous cloud resources.', 'Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.']"
14,208,14_languages_multilingual_english_lowresource,"['languages', 'multilingual', 'english', 'lowresource', 'crosslingual', 'language', 'pretraining', 'linguistic', 'indic', 'highresource']","[""Recently, Large Language Models (LLMs) have shown impressive language\ncapabilities. While most of the existing LLMs have very unbalanced performance\nacross different languages, multilingual alignment based on translation\nparallel data is an effective method to enhance the LLMs' multilingual\ncapabilities. In this work, we discover and comprehensively investigate the\nspontaneous multilingual alignment improvement of LLMs. We find that LLMs\ninstruction-tuned on the question translation data (i.e. without annotated\nanswers) are able to encourage the alignment between English and a wide range\nof languages, even including those unseen during instruction-tuning.\nAdditionally, we utilize different settings and mechanistic interpretability\nmethods to analyze the LLM's performance in the multilingual scenario\ncomprehensively. Our work suggests that LLMs have enormous potential for\nimproving multilingual alignment efficiently with great language and task\ngeneralization."", 'To democratize large language models (LLMs) to most natural languages, it is\nimperative to make these models capable of understanding and generating texts\nin many languages, in particular low-resource ones. While recent multilingual\nLLMs demonstrate remarkable performance in such capabilities, these LLMs still\nsupport a limited number of human languages due to the lack of training data\nfor low-resource languages. Moreover, these LLMs are not yet aligned with human\npreference for downstream tasks, which is crucial for the success of LLMs in\nEnglish. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively\nxLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100\nlanguages. To do so, we construct two datasets: a multilingual instruction\ndataset including 100 languages, which represents the largest language coverage\nto date, and a cross-lingual human feedback dataset encompassing 30 languages.\nWe perform multilingual instruction tuning on the constructed instruction data\nand further align the LLMs with human feedback using the DPO algorithm on our\ncross-lingual human feedback dataset. We evaluate the multilingual\nunderstanding and generating capabilities of xLLMs-100 on five multilingual\nbenchmarks. Experimental results show that xLLMs-100 consistently outperforms\nits peers across the benchmarks by considerable margins, defining a new\nstate-of-the-art multilingual LLM that supports 100 languages.', ""The development of Large Language Models (LLMs) relies on extensive text\ncorpora, which are often unevenly distributed across languages. This imbalance\nresults in LLMs performing significantly better on high-resource languages like\nEnglish, German, and French, while their capabilities in low-resource languages\nremain inadequate. Currently, there is a lack of quantitative methods to\nevaluate the performance of LLMs in these low-resource languages. To address\nthis gap, we propose the Language Ranker, an intrinsic metric designed to\nbenchmark and rank languages based on LLM performance using internal\nrepresentations. By comparing the LLM's internal representation of various\nlanguages against a baseline derived from English, we can assess the model's\nmultilingual capabilities in a robust and language-agnostic manner. Our\nanalysis reveals that high-resource languages exhibit higher similarity scores\nwith English, demonstrating superior performance, while low-resource languages\nshow lower similarity scores, underscoring the effectiveness of our metric in\nassessing language-specific capabilities. Besides, the experiments show that\nthere is a strong correlation between the LLM's performance in different\nlanguages and the proportion of those languages in its pre-training corpus.\nThese insights underscore the efficacy of the Language Ranker as a tool for\nevaluating LLM performance across different languages, particularly those with\nlimited resources.""]"
15,190,15_financial_stock_market_trading,"['financial', 'stock', 'market', 'trading', 'investment', 'finance', 'sentiment', 'news', 'price', 'analysis']","['Financial sentiment has become a crucial yet complex concept in finance,\nincreasingly used in market forecasting and investment strategies. Despite its\ngrowing importance, there remains a need to define and understand what\nfinancial sentiment truly represents and how it can be effectively measured. We\nexplore the nature of financial sentiment and investigate how large language\nmodels (LLMs) contribute to its estimation. We trace the evolution of sentiment\nmeasurement in finance, from market-based and lexicon-based methods to advanced\nnatural language processing techniques. The emergence of LLMs has significantly\nenhanced sentiment analysis, providing deeper contextual understanding and\ngreater accuracy in extracting sentiment from financial text. We examine how\nBERT-based models, such as RoBERTa and FinBERT, are optimized for structured\nsentiment classification, while GPT-based models, including GPT-4, OPT, and\nLLaMA, excel in financial text generation and real-time sentiment\ninterpretation. A comparative analysis of bidirectional and autoregressive\ntransformer architectures highlights their respective roles in investor\nsentiment analysis, algorithmic trading, and financial decision-making. By\nexploring what financial sentiment is and how it is estimated within LLMs, we\nprovide insights into the growing role of AI-driven sentiment analysis in\nfinance.', 'Large language models (LLMs) excel at generating human-like responses but\noften struggle with interactive tasks that require access to real-time\ninformation. This limitation poses challenges in finance, where models must\naccess up-to-date information, such as recent news or price movements, to\nsupport decision-making. To address this, we introduce Financial Agent, a\nknowledge-grounding approach for LLMs to handle financial queries using\nreal-time text and tabular data. Our contributions are threefold: First, we\ndevelop a Financial Context Dataset of over 50,000 financial queries paired\nwith the required context. Second, we train FinBloom 7B, a custom 7 billion\nparameter LLM, on 14 million financial news articles from Reuters and Deutsche\nPresse-Agentur, alongside 12 million Securities and Exchange Commission (SEC)\nfilings. Third, we fine-tune FinBloom 7B using the Financial Context Dataset to\nserve as a Financial Agent. This agent generates relevant financial context,\nenabling efficient real-time data retrieval to answer user queries. By reducing\nlatency and eliminating the need for users to manually provide accurate data,\nour approach significantly enhances the capability of LLMs to handle dynamic\nfinancial tasks. Our proposed approach makes real-time financial decisions,\nalgorithmic trading and other related tasks streamlined, and is valuable in\ncontexts with high-velocity data flows.', ""Financial analysis heavily relies on the evaluation of earnings reports to\ngain insights into company performance. Traditional generation of these reports\nrequires extensive financial expertise and is time-consuming. With the\nimpressive progress in Large Language Models (LLMs), a wide variety of\nfinancially focused LLMs has emerged, addressing tasks like sentiment analysis\nand entity recognition in the financial domain. This paper presents a novel\nchallenge: developing an LLM specifically for automating the generation of\nearnings reports analysis. Our methodology involves an in-depth analysis of\nexisting earnings reports followed by a unique approach to fine-tune an LLM for\nthis purpose. This approach combines retrieval augmentation and the generation\nof instruction-based data, specifically tailored for the financial sector, to\nenhance the LLM's performance. With extensive financial documents, we construct\nfinancial instruction data, enabling the refined adaptation of our LLM to\nfinancial contexts. Preliminary results indicate that our augmented LLM\noutperforms general open-source models and rivals commercial counterparts like\nGPT-3.5 in financial applications. Our research paves the way for streamlined\nand insightful automation in financial report generation, marking a significant\nstride in the field of financial analysis.""]"
16,177,16_jailbreak_attacks_attack_harmful,"['jailbreak', 'attacks', 'attack', 'harmful', 'jailbreaking', 'safety', 'prompts', 'defense', 'jailbreaks', 'adversarial']","['Jailbreak attacks on large language models (LLMs) involve inducing these\nmodels to generate harmful content that violates ethics or laws, posing a\nsignificant threat to LLM security. Current jailbreak attacks face two main\nchallenges: low success rates due to defensive measures and high resource\nrequirements for crafting specific prompts. This paper introduces Virtual\nContext, which leverages special tokens, previously overlooked in LLM security,\nto improve jailbreak attacks. Virtual Context addresses these challenges by\nsignificantly increasing the success rates of existing jailbreak methods and\nrequiring minimal background knowledge about the target model, thus enhancing\neffectiveness in black-box settings without additional overhead. Comprehensive\nevaluations show that Virtual Context-assisted jailbreak attacks can improve\nthe success rates of four widely used jailbreak methods by approximately 40%\nacross various LLMs. Additionally, applying Virtual Context to original\nmalicious behaviors still achieves a notable jailbreak effect. In summary, our\nresearch highlights the potential of special tokens in jailbreak attacks and\nrecommends including this threat in red-teaming testing to comprehensively\nenhance LLM security.', 'Jailbreak attacks in large language models (LLMs) entail inducing the models\nto generate content that breaches ethical and legal norm through the use of\nmalicious prompts, posing a substantial threat to LLM security. Current\nstrategies for jailbreak attack and defense often focus on optimizing locally\nwithin specific algorithmic frameworks, resulting in ineffective optimization\nand limited scalability. In this paper, we present a systematic analysis of the\ndependency relationships in jailbreak attack and defense techniques,\ngeneralizing them to all possible attack surfaces. We employ directed acyclic\ngraphs (DAGs) to position and analyze existing jailbreak attacks, defenses, and\nevaluation methodologies, and propose three comprehensive, automated, and\nlogical frameworks. \\texttt{AutoAttack} investigates dependencies in two lines\nof jailbreak optimization strategies: genetic algorithm (GA)-based attacks and\nadversarial-generation-based attacks, respectively. We then introduce an\nensemble jailbreak attack to exploit these dependencies. \\texttt{AutoDefense}\noffers a mixture-of-defenders approach by leveraging the dependency\nrelationships in pre-generative and post-generative defense strategies.\n\\texttt{AutoEvaluation} introduces a novel evaluation method that distinguishes\nhallucinations, which are often overlooked, from jailbreak attack and defense\nresponses. Through extensive experiments, we demonstrate that the proposed\nensemble jailbreak attack and defense framework significantly outperforms\nexisting research.', 'As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.']"
17,177,17_quantization_weights_ptq_lowbit,"['quantization', 'weights', 'ptq', 'lowbit', 'weight', 'quantized', 'precision', 'outliers', 'activations', 'posttraining']","['With the commercialization of large language models (LLMs), weight-activation\nquantization has emerged to compress and accelerate LLMs, achieving high\nthroughput while reducing inference costs. However, existing post-training\nquantization (PTQ) techniques for quantizing weights and activations of LLMs\nstill suffer from non-negligible accuracy drops, especially on massive\nmultitask language understanding. To address this issue, we propose Low-Rank\nQuantization (LRQ) - a simple yet effective post-training weight quantization\nmethod for LLMs that reconstructs the outputs of an intermediate Transformer\nblock by leveraging low-rank weight-scaling matrices, replacing the\nconventional full weight-scaling matrices that entail as many learnable scales\nas their associated weights. Thanks to parameter sharing via low-rank\nstructure, LRQ only needs to learn significantly fewer parameters while\nenabling the individual scaling of weights, thus boosting the generalization\ncapability of quantized LLMs. We show the superiority of LRQ over prior LLM PTQ\nworks under (i) 8-bit weight and per-tensor activation quantization, (ii) 4-bit\nweight and 8-bit per-token activation quantization, and (iii) low-bit\nweight-only quantization schemes. Our code is available at Software.', 'This paper presents a comprehensive analysis of quantization techniques for\noptimizing Large Language Models (LLMs), specifically focusing on Post-Training\nQuantization (PTQ) and Quantization-Aware Training (QAT). Through empirical\nevaluation across models ranging from 10M to 1B parameters, we demonstrate that\nquantization can achieve up to 68% reduction in model size while maintaining\nperformance within 6% of full-precision baselines when utilizing our proposed\nscaling factor {\\gamma}. Our experiments show that INT8 quantization delivers a\n40% reduction in computational cost and power consumption, while INT4\nquantization further improves these metrics by 60%. We introduce a novel\ntheoretical framework for mixed-precision quantization, deriving optimal bit\nallocation strategies based on layer sensitivity and weight variance. Hardware\nefficiency evaluations on edge devices reveal that our quantization approach\nenables up to 2.4x throughput improvement for INT8 and 3x for INT4, with 60%\npower reduction compared to full-precision models.', 'Large language models (LLMs) have wide applications in the field of natural\nlanguage processing(NLP), such as GPT-4 and Llama. However, with the\nexponential growth of model parameter sizes, LLMs bring significant resource\noverheads. Low-bit quantization, as a key technique, reduces memory usage and\ncomputational demands by decreasing the bit-width of model parameters,\nactivations, and gradients. Previous quantization methods for LLMs have largely\nemployed Post-Training Quantization (PTQ) and Quantization-Aware Training\n(QAT). PTQ does not require any retraining of the original model, while QAT\ninvolves optimizing precision during training to achieve the best quantization\nparameters. The BitNet team proposed a radically different approach, where\nquantization is performed from the start of model training, utilizing\nlow-precision binary weights during the training process. This approach has led\nto the emergence of many binary quantization techniques for large language\nmodels. This paper provides a comprehensive review of these binary quantization\ntechniques. Specifically, we will introduce binary quantization techniques in\ndeep neural networks and further explore their application to LLMs, reviewing\ntheir various contributions, implementations, and applications.']"
18,163,18_pruning_sparsity_compression_activation,"['pruning', 'sparsity', 'compression', 'activation', 'layers', 'pruned', 'performance', 'model', 'spiking', 'sparse']","['The colossal parameters and computational overhead of Large Language Models\n(LLMs) challenge their real-world applications. Network pruning, which targets\nunstructured or structured sparsity by removing redundant parameters, has\nrecently been explored for LLM acceleration. Existing LLM pruning works focus\non unstructured pruning, which typically requires special hardware support for\na practical speed-up. In contrast, structured pruning can reduce latency on\ngeneral devices. However, it remains a challenge to perform structured pruning\nefficiently and maintain performance, especially at high sparsity ratios. To\nthis end, we introduce an efficient structured pruning framework named CFSP,\nwhich leverages both Coarse (interblock) and Fine-grained (intrablock)\nactivation information as an importance criterion to guide pruning. The pruning\nis highly efficient, as it only requires one forward pass to compute feature\nactivations. Specifically, we first allocate the sparsity budget across blocks\nbased on their importance and then retain important weights within each block.\nIn addition, we introduce a recovery fine-tuning strategy that adaptively\nallocates training overhead based on coarse-grained importance to further\nimprove performance. Experimental results demonstrate that CFSP outperforms\nexisting methods on diverse models across various sparsity budgets. Our code\nwill be available at https://github.com/wyxscir/CFSP.', 'Post-training pruning has emerged as a crucial optimization technique as\nlarge language models (LLMs) continue to grow rapidly. However, the significant\nvariations in weight distributions across different LLMs make fixed pruning\nstrategies inadequate for multiple models. In this paper, we introduce\n\\textbf{\\textsc{OptiShear}}, an efficient evolutionary optimization framework\nfor adaptive LLM pruning. Our framework features two key innovations: an\neffective search space built on our Meta pruning metric to handle diverse\nweight distributions, and a model-wise reconstruction error for rapid\nevaluation during search trials. We employ Non-dominated Sorting Genetic\nAlgorithm III (NSGA-III) to optimize both pruning metrics and layerwise\nsparsity ratios. Through extensive evaluation on LLaMA-1/2/3 and Mistral models\n(7B-70B) across multiple benchmarks, we demonstrate that our adaptive pruning\nmetrics consistently outperform existing methods. Additionally, our discovered\nlayerwise sparsity ratios enhance the effectiveness of other pruning metrics.\nThe framework exhibits strong cross-task and cross-model generalizability,\nproviding a cost-effective solution for model compression.', 'Large language models (LLMs) have garnered significant attention for their\nremarkable capabilities across various domains, whose vast parameter scales\npresent challenges for practical deployment. Structured pruning is an effective\nmethod to balance model performance with efficiency, but performance\nrestoration under computational resource constraints is a principal challenge\nin pruning LLMs. Therefore, we present a low-cost and fast structured pruning\nmethod for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We\npropose Batched Greedy Pruning for rapid and near-optimal pruning, which\nenhances the accuracy of head-wise pruning error estimation through grouped\nCholesky decomposition and improves the pruning efficiency of FFN via Dynamic\nGroup Size, thereby achieving approximate local optimal pruning results within\none hour. Besides, we explore the limitations of layer-wise pruning from the\nperspective of error accumulation and propose Incremental Pruning Ratio, a\nnon-uniform pruning strategy to reduce performance degradation. Experimental\nresults on the LLaMA benchmark show that SlimGPT outperforms other methods and\nachieves state-of-the-art results.']"
19,163,19_confidence_uncertainty_calibration_evaluation,"['confidence', 'uncertainty', 'calibration', 'evaluation', 'reliability', 'llms', 'we', 'human', 'judges', 'their']","['LLM-as-a-Judge is a widely used method for evaluating the performance of\nLarge Language Models (LLMs) across various tasks. We address the challenge of\nquantifying the uncertainty of LLM-as-a-Judge evaluations. While uncertainty\nquantification has been well-studied in other domains, applying it effectively\nto LLMs poses unique challenges due to their complex decision-making\ncapabilities and computational demands. In this paper, we introduce a novel\nmethod for quantifying uncertainty designed to enhance the trustworthiness of\nLLM-as-a-Judge evaluations. The method quantifies uncertainty by analyzing the\nrelationships between generated assessments and possible ratings. By\ncross-evaluating these relationships and constructing a confusion matrix based\non token probabilities, the method derives labels of high or low uncertainty.\nWe evaluate our method across multiple benchmarks, demonstrating a strong\ncorrelation between the accuracy of LLM evaluations and the derived uncertainty\nscores. Our findings suggest that this method can significantly improve the\nreliability and consistency of LLM-as-a-Judge evaluations.', ""One important approach to improving the reliability of large language models\n(LLMs) is to provide accurate confidence estimations regarding the correctness\nof their answers. However, developing a well-calibrated confidence estimation\nmodel is challenging, as mistakes made by LLMs can be difficult to detect. We\npropose a novel method combining the LLM's self-consistency with labeled data\nand training an auxiliary model to estimate the correctness of its responses to\nquestions. This auxiliary model predicts the correctness of responses based\nsolely on their consistent information. To set up the learning problem, we use\na weighted graph to represent the consistency among the LLM's multiple\nresponses to a question. Correctness labels are assigned to these responses\nbased on their similarity to the correct answer. We then train a graph neural\nnetwork to estimate the probability of correct responses. Experiments\ndemonstrate that the proposed approach substantially outperforms several of the\nmost recent methods in confidence calibration across multiple widely adopted\nbenchmark datasets. Furthermore, the proposed approach significantly improves\nthe generalization capability of confidence calibration on out-of-domain (OOD)\ndata."", ""The rise of large language models (LLMs) and their tight integration into our\ndaily life make it essential to dedicate efforts towards their trustworthiness.\nUncertainty quantification for LLMs can establish more human trust into their\nresponses, but also allows LLM agents to make more informed decisions based on\neach other's uncertainty. To estimate the uncertainty in a response, internal\ntoken logits, task-specific proxy models, or sampling of multiple responses are\ncommonly used. This work focuses on asking the LLM itself to verbalize its\nuncertainty with a confidence score as part of its output tokens, which is a\npromising way for prompt- and model-agnostic uncertainty quantification with\nlow overhead. Using an extensive benchmark, we assess the reliability of\nverbalized confidence scores with respect to different datasets, models, and\nprompt methods. Our results reveal that the reliability of these scores\nstrongly depends on how the model is asked, but also that it is possible to\nextract well-calibrated confidence scores with certain prompt methods. We argue\nthat verbalized confidence scores can become a simple but effective and\nversatile uncertainty quantification method in the future. Our code is\navailable at https://github.com/danielyxyang/llm-verbalized-uq .""]"
20,163,20_students_student_education_educational,"['students', 'student', 'education', 'educational', 'programming', 'tutoring', 'feedback', 'teaching', 'learning', 'personalized']","[""Metacognitive education plays a crucial role in cultivating students'\nself-regulation and reflective thinking, providing essential support for those\nwith learning difficulties through academic advising. Simulating students with\ninsufficient learning capabilities using large language models offers a\npromising approach to refining pedagogical methods without ethical concerns.\nHowever, existing simulations often fail to authentically represent students'\nlearning struggles and face challenges in evaluation due to the lack of\nreliable metrics and ethical constraints in data collection. To address these\nissues, we propose a pipeline for automatically generating and filtering\nhigh-quality simulated student agents. Our approach leverages a two-round\nautomated scoring system validated by human experts and employs a score\npropagation module to obtain more consistent scores across the student graph.\nExperimental results demonstrate that our pipeline efficiently identifies\nhigh-quality student agents, and we discuss the traits that influence the\nsimulation's effectiveness. By simulating students with varying degrees of\nlearning difficulties, our work paves the way for broader applications in\npersonalized learning and educational assessment."", ""Large language models (LLMs) are quickly being adopted in a wide range of\nlearning experiences, especially via ubiquitous and broadly accessible chat\ninterfaces like ChatGPT and Copilot. This type of interface is readily\navailable to students and teachers around the world, yet relatively little\nresearch has been done to assess the impact of such generic tools on student\nlearning. Coding education is an interesting test case, both because LLMs have\nstrong performance on coding tasks, and because LLM-powered support tools are\nrapidly becoming part of the workflow of professional software engineers. To\nhelp understand the impact of generic LLM use on coding education, we conducted\na large-scale randomized control trial with 5,831 students from 146 countries\nin an online coding class in which we provided some students with access to a\nchat interface with GPT-4. We estimate positive benefits on exam performance\nfor adopters, the students who used the tool, but over all students, the\nadvertisement of GPT-4 led to a significant average decrease in exam\nparticipation. We observe similar decreases in other forms of course\nengagement. However, this decrease is modulated by the student's country of\norigin. Offering access to LLMs to students from low human development index\ncountries increased their exam participation rate on average. Our results\nsuggest there may be promising benefits to using LLMs in an introductory coding\nclass, but also potential harms for engagement, which makes their longer term\nimpact on student success unclear. Our work highlights the need for additional\ninvestigations to help understand the potential impact of future adoption and\nintegration of LLMs into classrooms."", 'There is a great need for data in computing education research. Data is\nneeded to understand how students behave, to train models of student behavior\nto optimally support students, and to develop and validate new assessment tools\nand learning analytics techniques. However, relatively few computing education\ndatasets are shared openly, often due to privacy regulations and issues in\nmaking sure the data is anonymous. Large language models (LLMs) offer a\npromising approach to create large-scale, privacy-preserving synthetic data,\nwhich can be used to explore various aspects of student learning, develop and\ntest educational technologies, and support research in areas where collecting\nreal student data may be challenging or impractical. This work explores\ngenerating synthetic buggy code submissions for introductory programming\nexercises using GPT-4o. We compare the distribution of test case failures\nbetween synthetic and real student data from two courses to analyze the\naccuracy of the synthetic data in mimicking real student data. Our findings\nsuggest that LLMs can be used to generate synthetic incorrect submissions that\nare not significantly different from real student data with regard to test case\nfailure distributions. Our research contributes to the development of reliable\nsynthetic datasets for computing education research and teaching, potentially\naccelerating progress in the field while preserving student privacy.']"
21,162,21_hallucinations_hallucination_detection_hallucinated,"['hallucinations', 'hallucination', 'detection', 'hallucinated', 'factual', 'llms', 'factuality', 'uncertainty', 'factually', 'in']","['Large Language Models (LLMs) are powerful computational models trained on\nextensive corpora of human-readable text, enabling them to perform\ngeneral-purpose language understanding and generation. LLMs have garnered\nsignificant attention in both industry and academia due to their exceptional\nperformance across various natural language processing (NLP) tasks. Despite\nthese successes, LLMs often produce inaccuracies, commonly referred to as\nhallucinations. Prompt engineering, the process of designing and formulating\ninstructions for LLMs to perform specific tasks, has emerged as a key approach\nto mitigating hallucinations. This paper provides a comprehensive empirical\nevaluation of different prompting strategies and frameworks aimed at reducing\nhallucinations in LLMs. Various prompting techniques are applied to a broad set\nof benchmark datasets to assess the accuracy and hallucination rate of each\nmethod. Additionally, the paper investigates the influence of tool-calling\nagents (LLMs augmented with external tools to enhance their capabilities beyond\nlanguage generation) on hallucination rates in the same benchmarks. The\nfindings demonstrate that the optimal prompting technique depends on the type\nof problem, and that simpler techniques often outperform more complex methods\nin reducing hallucinations. Furthermore, it is shown that LLM agents can\nexhibit significantly higher hallucination rates due to the added complexity of\nexternal tool usage.', 'Recent advances in large language models (LLMs) have shown promising\nimprovements, often surpassing existing methods across a wide range of\ndownstream tasks in natural language processing. However, these models still\nface challenges, which may hinder their practical applicability. For example,\nthe phenomenon of hallucination is known to compromise the reliability of LLMs,\nespecially in fields that demand high factual precision. Current benchmarks\nprimarily focus on hallucination detection and factuality evaluation but do not\nextend beyond identification. This paper proposes an explanation enhanced\nhallucination-detection model, coined as HuDEx, aimed at enhancing the\nreliability of LLM-generated responses by both detecting hallucinations and\nproviding detailed explanations. The proposed model provides a novel approach\nto integrate detection with explanations, and enable both users and the LLM\nitself to understand and reduce errors. Our measurement results demonstrate\nthat the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in\nhallucination detection accuracy, while maintaining reliable explanations.\nFurthermore, the proposed model performs well in both zero-shot and other test\nenvironments, showcasing its adaptability across diverse benchmark datasets.\nThe proposed approach further enhances the hallucination detection research by\nintroducing a novel approach to integrating interpretability with hallucination\ndetection, which further enhances the performance and reliability of evaluating\nhallucinations in language models.', 'Large Language Models (LLMs) have shown propensity to generate hallucinated\noutputs, i.e., texts that are factually incorrect or unsupported. Existing\nmethods for alleviating hallucinations typically require costly human\nannotations to identify and correct hallucinations in LLM outputs. Moreover,\nmost of these methods focus on a specific type of hallucination, e.g., entity\nor token errors, which limits their effectiveness in addressing various types\nof hallucinations exhibited in LLM outputs. To our best knowledge, in this\npaper we propose the first active learning framework to alleviate LLM\nhallucinations, reducing costly human annotations of hallucination needed. By\nmeasuring fine-grained hallucinations from errors in semantic frame, discourse\nand content verifiability in text summarization, we propose HAllucination\nDiversity-Aware Sampling (HADAS) to select diverse hallucinations for\nannotations in active learning for LLM finetuning. Extensive experiments on\nthree datasets and different backbone models demonstrate advantages of our\nmethod in effectively and efficiently mitigating LLM hallucinations.']"
22,160,22_speech_asr_recognition_spoken,"['speech', 'asr', 'recognition', 'spoken', 'audio', 'automatic', 'text', 'correction', 'error', 'translation']","['Large language models (LLMs) have demonstrated remarkable advancements in\nlanguage understanding and generation. Building on the success of text-based\nLLMs, recent research has adapted these models to use speech embeddings for\nprompting, resulting in Speech-LLM models that exhibit strong performance in\nautomatic speech recognition (ASR) and automatic speech translation (AST). In\nthis work, we propose a novel approach to leverage ASR transcripts as prompts\nfor AST in a Speech-LLM built on an encoder-decoder text LLM. The Speech-LLM\nmodel consists of a speech encoder and an encoder-decoder structure\nMegatron-T5. By first decoding speech to generate ASR transcripts and\nsubsequently using these transcripts along with encoded speech for prompting,\nwe guide the speech translation in a two-step process like chain-of-thought\n(CoT) prompting. Low-rank adaptation (LoRA) is used for the T5 LLM for model\nadaptation and shows superior performance to full model fine-tuning.\nExperimental results show that the proposed CoT prompting significantly\nimproves AST performance, achieving an average increase of 2.4 BLEU points\nacross 6 En->X or X->En AST tasks compared to speech prompting alone.\nAdditionally, compared to a related CoT prediction method that predicts a\nconcatenated sequence of ASR and AST transcripts, our method performs better by\nan average of 2 BLEU points.', 'One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts.', 'Recent advancements in integrating speech information into large language\nmodels (LLMs) have significantly improved automatic speech recognition (ASR)\naccuracy. However, existing methods often constrained by the capabilities of\nthe speech encoders under varied acoustic conditions, such as accents. To\naddress this, we propose LA-RAG, a novel Retrieval-Augmented Generation (RAG)\nparadigm for LLM-based ASR. LA-RAG leverages fine-grained token-level speech\ndatastores and a speech-to-speech retrieval mechanism to enhance ASR accuracy\nvia LLM in-context learning (ICL) capabilities. Experiments on Mandarin and\nvarious Chinese dialect datasets demonstrate significant improvements in ASR\naccuracy compared to existing methods, validating the effectiveness of our\napproach, especially in handling accent variations.']"
23,152,23_dialogue_dialogues_conversational_tod,"['dialogue', 'dialogues', 'conversational', 'tod', 'taskoriented', 'intent', 'conversation', 'conversations', 'user', 'multiturn']","['Dialogue policies play a crucial role in developing task-oriented dialogue\nsystems, yet their development and maintenance are challenging and typically\nrequire substantial effort from experts in dialogue modeling. While in many\nsituations, large amounts of conversational data are available for the task at\nhand, people lack an effective solution able to extract dialogue policies from\nthis data. In this paper, we address this gap by first illustrating how Large\nLanguage Models (LLMs) can be instrumental in extracting dialogue policies from\ndatasets, through the conversion of conversations into a unified intermediate\nrepresentation consisting of canonical forms. We then propose a novel method\nfor generating dialogue policies utilizing a controllable and interpretable\ngraph-based methodology. By combining canonical forms across conversations into\na flow network, we find that running graph traversal algorithms helps in\nextracting dialogue flows. These flows are a better representation of the\nunderlying interactions than flows extracted by prompting LLMs. Our technique\nfocuses on giving conversation designers greater control, offering a\nproductivity tool to improve the process of developing dialogue policies.', 'Developing language model-based dialogue agents requires effective data to\ntrain models that can follow specific task logic. However, most existing data\nsimulation methods focus on increasing diversity in language, topics, or\ndialogue acts at the utterance level, largely neglecting a critical aspect of\ntask logic diversity at the dialogue level. This paper proposes a novel data\nsimulation method designed to enhance the diversity of synthetic dialogues by\nfocusing on task execution logic. Our method uses LLMs to generate decision\ntree-structured task plans, which enables the derivation of diverse dialogue\ntrajectories for a given task. Each trajectory, referred to as a ""dialog flow"",\nguides the generation of a multi-turn dialogue that follows a unique\ntrajectory. We apply this method to generate a task-oriented dialogue dataset\ncomprising 3,886 dialogue flows across 15 different domains. We validate the\neffectiveness of this dataset using the next action prediction task, where\nmodels fine-tuned on our dataset outperform strong baselines, including GPT-4.\nUpon acceptance of this paper, we plan to release the code and data publicly.', ""Knowledge models are fundamental to dialogue systems for enabling\nconversational interactions, which require handling domain-specific knowledge.\nEnsuring effective communication in information-providing conversations entails\naligning user understanding with the knowledge available to the system.\nHowever, dialogue systems often face challenges arising from semantic\ninconsistencies in how information is expressed in natural language compared to\nhow it is represented within the system's internal knowledge. To address this\nproblem, we study the potential of large language models for conversational\ngrounding, a mechanism to bridge information gaps by establishing shared\nknowledge between dialogue participants. Our approach involves annotating human\nconversations across five knowledge domains to create a new dialogue corpus\ncalled BridgeKG. Through a series of experiments on this dataset, we\nempirically evaluate the capabilities of large language models in classifying\ngrounding acts and identifying grounded information items within a knowledge\ngraph structure. Our findings offer insights into how these models use\nin-context learning for conversational grounding tasks and common prediction\nerrors, which we illustrate with examples from challenging dialogues. We\ndiscuss how the models handle knowledge graphs as a semantic layer between\nunstructured dialogue utterances and structured information items.""]"
24,146,24_mental_health_depression_counseling,"['mental', 'health', 'depression', 'counseling', 'therapy', 'psychological', 'cbt', 'psychotherapy', 'support', 'care']","[""Access to mental health support remains limited, particularly in marginalized\ncommunities where structural and cultural barriers hinder timely care. This\npaper explores the potential of AI-enabled chatbots as a scalable solution,\nfocusing on advanced large language models (LLMs)-GPT v4, Mistral Large, and\nLLama V3.1-and assessing their ability to deliver empathetic, meaningful\nresponses in mental health contexts. While these models show promise in\ngenerating structured responses, they fall short in replicating the emotional\ndepth and adaptability of human therapists. Additionally, trustworthiness,\nbias, and privacy challenges persist due to unreliable datasets and limited\ncollaboration with mental health professionals. To address these limitations,\nwe propose a federated learning framework that ensures data privacy, reduces\nbias, and integrates continuous validation from clinicians to enhance response\nquality. This approach aims to develop a secure, evidence-based AI chatbot\ncapable of offering trustworthy, empathetic, and bias-reduced mental health\nsupport, advancing AI's role in digital mental health care."", 'Large Language Models (LLMs) are transforming mental health care by enhancing\naccessibility, personalization, and efficiency in therapeutic interventions.\nThese AI-driven tools empower mental health professionals with real-time\nsupport, improved data integration, and the ability to encourage care-seeking\nbehaviors, particularly in underserved communities. By harnessing LLMs,\npractitioners can deliver more empathetic, tailored, and effective support,\naddressing longstanding gaps in mental health service provision. However, their\nimplementation comes with significant challenges and ethical concerns.\nPerformance limitations, data privacy risks, biased outputs, and the potential\nfor generating misleading information underscore the critical need for\nstringent ethical guidelines and robust evaluation mechanisms. The sensitive\nnature of mental health data further necessitates meticulous safeguards to\nprotect patient rights and ensure equitable access to AI-driven care.\nProponents argue that LLMs have the potential to democratize mental health\nresources, while critics warn of risks such as misuse and the diminishment of\nhuman connection in therapy. Achieving a balance between innovation and ethical\nresponsibility is imperative. This paper examines the transformative potential\nof LLMs in mental health care, highlights the associated technical and ethical\ncomplexities, and advocates for a collaborative, multidisciplinary approach to\nensure these advancements align with the goal of providing compassionate,\nequitable, and effective mental health support.', 'Large language models (LLMs) are increasingly used in medical fields. In\nmental health support, the early identification of linguistic markers\nassociated with mental health conditions can provide valuable support to mental\nhealth professionals, and reduce long waiting times for patients. Despite the\nbenefits of LLMs for mental health support, there is limited research on their\napplication in mental health systems for languages other than English. Our\nstudy addresses this gap by focusing on the detection of depression severity in\nGreek through user-generated posts which are automatically translated from\nEnglish. Our results show that GPT3.5-turbo is not very successful in\nidentifying the severity of depression in English, and it has a varying\nperformance in Greek as well. Our study underscores the necessity for further\nresearch, especially in languages with less resources. Also, careful\nimplementation is necessary to ensure that LLMs are used effectively in mental\nhealth platforms, and human supervision remains crucial to avoid misdiagnosis.']"
25,138,25_longcontext_context_long_length,"['longcontext', 'context', 'long', 'length', 'compression', 'position', 'window', 'attention', 'positional', 'rope']","['Large language models (LLMs) are commonly trained on datasets consisting of\nfixed-length token sequences. These datasets are created by randomly\nconcatenating documents of various lengths and then chunking them into\nsequences of a predetermined target length (concat-and-chunk). Recent attention\nimplementations mask cross-document attention, reducing the effective length of\na chunk of tokens. Additionally, training on long sequences becomes\ncomputationally prohibitive due to the quadratic cost of attention. In this\nstudy, we introduce dataset decomposition, a novel variable sequence length\ntraining technique, to tackle these challenges. We decompose a dataset into a\nunion of buckets, each containing sequences of the same size extracted from a\nunique document. During training, we use variable sequence length and\nbatch-size, sampling simultaneously from all buckets with a curriculum. In\ncontrast to the concat-and-chunk baseline, which incurs a fixed attention cost\nat every step of training, our proposed method incurs a computational cost\nproportional to the actual document lengths at each step, resulting in\nsignificant savings in training time. We train an 8k context-length 1B model at\nthe same cost as a 2k context-length model trained with the baseline approach.\nExperiments on a web-scale corpus demonstrate that our approach significantly\nenhances performance on standard language evaluations and long-context\nbenchmarks, reaching target accuracy with up to 6x faster training compared to\nthe baseline. Our method not only enables efficient pretraining on long\nsequences but also scales effectively with dataset size. Lastly, we shed light\non a critical yet less studied aspect of training large language models: the\ndistribution and curriculum of sequence lengths, which results in a\nnon-negligible difference in performance.', 'We introduce LongSkywork, a long-context Large Language Model (LLM) capable\nof processing up to 200,000 tokens. We provide a training recipe for\nefficiently extending context length of LLMs. We identify that the critical\nelement in enhancing long-context processing capability is to incorporate a\nlong-context SFT stage following the standard SFT stage. A mere 200 iterations\ncan convert the standard SFT model into a long-context model. To reduce the\neffort in collecting and annotating data for long-context language modeling, we\ndevelop two novel methods for creating synthetic data. These methods are\napplied during the continual pretraining phase as well as the Supervised\nFine-Tuning (SFT) phase, greatly enhancing the training efficiency of our\nlong-context LLMs. Our findings suggest that synthetic long-context SFT data\ncan surpass the performance of data curated by humans to some extent.\nLongSkywork achieves outstanding performance on a variety of long-context\nbenchmarks. In the Needle test, a benchmark for long-context information\nretrieval, our models achieved perfect accuracy across multiple context spans.\nMoreover, in realistic application scenarios, LongSkywork-13B demonstrates\nperformance on par with Claude2.1, the leading long-context model, underscoring\nthe effectiveness of our proposed methods.', 'Large language models (LLMs) based on Transformer have been widely applied in\nthe filed of natural language processing (NLP), demonstrating strong\nperformance, particularly in handling short text tasks. However, when it comes\nto long context scenarios, the performance of LLMs degrades due to some\nchallenges. To alleviate this phenomenon, there is a number of work proposed\nrecently. In this survey, we first list the challenges of applying pre-trained\nLLMs to process long contexts. Then systematically review the approaches\nrelated to long context and propose our taxonomy categorizing them into four\nmain types: positional encoding, context compression, retrieval augmented, and\nattention pattern. In addition to the approaches, we focus on the evaluation of\nlong context, organizing relevant data, tasks, and metrics based on existing\nlong context benchmarks. Finally, we summarize unresolved issues in the long\ncontext domain and put forward our views on future developments.']"
26,133,26_unlearning_copyrighted_memorization_forget,"['unlearning', 'copyrighted', 'memorization', 'forget', 'copyright', 'membership', 'mia', 'data', 'forgetting', 'sensitive']","['Machine unlearning is a key requirement of many data protection regulations\nsuch as GDPR. Prior work on unlearning has mostly considered superficial\nunlearning tasks where a single or a few related pieces of information are\nrequired to be removed. However, the task of unlearning a fact is much more\nchallenging in recent large language models (LLMs), because the facts in LLMs\ncan be deduced from each other. In this work, we investigate whether current\nunlearning methods for LLMs succeed beyond superficial unlearning of facts.\nSpecifically, we formally propose a framework and a definition for deep\nunlearning facts that are interrelated. We design the metric, recall, to\nquantify the extent of deep unlearning. To systematically evaluate deep\nunlearning, we construct a synthetic dataset EDU-RELAT, which consists of a\nsynthetic knowledge base of family relationships and biographies, together with\na realistic logical rule set that connects them. We use this dataset to test\nfour unlearning methods in four LLMs at different sizes. Our findings reveal\nthat in the task of deep unlearning only a single fact, they either fail to\nproperly unlearn with high recall, or end up unlearning many other irrelevant\nfacts. Our dataset and code are publicly available at:\nhttps://github.com/wrh14/deep_unlearning.', ""Machine unlearning aims to solve the problem of removing the influence of\nselected training examples from a learned model. Despite the increasing\nattention to this problem, it remains an open research question how to evaluate\nunlearning in large language models (LLMs), and what are the critical\nproperties of the data to be unlearned that affect the quality and efficiency\nof unlearning. This work formalizes a metric to evaluate unlearning quality in\ngenerative models, and uses it to assess the trade-offs between unlearning\nquality and performance. We demonstrate that unlearning out-of-distribution\nexamples requires more unlearning steps but overall presents a better trade-off\noverall. For in-distribution examples, however, we observe a rapid decay in\nperformance as unlearning progresses. We further evaluate how example's\nmemorization and difficulty affect unlearning under a classical gradient\nascent-based approach."", 'Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning.']"
27,131,27_cybersecurity_cyber_threat_security,"['cybersecurity', 'cyber', 'threat', 'security', 'penetration', 'threats', 'attack', 'cti', 'detection', 'network']","['The rapid advancement of Large Language Models (LLMs) has opened up new\nopportunities for leveraging artificial intelligence in various domains,\nincluding cybersecurity. As the volume and sophistication of cyber threats\ncontinue to grow, there is an increasing need for intelligent systems that can\nautomatically detect vulnerabilities, analyze malware, and respond to attacks.\nIn this survey, we conduct a comprehensive review of the literature on the\napplication of LLMs in cybersecurity (LLM4Security). By comprehensively\ncollecting over 30K relevant papers and systematically analyzing 127 papers\nfrom top security and software engineering venues, we aim to provide a holistic\nview of how LLMs are being used to solve diverse problems across the\ncybersecurity domain. Through our analysis, we identify several key findings.\nFirst, we observe that LLMs are being applied to a wide range of cybersecurity\ntasks, including vulnerability detection, malware analysis, network intrusion\ndetection, and phishing detection. Second, we find that the datasets used for\ntraining and evaluating LLMs in these tasks are often limited in size and\ndiversity, highlighting the need for more comprehensive and representative\ndatasets. Third, we identify several promising techniques for adapting LLMs to\nspecific cybersecurity domains, such as fine-tuning, transfer learning, and\ndomain-specific pre-training. Finally, we discuss the main challenges and\nopportunities for future research in LLM4Security, including the need for more\ninterpretable and explainable models, the importance of addressing data privacy\nand security concerns, and the potential for leveraging LLMs for proactive\ndefense and threat hunting. Overall, our survey provides a comprehensive\noverview of the current state-of-the-art in LLM4Security and identifies several\npromising directions for future research.', 'To address the increasing complexity and frequency of cybersecurity incidents\nemphasized by the recent cybersecurity threat reports with over 10 billion\ninstances, cyber threat intelligence (CTI) plays a critical role in the modern\ncybersecurity landscape by offering the insights required to understand and\ncombat the constantly evolving nature of cyber threats. Inspired by the\npowerful capability of large language models (LLMs) in handling complex tasks,\nin this paper, we introduce a framework to benchmark, elicit, and improve\ncybersecurity incident analysis and response abilities in LLMs for Security\nEvents (SEvenLLM). Specifically, we create a high-quality bilingual instruction\ncorpus by crawling cybersecurity raw text from cybersecurity websites to\novercome the lack of effective data for information extraction. Then, we design\na pipeline to auto-select tasks from the tasks pool and convert the raw text\ninto supervised corpora comprised of question and response. The instruction\ndataset SEvenLLM-Instruct is used to train cybersecurity LLMs with the\nmulti-task learning objective (27 well-designed tasks) for augmenting the\nanalysis of cybersecurity events. Extensive experiments in our curated\nbenchmark (SEvenLLM-bench) demonstrate that SEvenLLM performs more\nsophisticated threat analysis and fortifies defenses against the evolving\nlandscape of cyber threats.', 'This paper provides a comprehensive review of the future of cybersecurity\nthrough Generative AI and Large Language Models (LLMs). We explore LLM\napplications across various domains, including hardware design security,\nintrusion detection, software engineering, design verification, cyber threat\nintelligence, malware detection, and phishing detection. We present an overview\nof LLM evolution and its current state, focusing on advancements in models such\nas GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends\nto LLM vulnerabilities, such as prompt injection, insecure output handling,\ndata poisoning, DDoS attacks, and adversarial instructions. We delve into\nmitigation strategies to protect these models, providing a comprehensive look\nat potential attack scenarios and prevention techniques. Furthermore, we\nevaluate the performance of 42 LLM models in cybersecurity knowledge and\nhardware security, highlighting their strengths and weaknesses. We thoroughly\nevaluate cybersecurity datasets for LLM training and testing, covering the\nlifecycle from data creation to usage and identifying gaps for future research.\nIn addition, we review new strategies for leveraging LLMs, including techniques\nlike Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human\nFeedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank\nAdapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim\nto enhance real-time cybersecurity defenses and improve the sophistication of\nLLM applications in threat detection and response. Our paper provides a\nfoundational understanding and strategic direction for integrating LLMs into\nfuture cybersecurity frameworks, emphasizing innovation and robust model\ndeployment to safeguard against evolving cyber threats.']"
28,131,28_agents_multiagent_agent_workflows,"['agents', 'multiagent', 'agent', 'workflows', 'automation', 'systems', 'process', 'design', 'workflow', 'system']","[""This paper presents a Spark-based modular LangGraph framework, designed to\nenhance machine learning workflows through scalability, visualization, and\nintelligent process optimization. At its core, the framework introduces Agent\nAI, a pivotal innovation that leverages Spark's distributed computing\ncapabilities and integrates with LangGraph for workflow orchestration.\n  Agent AI facilitates the automation of data preprocessing, feature\nengineering, and model evaluation while dynamically interacting with data\nthrough Spark SQL and DataFrame agents. Through LangGraph's graph-structured\nworkflows, the agents execute complex tasks, adapt to new inputs, and provide\nreal-time feedback, ensuring seamless decision-making and execution in\ndistributed environments. This system simplifies machine learning processes by\nallowing users to visually design workflows, which are then converted into\nSpark-compatible code for high-performance execution.\n  The framework also incorporates large language models through the LangChain\necosystem, enhancing interaction with unstructured data and enabling advanced\ndata analysis. Experimental evaluations demonstrate significant improvements in\nprocess efficiency and scalability, as well as accurate data-driven\ndecision-making in diverse application scenarios.\n  This paper emphasizes the integration of Spark with intelligent agents and\ngraph-based workflows to redefine the development and execution of machine\nlearning tasks in big data environments, paving the way for scalable and\nuser-friendly AI solutions."", 'In recent years, data science agents powered by Large Language Models (LLMs),\nknown as ""data agents,"" have shown significant potential to transform the\ntraditional data analysis paradigm. This survey provides an overview of the\nevolution, capabilities, and applications of LLM-based data agents,\nhighlighting their role in simplifying complex data tasks and lowering the\nentry barrier for users without related expertise. We explore current trends in\nthe design of LLM-based frameworks, detailing essential features such as\nplanning, reasoning, reflection, multi-agent collaboration, user interface,\nknowledge integration, and system design, which enable agents to address\ndata-centric problems with minimal human intervention. Furthermore, we analyze\nseveral case studies to demonstrate the practical applications of various data\nagents in real-world scenarios. Finally, we identify key challenges and propose\nfuture research directions to advance the development of data agents into\nintelligent statistical analysis software.', 'Autonomous agents driven by Large Language Models (LLMs) offer enormous\npotential for automation. Early proof of this technology can be found in\nvarious demonstrations of agents solving complex tasks, interacting with\nexternal systems to augment their knowledge, and triggering actions. In\nparticular, workflows involving multiple agents solving complex tasks in a\ncollaborative fashion exemplify their capacity to operate in less strict and\nless well-defined environments. Thus, a multi-agent approach has great\npotential for serving as a backbone in many industrial applications, ranging\nfrom complex knowledge retrieval systems to next generation robotic process\nautomation. Given the reasoning abilities within the current generation of\nLLMs, complex processes require a multi-step approach that includes a plan of\nwell-defined and modular tasks. Depending on the level of complexity, these\ntasks can be executed either by a single agent or a group of agents. In this\nwork, we focus on designing a flexible agent engineering framework with careful\nattention to planning and execution, capable of handling complex use case\napplications across various domains. The proposed framework provides\nreliability in industrial applications and presents techniques to ensure a\nscalable, flexible, and collaborative workflow for multiple autonomous agents\nworking together towards solving tasks.']"
29,124,29_kv_cache_attention_memory,"['kv', 'cache', 'attention', 'memory', 'keyvalue', 'compression', 'tokens', 'inference', 'longcontext', 'heads']","['In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.', 'With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.', 'To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.']"
30,124,30_translation_mt_translations_machine,"['translation', 'mt', 'translations', 'machine', 'nmt', 'parallel', 'languages', 'quality', 'lowresource', 'csc']","['This paper introduces a novel framework that leverages large language models\n(LLMs) for machine translation (MT). We start with one conjecture: an ideal\ntranslation should contain complete and accurate information for a strong\nenough LLM to recover the original sentence. We generate multiple translation\ncandidates from a source language A to a target language B, and subsequently\ntranslate these candidates back to the original language A. By evaluating the\ncycle consistency between the original and back-translated sentences using\nmetrics such as token-level precision and accuracy, we implicitly estimate the\ntranslation quality in language B, without knowing its ground-truth. This also\nhelps to evaluate the LLM translation capability, only with monolingual\ncorpora. For each source sentence, we identify the translation candidate with\noptimal cycle consistency with the original sentence as the final answer. Our\nexperiments demonstrate that larger LLMs, or the same LLM with more forward\npasses during inference, exhibit increased cycle consistency, aligning with the\nLLM model size scaling law and test-time computation scaling law. This work\nprovide methods for, 1) to implicitly evaluate translation quality of a\nsentence in the target language, 2), to evaluate capability of LLM for\nany-to-any-language translation, and 3), how to generate a better translation\nfor a specific LLM.', 'In recent years, with the rapid development of deep learning technology,\nlarge language models (LLMs) such as BERT and GPT have achieved breakthrough\nresults in natural language processing tasks. Machine translation (MT), as one\nof the core tasks of natural language processing, has also benefited from the\ndevelopment of large language models and achieved a qualitative leap. Despite\nthe significant progress in translation performance achieved by large language\nmodels, machine translation still faces many challenges. Therefore, in this\npaper, we construct the dataset Euas-20 to evaluate the performance of large\nlanguage models on translation tasks, the translation ability on different\nlanguages, and the effect of pre-training data on the translation ability of\nLLMs for researchers and developers.', 'This paper presents the submission of Huawei Translation Services Center\n(HW-TSC) to machine translation tasks of the 20th China Conference on Machine\nTranslation (CCMT 2024). We participate in the bilingual machine translation\ntask and multi-domain machine translation task. For these two translation\ntasks, we use training strategies such as regularized dropout, bidirectional\ntraining, data diversification, forward translation, back translation,\nalternated training, curriculum learning, and transductive ensemble learning to\ntrain neural machine translation (NMT) models based on the deep Transformer-big\narchitecture. Furthermore, to explore whether large language model (LLM) can\nhelp improve the translation quality of NMT systems, we use supervised\nfine-tuning to train llama2-13b as an Automatic post-editing (APE) model to\nimprove the translation results of the NMT model on the multi-domain machine\ntranslation task. By using these plyometric strategies, our submission achieves\na competitive result in the final evaluation.']"
31,123,31_medical_radiology_reports_report,"['medical', 'radiology', 'reports', 'report', 'image', 'images', 'xray', 'imaging', 'clinical', 'chest']","['Multimodal Large Language Models (MLLMs) inherit the superior text\nunderstanding capabilities of LLMs and extend these capabilities to multimodal\nscenarios. These models achieve excellent results in the general domain of\nmultimodal tasks. However, in the medical domain, the substantial training\ncosts and the requirement for extensive medical data pose challenges to the\ndevelopment of medical MLLMs. Furthermore, due to the free-text form of\nanswers, tasks such as visual grounding that need to produce output in a\nprescribed form become difficult for MLLMs. So far, there have been no medical\nMLLMs works in medical visual grounding area. For the medical vision grounding\ntask, which involves identifying locations in medical images based on short\ntext descriptions, we propose Parameter-efficient Fine-tuning medical\nmultimodal large language models for Medcial Visual Grounding (PFMVG). To\nvalidate the performance of the model, we evaluate it on a public benchmark\ndataset for medical visual grounding, where it achieves competitive results,\nand significantly outperforming GPT-4v. Our code will be open sourced after\npeer review.', ""Recent advancements in artificial intelligence (AI) have precipitated\nsignificant breakthroughs in healthcare, particularly in refining diagnostic\nprocedures. However, previous studies have often been constrained to limited\nfunctionalities. This study introduces MiniGPT-Med, a vision-language model\nderived from large-scale language models and tailored for medical applications.\nMiniGPT-Med demonstrates remarkable versatility across various imaging\nmodalities, including X-rays, CT scans, and MRIs, enhancing its utility. The\nmodel is capable of performing tasks such as medical report generation, visual\nquestion answering (VQA), and disease identification within medical imagery.\nIts integrated processing of both image and textual clinical data markedly\nimproves diagnostic accuracy. Our empirical assessments confirm MiniGPT-Med's\nsuperior performance in disease grounding, medical report generation, and VQA\nbenchmarks, representing a significant step towards reducing the gap in\nassisting radiology practice. Furthermore, it achieves state-of-the-art\nperformance on medical report generation, higher than the previous best model\nby 19\\% accuracy. MiniGPT-Med promises to become a general interface for\nradiology diagnoses, enhancing diagnostic efficiency across a wide range of\nmedical imaging applications."", 'Medical report generation is the task of automatically writing radiology\nreports for chest X-ray images. Manually composing these reports is a\ntime-consuming process that is also prone to human errors. Generating medical\nreports can therefore help reduce the burden on radiologists. In other words,\nwe can promote greater clinical automation in the medical domain. In this work,\nwe propose a new framework leveraging vision-enabled Large Language Models\n(LLM) for the task of medical report generation. We introduce a lightweight\nsolution that achieves better or comparative performance as compared to\nprevious solutions on the task of medical report generation. We conduct\nextensive experiments exploring different model sizes and enhancement\napproaches, such as prefix tuning to improve the text generation abilities of\nthe LLMs. We evaluate our approach on a prominent large-scale radiology report\ndataset - MIMIC-CXR. Our results demonstrate the capability of our\nresource-efficient framework to generate patient-specific reports with strong\nmedical contextual understanding and high precision.']"
32,114,32_legal_law_case_domain,"['legal', 'law', 'case', 'domain', 'court', 'judicial', 'documents', 'judgment', 'cases', 'lawyers']","['Legal case retrieval for sourcing similar cases is critical in upholding\njudicial fairness. Different from general web search, legal case retrieval\ninvolves processing lengthy, complex, and highly specialized legal documents.\nExisting methods in this domain often overlook the incorporation of legal\nexpert knowledge, which is crucial for accurately understanding and modeling\nlegal cases, leading to unsatisfactory retrieval performance. This paper\nintroduces KELLER, a legal knowledge-guided case reformulation approach based\non large language models (LLMs) for effective and interpretable legal case\nretrieval. By incorporating professional legal knowledge about crimes and law\narticles, we enable large language models to accurately reformulate the\noriginal legal case into concise sub-facts of crimes, which contain the\nessential information of the case. Extensive experiments on two legal case\nretrieval benchmarks demonstrate superior retrieval performance and robustness\non complex legal case queries of KELLER over existing methods.', 'Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.', 'In this paper, we review legal testing methods based on Large Language Models\n(LLMs), using the OPENAI o1 model as a case study to evaluate the performance\nof large models in applying legal provisions. We compare current\nstate-of-the-art LLMs, including open-source, closed-source, and legal-specific\nmodels trained specifically for the legal domain. Systematic tests are\nconducted on English and Chinese legal cases, and the results are analyzed in\ndepth. Through systematic testing of legal cases from common law systems and\nChina, this paper explores the strengths and weaknesses of LLMs in\nunderstanding and applying legal texts, reasoning through legal issues, and\npredicting judgments. The experimental results highlight both the potential and\nlimitations of LLMs in legal applications, particularly in terms of challenges\nrelated to the interpretation of legal language and the accuracy of legal\nreasoning. Finally, the paper provides a comprehensive analysis of the\nadvantages and disadvantages of various types of models, offering valuable\ninsights and references for the future application of AI in the legal field.']"
33,113,33_vulnerability_vulnerabilities_code_security,"['vulnerability', 'vulnerabilities', 'code', 'security', 'software', 'detection', 'secure', 'vulnerable', 'analysis', 'developers']","['Large Language Models (LLMs) such as ChatGPT and GitHub Copilot have\nrevolutionized automated code generation in software engineering. However, as\nthese models are increasingly utilized for software development, concerns have\narisen regarding the security and quality of the generated code. These concerns\nstem from LLMs being primarily trained on publicly available code repositories\nand internet-based textual data, which may contain insecure code. This presents\na significant risk of perpetuating vulnerabilities in the generated code,\ncreating potential attack vectors for exploitation by malicious actors. Our\nresearch aims to tackle these issues by introducing a framework for secure\nbehavioral learning of LLMs through In-Content Learning (ICL) patterns during\nthe code generation process, followed by rigorous security evaluations. To\nachieve this, we have selected four diverse LLMs for experimentation. We have\nevaluated these coding LLMs across three programming languages and identified\nsecurity vulnerabilities and code smells. The code is generated through ICL\nwith curated problem sets and undergoes rigorous security testing to evaluate\nthe overall quality and trustworthiness of the generated code. Our research\nindicates that ICL-driven one-shot and few-shot learning patterns can enhance\ncode security, reducing vulnerabilities in various programming scenarios.\nDevelopers and researchers should know that LLMs have a limited understanding\nof security principles. This may lead to security breaches when the generated\ncode is deployed in production systems. Our research highlights LLMs are a\npotential source of new vulnerabilities to the software supply chain. It is\nimportant to consider this when using LLMs for code generation. This research\narticle offers insights into improving LLM security and encourages proactive\nuse of LLMs for code generation to ensure software system safety.', 'Large language models (LLMs) have brought significant advancements to code\ngeneration and code repair, benefiting both novice and experienced developers.\nHowever, their training using unsanitized data from open-source repositories,\nlike GitHub, raises the risk of inadvertently propagating security\nvulnerabilities. Despite numerous studies investigating the safety of code\nLLMs, there remains a gap in comprehensively addressing their security\nfeatures. In this work, we aim to present a comprehensive study aimed at\nprecisely evaluating and enhancing the security aspects of code LLMs. To\nsupport our research, we introduce CodeSecEval, a meticulously curated dataset\ndesigned to address 44 critical vulnerability types with 180 distinct samples.\nCodeSecEval serves as the foundation for the automatic evaluation of code\nmodels in two crucial tasks: code generation and code repair, with a strong\nemphasis on security. Our experimental results reveal that current models\nfrequently overlook security issues during both code generation and repair\nprocesses, resulting in the creation of vulnerable code. In response, we\npropose different strategies that leverage vulnerability-aware information and\ninsecure code explanations to mitigate these security vulnerabilities.\nFurthermore, our findings highlight that certain vulnerability types\nparticularly challenge model performance, influencing their effectiveness in\nreal-world applications. Based on these findings, we believe our study will\nhave a positive impact on the software engineering community, inspiring the\ndevelopment of improved methods for training and utilizing LLMs, thereby\nleading to safer and more trustworthy model deployment.', 'The increasing use of generative Artificial Intelligence (AI) in modern\nsoftware engineering, particularly Large Language Models (LLMs) for code\ngeneration, has transformed professional software development by boosting\nproductivity and automating development processes. This adoption, however, has\nhighlighted a significant issue: the introduction of security vulnerabilities\ninto the code. These vulnerabilities result, e.g., from flaws in the training\ndata that propagate into the generated code, creating challenges in disclosing\nthem. Traditional vulnerability handling processes often involve extensive\nmanual review. Applying such traditional processes to AI-generated code is\nchallenging. AI-generated code may include several vulnerabilities, possibly in\nslightly different forms as developers might not build on already implemented\ncode but prompt similar tasks. In this work, we explore the current state of\nLLM-based approaches for vulnerability handling, focusing on approaches for\nvulnerability detection, localization, and repair. We provide an overview of\nrecent progress in this area and highlight open challenges that must be\naddressed in order to establish a reliable and scalable vulnerability handling\nprocess of AI-generated code.']"
34,108,34_emotion_emotions_emotional_facial,"['emotion', 'emotions', 'emotional', 'facial', 'recognition', 'empathetic', 'speech', 'affective', 'expressions', 'empathy']","['Sentiment and emotion understanding are essential to applications such as\nhuman-computer interaction and depression detection. While Multimodal Large\nLanguage Models (MLLMs) demonstrate robust general capabilities, they face\nconsiderable challenges in the field of affective computing, particularly in\ndetecting subtle facial expressions and handling complex emotion-related tasks,\nsuch as emotion reason inference and understanding emotions in long-context\nscenarios. Furthermore, there is a lack of a unified MLLM that can effectively\nhandle both sentiment and emotion-related tasks. To address these challenges,\nwe explore multi-task training strategies for MLLMs in affective computing and\nintroduce Emotion Universe (EmoVerse), an MLLM designed to handle a broad\nspectrum of sentiment and emotion-related tasks. In addition, EmoVerse is\ncapable of deeply analyzing the underlying causes of emotional states. We also\nintroduce the Affective Multitask (AMT) Dataset, which supports multimodal\nsentiment analysis, multimodal emotion recognition, facial expression\nrecognition, emotion reason inference, and emotion cause-pair extraction tasks.\nExtensive experiments demonstrate that EmoVerse outperforms existing methods,\nachieving state-of-the-art results in sentiment and emotion-related tasks. The\ncode is available at https://github.com/liaolea/EmoVerse.', ""Emotion recognition in social situations is a complex task that requires\nintegrating information from both facial expressions and the situational\ncontext. While traditional approaches to automatic emotion recognition have\nfocused on decontextualized signals, recent research emphasizes the importance\nof context in shaping emotion perceptions. This paper contributes to the\nemerging field of context-based emotion recognition by leveraging psychological\ntheories of human emotion perception to inform the design of automated methods.\nWe propose an approach that combines emotion recognition methods with Bayesian\nCue Integration (BCI) to integrate emotion inferences from decontextualized\nfacial expressions and contextual knowledge inferred via Large-language Models.\nWe test this approach in the context of interpreting facial expressions during\na social task, the prisoner's dilemma. Our results provide clear support for\nBCI across a range of automatic emotion recognition methods. The best automated\nmethod achieved results comparable to human observers, suggesting the potential\nfor this approach to advance the field of affective computing."", ""Emotion recognition in social situations is a complex task that requires\nintegrating information from both facial expressions and the situational\ncontext. While traditional approaches to automatic emotion recognition have\nfocused on decontextualized signals, recent research emphasizes the importance\nof context in shaping emotion perceptions. This paper contributes to the\nemerging field of context-based emotion recognition by leveraging psychological\ntheories of human emotion perception to inform the design of automated methods.\nWe propose an approach that combines emotion recognition methods with Bayesian\nCue Integration (BCI) to integrate emotion inferences from decontextualized\nfacial expressions and contextual knowledge inferred via Large-language Models.\nWe test this approach in the context of interpreting facial expressions during\na social task, the prisoner's dilemma. Our results provide clear support for\nBCI across a range of automatic emotion recognition methods. The best automated\nmethod achieved results comparable to human observers, suggesting the potential\nfor this approach to advance the field of affective computing.""]"
35,102,35_sql_texttosql_database_schema,"['sql', 'texttosql', 'database', 'schema', 'queries', 'query', 'databases', 'bird', 'nl2sql', 'spider']","[""Generating accurate SQL from users' natural language questions (text-to-SQL)\nremains a long-standing challenge due to the complexities involved in user\nquestion understanding, database schema comprehension, and SQL generation.\nTraditional text-to-SQL systems, which combine human engineering and deep\nneural networks, have made significant progress. Subsequently, pre-trained\nlanguage models (PLMs) have been developed for text-to-SQL tasks, achieving\npromising results. However, as modern databases and user questions grow more\ncomplex, PLMs with a limited parameter size often produce incorrect SQL. This\nnecessitates more sophisticated and tailored optimization methods, which\nrestricts the application of PLM-based systems. Recently, large language models\n(LLMs) have shown significant capabilities in natural language understanding as\nmodel scale increases. Thus, integrating LLM-based solutions can bring unique\nopportunities, improvements, and solutions to text-to-SQL research. In this\nsurvey, we provide a comprehensive review of existing LLM-based text-to-SQL\nstudies. Specifically, we offer a brief overview of the technical challenges\nand evolutionary process of text-to-SQL. Next, we introduce the datasets and\nmetrics designed to evaluate text-to-SQL systems. Subsequently, we present a\nsystematic analysis of recent advances in LLM-based text-to-SQL. Finally, we\nmake a summarization and discuss the remaining challenges in this field and\nsuggest expectations for future research directions."", ""The text-to-SQL task aims to convert natural language into Structured Query\nLanguage (SQL) without bias. Recently, text-to-SQL methods based on large\nlanguage models (LLMs) have garnered significant attention. The core of\nmainstream text-to-SQL frameworks is schema linking, which aligns user queries\nwith relevant tables and columns in the database. Previous methods focused on\nschema linking while neglecting to enhance LLMs' understanding of database\nschema. The complex coupling relationships between tables in the database\nconstrain the SQL generation capabilities of LLMs. To tackle this issue, this\npaper proposes a simple yet effective strategy called view-based schema. This\nstrategy aids LLMs in understanding the database schema by decoupling tightly\ncoupled tables into low-coupling views. We then introduce V-SQL, a view-based\ntwo-stage text-to-SQL framework. V-SQL involves the view-based schema strategy\nto enhance LLMs' understanding of database schema. Results on the authoritative\ndatasets Bird indicate that V-SQL achieves competitive performance compared to\nexisting state-of-the-art methods."", ""Translating Natural Language Queries into Structured Query Language\n(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the\nnatural language processing and database communities, aimed at providing a\nnatural language interface to databases (NLIDB) and lowering the barrier for\nnon-experts. Despite recent advancements made through the use of Large Language\nModels (LLMs), significant challenges remain. These include handling complex\ndatabase schemas, resolving ambiguity in user queries, and generating SQL\nqueries with intricate structures that accurately reflect the user's intent. In\nthis work, we introduce E-SQL, a novel pipeline specifically designed to\naddress these challenges through direct schema linking and candidate predicate\naugmentation. E-SQL enhances the natural language query by incorporating\nrelevant database items (i.e., tables, columns, and values) and conditions\ndirectly into the question and SQL construction plan, bridging the gap between\nthe query and the database structure. The pipeline leverages candidate\npredicate augmentation to mitigate erroneous or incomplete predicates in\ngenerated SQLs. Comprehensive evaluations on the BIRD benchmark illustrate that\nE-SQL achieves competitive performance, particularly excelling in complex\nqueries with a 66.29% execution accuracy on the test set. A further observation\nfrom our experiments reveals that incorporating schema filtering into the\ntranslation pipeline does not have a positive impact on performance when the\nmost advanced proprietary LLMs are used. Additionally, our experiments with\nsmall LLMs highlight the importance and positive impact of enriched questions\non their performance. Without fine-tuning, single-prompt SQL generation using\nenriched questions with DeepSeek Coder 7B Instruct 1.5v achieves 56.45%\nexecution accuracy on the BIRD development set.""]"
36,101,36_instruction_data_instructions_tuning,"['instruction', 'data', 'instructions', 'tuning', 'training', 'selection', 'finetuning', 'diversity', 'datasets', 'samples']","['The rapid evolution of Large Language Models (LLMs) has enabled the industry\nto develop various AI-based services. Instruction tuning is considered\nessential in adapting foundation models for target domains to provide\nhigh-quality services to customers. A key challenge in instruction tuning is\nobtaining high-quality instruction data. Self-Instruct, which automatically\ngenerates instruction data using ChatGPT APIs, alleviates the data scarcity\nproblem. To improve the quality of instruction data, Self-Instruct discards\nmany of the instructions generated from ChatGPT, even though it is inefficient\nin terms of cost owing to many useless API calls. To generate high-quality\ninstruction data at a low cost, we propose a novel data generation framework,\nSelf-Direct Instruction generation (SeDi-Instruct), which employs\ndiversity-based filtering and iterative feedback task generation.\nDiversity-based filtering maintains model accuracy without excessively\ndiscarding low-quality generated instructions by enhancing the diversity of\ninstructions in a batch. This reduces the cost of synthesizing instruction\ndata. The iterative feedback task generation integrates instruction generation\nand training tasks and utilizes information obtained during the training to\ncreate high-quality instruction sets. Our results show that SeDi-Instruct\nenhances the accuracy of AI models by 5.2%, compared with traditional methods,\nwhile reducing data generation costs by 36%.', 'As large language models (LLMs) continue to advance, instruction tuning has\nbecome critical for improving their ability to generate accurate and\ncontextually appropriate responses. Although numerous instruction-tuning\ndatasets have been developed to enhance LLM performance, selecting high-quality\ninstruction data from large source datasets typically demands significant human\neffort. In this work, we introduce $\\textbf{IterSelectTune}$, an efficient,\ncost-effective iterative training policy for selecting high-quality instruction\ndata with no human involvement and limited reliance on GPT-4. By fine-tuning on\napproximately 20\\% of the source data, our method consistently outperforms\nmodels fine-tuned on the full dataset across multiple benchmarks and public\ntest datasets. These results highlight the effectiveness of our approach in\nenhancing LLM performance while reducing the computational resources required\nfor instruction tuning.', ""Instruction tuning -- tuning large language models on instruction-output\npairs -- is a promising technique for making models better adapted to the real\nworld. Yet, the key factors driving the model's capability to understand and\nfollow instructions not seen during training remain under-explored. Our\ninvestigation begins with a series of synthetic experiments within the\ntheoretical framework of a Turing-complete algorithm called Markov algorithm,\nwhich allows fine-grained control over the instruction-tuning data.\nGeneralization and robustness with respect to the training distribution emerge\nonce a diverse enough set of tasks is provided, even though very few examples\nare provided for each task. We extend these initial results to a real-world\napplication scenario of code generation and find that a more diverse\ninstruction set, extending beyond code-related tasks, improves the performance\nof code generation. Our observations suggest that a more diverse semantic space\nfor instruction-tuning sets greatly improves the model's ability to follow\ninstructions and perform tasks.""]"
37,100,37_text_aigenerated_texts_detectors,"['text', 'aigenerated', 'texts', 'detectors', 'detection', 'humanwritten', 'machinegenerated', 'ai', 'llmgenerated', 'detecting']","['The ability of large language models to generate complex texts allows them to\nbe widely integrated into many aspects of life, and their output can quickly\nfill all network resources. As the impact of LLMs grows, it becomes\nincreasingly important to develop powerful detectors for the generated text.\nThis detector is essential to prevent the potential misuse of these\ntechnologies and to protect areas such as social media from the negative\neffects of false content generated by LLMS. The main goal of LLM-generated text\ndetection is to determine whether text is generated by an LLM, which is a basic\nbinary classification task. In our work, we mainly use three different\nclassification methods based on open source datasets: traditional machine\nlearning techniques such as logistic regression, k-means clustering, Gaussian\nNaive Bayes, support vector machines, and methods based on converters such as\nBERT, and finally algorithms that use LLMs to detect LLM-generated text. We\nfocus on model generalization, potential adversarial attacks, and accuracy of\nmodel evaluation. Finally, the possible research direction in the future is\nproposed, and the current experimental results are summarized.', 'Large Language Models (LLMs) have demonstrated exceptional performance on a\nrange of downstream NLP tasks by generating text that closely resembles human\nwriting. However, the ease of achieving this similarity raises concerns from\npotential malicious uses at scale by bad actors, as LLM-generated text becomes\nincreasingly difficult to discern from human text. Although detection methods\nhave been developed to address this issue, bad actors can further manipulate\nLLM-generated texts to make them less detectable. In this work, we study how\nfurther editing texts with Reinforcement Learning from Human Feedback (RLHF),\nwhich aligns model outputs with human preferences, affects (a) the quality of\ngenerated texts for two tasks, and (b) the performance of LLM-generated text\ndetectors, looking at both training-based and zero-shot detection methods.\nAlthough RLHF improves the quality of LLM-generated texts, we find that it also\ntends to produce more detectable, lengthy, and repetitive outputs.\nAdditionally, we observe that training-based detectors are vulnerable to short\ntexts and to texts that incorporate code, whereas zero-shot detectors exhibit\ngreater robustness.', 'Recent improvements in the quality of the generations by large language\nmodels have spurred research into identifying machine-generated text. Such work\noften presents high-performing detectors. However, humans and machines can\nproduce text in different styles and domains, yet the performance impact of\nsuch on machine generated text detection systems remains unclear. In this\npaper, we audit the classification performance for detecting machine-generated\ntext by evaluating on texts with varying writing styles. We find that\nclassifiers are highly sensitive to stylistic changes and differences in text\ncomplexity, and in some cases degrade entirely to random classifiers. We\nfurther find that detection systems are particularly susceptible to misclassify\neasy-to-read texts while they have high performance for complex texts, leading\nto concerns about the reliability of detection systems. We recommend that\nfuture work attends to stylistic factors and reading difficulty levels of\nhuman-written and machine-generated text.']"
38,90,38_creativity_creative_stories_story,"['creativity', 'creative', 'stories', 'story', 'writing', 'narrative', 'ideation', 'storytelling', 'human', 'ideas']","['This paper investigates whether large language models (LLMs) show agreement\nin assessing creativity in responses to the Alternative Uses Test (AUT). While\nLLMs are increasingly used to evaluate creative content, previous studies have\nprimarily focused on a single model assessing responses generated by the same\nmodel or humans. This paper explores whether LLMs can impartially and\naccurately evaluate creativity in outputs generated by both themselves and\nother models. Using an oracle benchmark set of AUT responses, categorized by\ncreativity level (common, creative, and highly creative), we experiment with\nfour state-of-the-art LLMs evaluating these outputs. We test both scoring and\nranking methods and employ two evaluation settings (comprehensive and\nsegmented) to examine if LLMs agree on the creativity evaluation of alternative\nuses. Results reveal high inter-model agreement, with Spearman correlations\naveraging above 0.7 across models and reaching over 0.77 with respect to the\noracle, indicating a high level of agreement and validating the reliability of\nLLMs in creativity assessment of alternative uses. Notably, models do not\nfavour their own responses, instead they provide similar creativity assessment\nscores or rankings for alternative uses generated by other models. These\nfindings suggest that LLMs exhibit impartiality and high alignment in\ncreativity evaluation, offering promising implications for their use in\nautomated creativity assessment.', 'Numerous powerful large language models (LLMs) are now available for use as\nwriting support tools, idea generators, and beyond. Although these LLMs are\nmarketed as helpful creative assistants, several works have shown that using an\nLLM as a creative partner results in a narrower set of creative outputs.\nHowever, these studies only consider the effects of interacting with a single\nLLM, begging the question of whether such narrowed creativity stems from using\na particular LLM -- which arguably has a limited range of outputs -- or from\nusing LLMs in general as creative assistants. To study this question, we elicit\ncreative responses from humans and a broad set of LLMs using standardized\ncreativity tests and compare the population-level diversity of responses. We\nfind that LLM responses are much more similar to other LLM responses than human\nresponses are to each other, even after controlling for response structure and\nother key variables. This finding of significant homogeneity in creative\noutputs across the LLMs we evaluate adds a new dimension to the ongoing\nconversation about creativity and LLMs. If today\'s LLMs behave similarly, using\nthem as a creative partners -- regardless of the model used -- may drive all\nusers towards a limited set of ""creative"" outputs.', 'Story-writing is a fundamental aspect of human imagination, relying heavily\non creativity to produce narratives that are novel, effective, and surprising.\nWhile large language models (LLMs) have demonstrated the ability to generate\nhigh-quality stories, their creative story-writing capabilities remain\nunder-explored. In this work, we conduct a systematic analysis of creativity in\nshort story generation across 60 LLMs and 60 people using a five-sentence\ncreative story-writing task. We use measures to automatically evaluate model-\nand human-generated stories across several dimensions of creativity, including\nnovelty, surprise, diversity, and linguistic complexity. We also collect\ncreativity ratings and Turing Test classifications from non-expert and expert\nhuman raters and LLMs. Automated metrics show that LLMs generate stylistically\ncomplex stories, but tend to fall short in terms of novelty, surprise and\ndiversity when compared to average human writers. Expert ratings generally\ncoincide with automated metrics. However, LLMs and non-experts rate LLM stories\nto be more creative than human-generated stories. We discuss why and how these\ndifferences in ratings occur, and their implications for both human and\nartificial creativity.']"
39,88,39_draft_speculative_decoding_tokens,"['draft', 'speculative', 'decoding', 'tokens', 'speedup', 'inference', 'acceptance', 'sd', 'drafting', 'target']","['Deployment of autoregressive large language models (LLMs) is costly, and as\nthese models increase in size, the associated costs will become even more\nconsiderable. Consequently, different methods have been proposed to accelerate\nthe token generation process and reduce costs. Speculative decoding (SD) is\namong the most promising approaches to speed up the LLM decoding process by\nverifying multiple tokens in parallel and using an auxiliary smaller draft\nmodel to generate the possible tokens. In SD, usually, one draft model is used\nto serve a specific target model; however, in practice, LLMs are diverse, and\nwe might need to deal with many target models or more than one target model\nsimultaneously. In this scenario, it is not clear which draft model should be\nused for which target model, and searching among different draft models or\ntraining customized draft models can further increase deployment costs. In this\npaper, we first introduce a novel multi-target scenario for the deployment of\ndraft models for faster inference. Then, we present a novel, more efficient\nsorted speculative decoding mechanism that outperforms regular baselines in\nmulti-target settings. We evaluated our method on Spec-Bench in different\nsettings, including base models such as Vicuna 7B, 13B, and LLama Chat 70B. Our\nresults suggest that our draft models perform better than baselines for\nmultiple target models at the same time.', 'Inference acceleration of large language models (LLMs) has been put forward\nin many application scenarios and speculative decoding has shown its advantage\nin addressing inference acceleration. Speculative decoding usually introduces a\ndraft model to assist the base LLM where the draft model produces drafts and\nthe base LLM verifies the draft for acceptance or rejection. In this framework,\nthe final inference speed is decided by the decoding speed of the draft model\nand the acceptance rate of the draft provided by the draft model. Currently the\nwidely used draft models usually generate draft tokens for the next several\npositions in a non-autoregressive way without considering the correlations\nbetween draft tokens. Therefore, it has a high decoding speed but an\nunsatisfactory acceptance rate. In this paper, we focus on how to improve the\nperformance of the draft model and aim to accelerate inference via a high\nacceptance rate. To this end, we propose a CTC-based draft model which\nstrengthens the correlations between draft tokens during the draft phase,\nthereby generating higher-quality draft candidate sequences. Experiment results\nshow that compared to strong baselines, the proposed method can achieve a\nhigher acceptance rate and hence a faster inference speed.', ""Speculative decoding accelerates inference in large language models (LLMs) by\ngenerating draft tokens for target model verification. Current approaches for\nobtaining draft tokens rely on lightweight draft models or additional model\nstructures to generate draft tokens and retrieve context from databases. Due to\nthe draft model's small size and limited training data, model-based speculative\ndecoding frequently becomes less effective in out-of-domain scenarios.\nAdditionally, the time cost of the drafting phase results in a low upper limit\non acceptance length during the verification step, limiting overall efficiency.\nThis paper proposes RASD (Retrieval-Augmented Speculative Decoding), which\nadopts retrieval methods to enhance model-based speculative decoding. We\nintroduce tree pruning and tree fusion to achieve this. Specifically, we\ndevelop a pruning method based on the draft model's probability distribution to\nconstruct the optimal retrieval tree. Second, we employ the longest prefix\nmatching algorithm to merge the tree generated by the draft model with the\nretrieval tree, resulting in a unified tree for verification. Experimental\nresults demonstrate that RASD achieves state-of-the-art inference acceleration\nacross tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD\nexhibits strong scalability, seamlessly integrating with various speculative\ndecoding approaches, including both generation-based and retrieval-based\nmethods.""]"
40,86,40_summarization_summaries_summary_abstractive,"['summarization', 'summaries', 'summary', 'abstractive', 'extractive', 'text', 'information', 'keyphrase', 'keyphrases', 'documents']","['Large language models (LLMs) have shown remarkable capabilities in generating\nuser summaries from a long list of raw user activity data. These summaries\ncapture essential user information such as preferences and interests, and\ntherefore are invaluable for LLM-based personalization applications, such as\nexplainable recommender systems. However, the development of new summarization\ntechniques is hindered by the lack of ground-truth labels, the inherent\nsubjectivity of user summaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce \\UserSumBench, a\nbenchmark framework designed to facilitate iterative development of LLM-based\nsummarization approaches. This framework offers two key components: (1) A\nreference-free summary quality metric. We show that this metric is effective\nand aligned with human preferences across three diverse datasets (MovieLens,\nYelp and Amazon Review). (2) A novel robust summarization method that leverages\ntime-hierarchical summarizer and self-critique verifier to produce high-quality\nsummaries while eliminating hallucination. This method serves as a strong\nbaseline for further innovation in summarization techniques.', 'The increasing demand for efficient summarization tools in\nresource-constrained environments highlights the need for effective solutions.\nWhile large language models (LLMs) deliver superior summarization quality,\ntheir high computational resource requirements limit practical use\napplications. In contrast, small language models (SLMs) present a more\naccessible alternative, capable of real-time summarization on edge devices.\nHowever, their summarization capabilities and comparative performance against\nLLMs remain underexplored. This paper addresses this gap by presenting a\ncomprehensive evaluation of 19 SLMs for news summarization across 2,000 news\nsamples, focusing on relevance, coherence, factual consistency, and summary\nlength. Our findings reveal significant variations in SLM performance, with\ntop-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results\ncomparable to those of 70B LLMs while generating more concise summaries.\nNotably, SLMs are better suited for simple prompts, as overly complex prompts\nmay lead to a decline in summary quality. Additionally, our analysis indicates\nthat instruction tuning does not consistently enhance the news summarization\ncapabilities of SLMs. This research not only contributes to the understanding\nof SLMs but also provides practical insights for researchers seeking efficient\nsummarization solutions that balance performance and resource use.', 'Automatic summarization of legal case judgements, which are known to be long\nand complex, has traditionally been tried via extractive summarization models.\nIn recent years, generative models including abstractive summarization models\nand Large language models (LLMs) have gained huge popularity. In this paper, we\nexplore the applicability of such models for legal case judgement\nsummarization. We applied various domain specific abstractive summarization\nmodels and general domain LLMs as well as extractive summarization models over\ntwo sets of legal case judgements from the United Kingdom (UK) Supreme Court\nand the Indian (IN) Supreme Court and evaluated the quality of the generated\nsummaries. We also perform experiments on a third dataset of legal documents of\na different type, Government reports from the United States (US). Results show\nthat abstractive summarization models and LLMs generally perform better than\nthe extractive methods as per traditional metrics for evaluating summary\nquality. However, detailed investigation shows the presence of inconsistencies\nand hallucinations in the outputs of the generative models, and we explore ways\nto reduce the hallucinations and inconsistencies in the summaries. Overall, the\ninvestigation suggests that further improvements are needed to enhance the\nreliability of abstractive models and LLMs for legal case judgement\nsummarization. At present, a human-in-the-loop technique is more suitable for\nperforming manual checks to identify inconsistencies in the generated\nsummaries.']"
41,85,41_editing_knowledge_edits_edit,"['editing', 'knowledge', 'edits', 'edit', 'updates', 'edited', 'ke', 'multihop', 'lifelong', 'methods']","[""Model editing aims to correct outdated or erroneous knowledge in large\nlanguage models (LLMs) without the need for costly retraining. Lifelong model\nediting is the most challenging task that caters to the continuous editing\nrequirements of LLMs. Prior works primarily focus on single or batch editing;\nnevertheless, these methods fall short in lifelong editing scenarios due to\ncatastrophic knowledge forgetting and the degradation of model performance.\nAlthough retrieval-based methods alleviate these issues, they are impeded by\nslow and cumbersome processes of integrating the retrieved knowledge into the\nmodel. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous\nPrompt lEarning method, to boost editing efficacy and inference efficiency in\nlifelong learning. RECIPE first converts knowledge statements into short and\ninformative continuous prompts, prefixed to the LLM's input query embedding, to\nefficiently refine the response grounded on the knowledge. It further\nintegrates the Knowledge Sentinel (KS) that acts as an intermediary to\ncalculate a dynamic threshold, determining whether the retrieval repository\ncontains relevant knowledge. Our retriever and prompt encoder are jointly\ntrained to achieve editing properties, i.e., reliability, generality, and\nlocality. In our experiments, RECIPE is assessed extensively across multiple\nLLMs and editing datasets, where it achieves superior editing performance.\nRECIPE also demonstrates its capability to maintain the overall performance of\nLLMs alongside showcasing fast editing and inference speed."", 'Knowledge editing has become a promising approach for efficiently and\nprecisely updating knowledge embedded in large language models (LLMs). In this\nwork, we focus on Same-Subject Editing, which involves modifying multiple\nattributes of a single entity to ensure comprehensive and consistent updates to\nentity-centric knowledge. Through preliminary observation, we identify a\nsignificant challenge: Current state-of-the-art editing methods struggle when\ntasked with editing multiple related knowledge pieces for the same subject. To\naddress the lack of relevant editing data for identical subjects in traditional\nbenchmarks, we introduce the $\\text{S}^2\\text{RKE}$(Same-Subject Related\nKnowledge Editing) benchmark. Our extensive experiments reveal that only\nmainstream locate-then-edit methods, such as ROME and MEMIT, exhibit ""related\nknowledge perturbation,"" where subsequent edits interfere with earlier ones.\nFurther analysis reveals that these methods over-rely on subject information,\nneglecting other critical factors, resulting in reduced editing effectiveness.', 'This study presents a targeted model editing analysis focused on the latest\nlarge language model, Llama-3. We explore the efficacy of popular model editing\ntechniques - ROME, MEMIT, and EMMET, which are designed for precise layer\ninterventions. We identify the most effective layers for targeted edits through\nan evaluation that encompasses up to 4096 edits across three distinct\nstrategies: sequential editing, batch editing, and a hybrid approach we call as\nsequential-batch editing. Our findings indicate that increasing edit\nbatch-sizes may degrade model performance more significantly than using smaller\nedit batches sequentially for equal number of edits. With this, we argue that\nsequential model editing is an important component for scaling model editing\nmethods and future research should focus on methods that combine both batched\nand sequential editing. This observation suggests a potential limitation in\ncurrent model editing methods which push towards bigger edit batch sizes, and\nwe hope it paves way for future investigations into optimizing batch sizes and\nmodel editing performance.']"
42,83,42_moe_experts_routing_expert,"['moe', 'experts', 'routing', 'expert', 'mixtureofexperts', 'router', 'dense', 'inference', 'mixture', 'sparse']","['Neurons in large language models often exhibit \\emph{polysemanticity},\nsimultaneously encoding multiple unrelated concepts and obscuring\ninterpretability. Instead of relying on post-hoc methods, we present\n\\textbf{MoE-X}, a Mixture-of-Experts (MoE) language model designed to be\n\\emph{intrinsically} interpretable. Our approach is motivated by the\nobservation that, in language models, wider networks with sparse activations\nare more likely to capture interpretable factors. However, directly training\nsuch large sparse networks is computationally prohibitive. MoE architectures\noffer a scalable alternative by activating only a subset of experts for any\ngiven input, inherently aligning with interpretability objectives. In MoE-X, we\nestablish this connection by rewriting the MoE layer as an equivalent sparse,\nlarge MLP. This approach enables efficient scaling of the hidden size while\nmaintaining sparsity. To further enhance interpretability, we enforce sparse\nactivation within each expert and redesign the routing mechanism to prioritize\nexperts with the highest activation sparsity. These designs ensure that only\nthe most salient features are routed and processed by the experts. We evaluate\nMoE-X on chess and natural language tasks, showing that it achieves performance\ncomparable to dense models while significantly improving interpretability.\nMoE-X achieves a perplexity better than GPT-2, with interpretability surpassing\neven sparse autoencoder (SAE)-based approaches.', 'Mixture-of-Experts (MoE) models have become a key approach for scaling large\nlanguage models efficiently by activating only a subset of experts during\ntraining and inference. Typically, the number of activated experts presents a\ntrade-off: fewer experts reduce computational costs, while more experts improve\nperformance. Recent studies reveal that not all activated experts contribute\nequally to model performance, with some providing minimal utility, particularly\nwhen finetuning pretrained MoE models for specialized downstream tasks. The\nco-existence of significant and redundant parameters in experts provides us an\nopportunity to reduce the number of activated experts while maintaining model\nperformance. In this work, we propose the concept of compressed experts,\nlightweight modules that serve as compact representations of full experts. Our\napproach preserves the most important experts while replacing other auxiliary\nactivated experts with compressed experts. The reduction of active parameters\nsignificantly lowers inference costs while achieving comparable performance.\nExtensive experiments on models including Phi-MoE and OLMoE demonstrate that\ncompressed experts recover over 90% of full expert performance across various\ntasks while reducing more than 30% active parameters and saving 20% in\ninference costs. This approach enables efficient deployment of MoE models in\nresource-constrained settings and facilitates scaling to larger models with\nmanageable overhead. Our code is available at\nhttps://github.com/yifei-he/Compressed-Experts.', 'Mixture-of-experts (MoE) is gaining increasing attention due to its unique\nproperties and remarkable performance, especially for language tasks. By\nsparsely activating a subset of parameters for each token, MoE architecture\ncould increase the model size without sacrificing computational efficiency,\nachieving a better trade-off between performance and training costs. However,\nthe underlying mechanism of MoE still lacks further exploration, and its\nmodularization degree remains questionable. In this paper, we make an initial\nattempt to understand the inner workings of MoE-based large language models.\nConcretely, we comprehensively study the parametric and behavioral features of\nthree popular MoE-based models and reveal some intriguing observations,\nincluding 1) Neurons act like fine-grained experts; 2) The router of MoE\nusually selects experts with larger output norms; 3) The expert diversity\nincreases as the layer increases, while the last layer is an outlier, which is\nfurther validated by an initial experiment. Based on the observations, we also\nprovide suggestions for a broad spectrum of MoE practitioners, such as router\ndesign and expert allocation. We hope this work could shed light on future\nresearch on the MoE framework and other modular architectures. Code is\navailable at https://github.com/kamanphoebe/Look-into-MoEs.']"
43,82,43_series_time_forecasting_timeseries,"['series', 'time', 'forecasting', 'timeseries', 'tsf', 'data', 'load', 'multivariate', 'temporal', 'foundation']","['Large language models (LLMs) are being applied to time series forecasting.\nBut are language models actually useful for time series? In a series of\nablation studies on three recent and popular LLM-based time series forecasting\nmethods, we find that removing the LLM component or replacing it with a basic\nattention layer does not degrade forecasting performance -- in most cases, the\nresults even improve! We also find that despite their significant computational\ncost, pretrained LLMs do no better than models trained from scratch, do not\nrepresent the sequential dependencies in time series, and do not assist in\nfew-shot settings. Additionally, we explore time series encoders and find that\npatching and attention structures perform similarly to LLM-based forecasters.', 'Recent research has shown that large language models (LLMs) can be\neffectively used for real-world time series forecasting due to their strong\nnatural language understanding capabilities. However, aligning time series into\nsemantic spaces of LLMs comes with high computational costs and inference\ncomplexity, particularly for long-range time series generation. Building on\nrecent advancements in using linear models for time series, this paper\nintroduces an LLM-enhanced mixture of linear experts for precise and efficient\ntime series forecasting. This approach involves developing a mixture of linear\nexperts with multiple lookback lengths and a new multimodal fusion mechanism.\nThe use of a mixture of linear experts is efficient due to its simplicity,\nwhile the multimodal fusion mechanism adaptively combines multiple linear\nexperts based on the learned features of the text modality from pre-trained\nlarge language models. In experiments, we rethink the need to align time series\nto LLMs by existing time-series large language models and further discuss their\nefficiency and effectiveness in time series forecasting. Our experimental\nresults show that the proposed LeMoLE model presents lower prediction errors\nand higher computational efficiency than existing LLM models.', 'Time series forecasting has traditionally focused on univariate and\nmultivariate numerical data, often overlooking the benefits of incorporating\nmultimodal information, particularly textual data. In this paper, we propose a\nnovel framework that integrates time series models with Large Language Models\nto improve high-dimensional time series forecasting. Inspired by multimodal\nmodels, our method combines time series and textual data in the dual-tower\nstructure. This fusion of information creates a comprehensive representation,\nwhich is then processed through a linear layer to generate the final forecast.\nExtensive experiments demonstrate that incorporating text enhances\nhigh-dimensional time series forecasting performance. This work paves the way\nfor further research in multimodal time series forecasting.']"
44,80,44_gui_mobile_ui_agents,"['gui', 'mobile', 'ui', 'agents', 'agent', 'user', 'screen', 'interfaces', 'graphical', 'interaction']","['Nowadays, research on GUI agents is a hot topic in the AI community. However,\ncurrent research focuses on GUI task automation, limiting the scope of\napplications in various GUI scenarios. In this paper, we propose a formalized\nand comprehensive environment to evaluate the entire process of automated GUI\nTesting (GTArena), offering a fair, standardized environment for consistent\noperation of diverse multimodal large language models. We divide the testing\nprocess into three key subtasks: test intention generation, test task\nexecution, and GUI defect detection, and construct a benchmark dataset based on\nthese to conduct a comprehensive evaluation. It evaluates the performance of\ndifferent models using three data types: real mobile applications, mobile\napplications with artificially injected defects, and synthetic data, thoroughly\nassessing their capabilities in this relevant task. Additionally, we propose a\nmethod that helps researchers explore the correlation between the performance\nof multimodal language large models in specific scenarios and their general\ncapabilities in standard benchmark tests. Experimental results indicate that\neven the most advanced models struggle to perform well across all sub-tasks of\nautomated GUI Testing, highlighting a significant gap between the current\ncapabilities of Autonomous GUI Testing and its practical, real-world\napplicability. This gap provides guidance for the future direction of GUI Agent\ndevelopment. Our code is available at\nhttps://github.com/ZJU-ACES-ISE/ChatUITest.', 'Recently, Multimodal Large Language Models (MLLMs) have been used as agents\nto control keyboard and mouse inputs by directly perceiving the Graphical User\nInterface (GUI) and generating corresponding commands. However, current agents\nprimarily demonstrate strong understanding capabilities in static environments\nand are mainly applied to relatively simple domains, such as Web or mobile\ninterfaces. We argue that a robust GUI agent should be capable of perceiving\ntemporal information on the GUI, including dynamic Web content and multi-step\ntasks. Additionally, it should possess a comprehensive understanding of various\nGUI scenarios, including desktop software and multi-window interactions. To\nthis end, this paper introduces a new dataset, termed GUI-World, which features\nmeticulously crafted Human-MLLM annotations, extensively covering six GUI\nscenarios and eight types of GUI-oriented questions in three formats. We\nevaluate the capabilities of current state-of-the-art MLLMs, including Image\nLLMs and Video LLMs, in understanding various types of GUI content, especially\ndynamic and sequential content. Our findings reveal that current models\nstruggle with dynamic GUI content without manually annotated keyframes or\noperation history. On the other hand, Video LLMs fall short in all GUI-oriented\ntasks given the sparse GUI video dataset. Therefore, we take the initial step\nof leveraging a fine-tuned Video LLM, GUI-Vid, as a GUI-oriented assistant,\ndemonstrating an improved understanding of various GUI tasks. However, due to\nthe limitations in the performance of base LLMs, we conclude that using video\nLLMs as GUI agents remains a significant challenge. We believe our work\nprovides valuable insights for future research in dynamic GUI content\nunderstanding. All the dataset and code are publicly available at:\nhttps://gui-world.github.io.', 'GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.']"
45,75,45_sentiment_absa_sarcasm_analysis,"['sentiment', 'absa', 'sarcasm', 'analysis', 'aspectbased', 'irony', 'aspect', 'sentiments', 'varieties', 'polarity']","['Aspect-based sentiment analysis (ABSA) is a crucial task in information\nextraction and sentiment analysis, aiming to identify aspects with associated\nsentiment elements in text. However, existing ABSA datasets are predominantly\nEnglish-centric, limiting the scope for multilingual evaluation and research.\nTo bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7\ndomains and 21 languages, making it the most extensive multilingual parallel\ndataset for ABSA to date. Our primary focus is on triplet extraction, which\ninvolves identifying aspect terms, aspect categories, and sentiment polarities.\nThe dataset is constructed through an automatic translation process with human\nreview to ensure quality. We perform extensive experiments using various\nbaselines to assess performance and compatibility on M-ABSA. Our empirical\nfindings highlight that the dataset enables diverse evaluation tasks, such as\nmultilingual and multi-domain transfer learning, and large language model\nevaluation, underscoring its inclusivity and its potential to drive\nadvancements in multilingual ABSA research.', ""Large Language Models (LLMs) have recently displayed their extraordinary\ncapabilities in language understanding. However, how to comprehensively assess\nthe sentiment capabilities of LLMs continues to be a challenge. This paper\ninvestigates the ability of LLMs to detect and react to sentiment in text\nmodal. As the integration of LLMs into diverse applications is on the rise, it\nbecomes highly critical to comprehend their sensitivity to emotional tone, as\nit can influence the user experience and the efficacy of sentiment-driven\ntasks. We conduct a series of experiments to evaluate the performance of\nseveral prominent LLMs in identifying and responding appropriately to\nsentiments like positive, negative, and neutral emotions. The models' outputs\nare analyzed across various sentiment benchmarks, and their responses are\ncompared with human evaluations. Our discoveries indicate that although LLMs\nshow a basic sensitivity to sentiment, there are substantial variations in\ntheir accuracy and consistency, emphasizing the requirement for further\nenhancements in their training processes to better capture subtle emotional\ncues. Take an example in our findings, in some cases, the models might wrongly\nclassify a strongly positive sentiment as neutral, or fail to recognize sarcasm\nor irony in the text. Such misclassifications highlight the complexity of\nsentiment analysis and the areas where the models need to be refined. Another\naspect is that different LLMs might perform differently on the same set of\ndata, depending on their architecture and training datasets. This variance\ncalls for a more in-depth study of the factors that contribute to the\nperformance differences and how they can be optimized."", 'Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment\nanalysis that aims to extract and classify sentiments based on specific aspects\nor features of a product, service, or entity. Unlike traditional sentiment\nanalysis, which assigns a general sentiment score to entire reviews or texts,\nABSA focuses on breaking down the text into individual components or aspects\n(e.g., quality, price, service) and evaluating the sentiment towards each. This\nallows for a more granular level of understanding of customer opinions,\nenabling businesses to pinpoint specific areas of strength and improvement. The\nprocess involves several key steps, including aspect extraction, sentiment\nclassification, and aspect-level sentiment aggregation for a review paragraph\nor any other form that the users have provided. ABSA has significant\napplications in areas such as product reviews, social media monitoring,\ncustomer feedback analysis, and market research. By leveraging techniques from\nnatural language processing (NLP) and machine learning, ABSA facilitates the\nextraction of valuable insights, enabling companies to make data-driven\ndecisions that enhance customer satisfaction and optimize offerings. As ABSA\nevolves, it holds the potential to greatly improve personalized customer\nexperiences by providing a deeper understanding of sentiment across various\nproduct aspects. In this work, we have analyzed the strength of LLMs for a\ncomplete cross-domain aspect-based sentiment analysis with the aim of defining\nthe framework for certain products and using it for other similar situations.\nWe argue that it is possible to that at an effectiveness of 92\\% accuracy for\nthe Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12.']"
46,75,46_causal_discovery_variables_causality,"['causal', 'discovery', 'variables', 'causality', 'relationships', 'graphs', 'observational', 'counterfactual', 'reasoning', 'graph']","[""Recent advances in artificial intelligence have seen Large Language Models\n(LLMs) demonstrate notable proficiency in causal discovery tasks. This study\nexplores the factors influencing the performance of LLMs in causal discovery\ntasks. Utilizing open-source LLMs, we examine how the frequency of causal\nrelations within their pre-training corpora affects their ability to accurately\nrespond to causal discovery queries. Our findings reveal that a higher\nfrequency of causal mentions correlates with better model performance,\nsuggesting that extensive exposure to causal information during training\nenhances the models' causal discovery capabilities. Additionally, we\ninvestigate the impact of context on the validity of causal relations. Our\nresults indicate that LLMs might exhibit divergent predictions for identical\ncausal relations when presented in different contexts. This paper provides the\nfirst comprehensive analysis of how different factors contribute to LLM\nperformance in causal discovery tasks."", ""Large language models (LLMs) have achieved significant success across various\ndomains. However, the inherent complexity of causal problems and causal theory\nposes challenges in accurately describing them in natural language, making it\ndifficult for LLMs to comprehend and use them effectively. Causal methods are\nnot easily conveyed through natural language, which hinders LLMs' ability to\napply them accurately. Additionally, causal datasets are typically tabular,\nwhile LLMs excel in handling natural language data, creating a structural\nmismatch that impedes effective reasoning with tabular data. This lack of\ncausal reasoning capability limits the development of LLMs. To address these\nchallenges, we have equipped the LLM with causal tools within an agent\nframework, named the Causal Agent, enabling it to tackle causal problems. The\ncausal agent comprises tools, memory, and reasoning modules. In the tools\nmodule, the causal agent applies causal methods to align tabular data with\nnatural language. In the reasoning module, the causal agent employs the ReAct\nframework to perform reasoning through multiple iterations with the tools. In\nthe memory module, the causal agent maintains a dictionary instance where the\nkeys are unique names and the values are causal graphs. To verify the causal\nability of the causal agent, we established a benchmark consisting of four\nlevels of causal problems: variable level, edge level, causal graph level, and\ncausal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for\nthese four levels of issues and tested the causal agent on the datasets. Our\nmethodology demonstrates remarkable efficacy on the four-level causal problems,\nwith accuracy rates all above 80%. For further insights and implementation\ndetails, our code is accessible via the GitHub repository\nhttps://github.com/Kairong-Han/Causal_Agent."", ""Causal structure discovery methods are commonly applied to structured data\nwhere the causal variables are known and where statistical testing can be used\nto assess the causal relationships. By contrast, recovering a causal structure\nfrom unstructured natural language data such as news articles contains numerous\nchallenges due to the absence of known variables or counterfactual data to\nestimate the causal links. Large Language Models (LLMs) have shown promising\nresults in this direction but also exhibit limitations. This work investigates\nLLM's abilities to build causal graphs from text documents and perform\ncounterfactual causal inference. We propose an end-to-end causal structure\ndiscovery and causal inference method from natural language: we first use an\nLLM to extract the instantiated causal variables from text data and build a\ncausal graph. We merge causal graphs from multiple data sources to represent\nthe most exhaustive set of causes possible. We then conduct counterfactual\ninference on the estimated graph. The causal graph conditioning allows\nreduction of LLM biases and better represents the causal estimands. We use our\nmethod to show that the limitations of LLMs in counterfactual causal reasoning\ncome from prediction errors and propose directions to mitigate them. We\ndemonstrate the applicability of our method on real-world news articles.""]"
47,75,47_lora_lowrank_adaptation_finetuning,"['lora', 'lowrank', 'adaptation', 'finetuning', 'parameters', 'parameterefficient', 'trainable', 'matrices', 'peft', 'matrix']","[""Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning\nmethod for large language models. LoRA saves memory by training only low rank\nperturbations to selected weight matrices. In this work, we compare the\nperformance of LoRA and full finetuning on two target domains, programming and\nmathematics. We consider both the instruction finetuning (approximately 100K\nprompt-response pairs) and continued pretraining (20B unstructured tokens) data\nregimes. Our results show that, in the standard low-rank settings, LoRA\nsubstantially underperforms full finetuning. Nevertheless, LoRA better\nmaintains the base model's performance on tasks outside the target domain. We\nshow that LoRA mitigates forgetting more than common regularization techniques\nsuch as weight decay and dropout; it also helps maintain more diverse\ngenerations. Finally, we show that full finetuning learns perturbations with a\nrank that is 10-100X greater than typical LoRA configurations, possibly\nexplaining some of the reported gaps. We conclude by proposing best practices\nfor finetuning with LoRA."", 'As the adoption of large language models increases and the need for per-user\nor per-task model customization grows, the parameter-efficient fine-tuning\n(PEFT) methods, such as low-rank adaptation (LoRA) and its variants, incur\nsubstantial storage and transmission costs. To further reduce stored\nparameters, we introduce a ""divide-and-share"" paradigm that breaks the barriers\nof low-rank decomposition across matrix dimensions, modules, and layers by\nsharing parameters globally via a vector bank. As an instantiation of the\nparadigm to LoRA, our proposed VB-LoRA composites all the low-rank matrices of\nLoRA from a shared vector bank with a differentiable top-k admixture module.\nVB-LoRA achieves extreme parameter efficiency while maintaining comparable or\nbetter performance compared to state-of-the-art PEFT methods. Extensive\nexperiments demonstrate the effectiveness of VB-LoRA on natural language\nunderstanding, natural language generation, instruction tuning, and\nmathematical reasoning tasks. When fine-tuning the Llama2-13B model, VB-LoRA\nonly uses 0.4% of LoRA\'s stored parameters, yet achieves superior results. Our\nsource code is available at https://github.com/leo-yangli/VB-LoRA. This method\nhas been merged into the Hugging Face PEFT package.', 'The rapid advancements in large language models (LLMs) have revolutionized\nnatural language processing, creating an increased need for efficient,\ntask-specific fine-tuning methods. Traditional fine-tuning of LLMs involves\nupdating a large number of parameters, which is computationally expensive and\nmemory-intensive. Low-Rank Adaptation (LoRA) has emerged as a promising\nsolution, enabling parameter-efficient fine-tuning by reducing the number of\ntrainable parameters. However, while LoRA reduces the number of trainable\nparameters, LoRA modules still create significant storage challenges. We\npropose LoRA-Mini, an optimized adaptation of LoRA that improves parameter\nefficiency by splitting low-rank matrices into four parts, with only the two\ninner matrices being trainable. This approach achieves upto a 20x reduction\ncompared to standard LoRA in the number of trainable parameters while\npreserving performance levels comparable to standard LoRA, addressing both\ncomputational and storage efficiency in LLM fine-tuning.']"
48,74,48_backdoor_attacks_attack_triggers,"['backdoor', 'attacks', 'attack', 'triggers', 'trigger', 'injection', 'poisoning', 'backdoors', 'defense', 'malicious']","['Large Language Models (LLMs), which bridge the gap between human language\nunderstanding and complex problem-solving, achieve state-of-the-art performance\non several NLP tasks, particularly in few-shot and zero-shot settings. Despite\nthe demonstrable efficacy of LLMs, due to constraints on computational\nresources, users have to engage with open-source language models or outsource\nthe entire training process to third-party platforms. However, research has\ndemonstrated that language models are susceptible to potential security\nvulnerabilities, particularly in backdoor attacks. Backdoor attacks are\ndesigned to introduce targeted vulnerabilities into language models by\npoisoning training samples or model weights, allowing attackers to manipulate\nmodel responses through malicious triggers. While existing surveys on backdoor\nattacks provide a comprehensive overview, they lack an in-depth examination of\nbackdoor attacks specifically targeting LLMs. To bridge this gap and grasp the\nlatest trends in the field, this paper presents a novel perspective on backdoor\nattacks for LLMs by focusing on fine-tuning methods. Specifically, we\nsystematically classify backdoor attacks into three categories: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on\ninsights from a substantial review, we also discuss crucial issues for future\nresearch on backdoor attacks, such as further exploring attack algorithms that\ndo not require fine-tuning, or developing more covert attack algorithms.', 'Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on feature alignment-enhanced knowledge\ndistillation (W2SAttack). Specifically, we poison small-scale language models\nthrough full-parameter fine-tuning to serve as the teacher model. The teacher\nmodel then covertly transfers the backdoor to the large-scale student model\nthrough feature alignment-enhanced knowledge distillation, which employs PEFT.\nTheoretical analysis reveals that W2SAttack has the potential to augment the\neffectiveness of backdoor attacks. We demonstrate the superior performance of\nW2SAttack on classification tasks across four language models, four backdoor\nattack algorithms, and two different architectures of teacher models.\nExperimental results indicate success rates close to 100% for backdoor attacks\ntargeting PEFT.', ""Large language models (LLMs) have seen significant advancements, achieving\nsuperior performance in various Natural Language Processing (NLP) tasks, from\nunderstanding to reasoning. However, they remain vulnerable to backdoor\nattacks, where models behave normally for standard queries but generate harmful\nresponses or unintended output when specific triggers are activated. Existing\nbackdoor defenses often suffer from drawbacks that they either focus on\ndetection without removal, rely on rigid assumptions about trigger properties,\nor prove to be ineffective against advanced attacks like multi-trigger\nbackdoors. In this paper, we present a novel method to eliminate backdoor\nbehaviors from LLMs through the construction of information conflicts using\nboth internal and external mechanisms. Internally, we leverage a lightweight\ndataset to train a conflict model, which is then merged with the backdoored\nmodel to neutralize malicious behaviors by embedding contradictory information\nwithin the model's parametric memory. Externally, we incorporate convincing\ncontradictory evidence into the prompt to challenge the model's internal\nbackdoor knowledge. Experimental results on classification and conversational\ntasks across 4 widely used LLMs demonstrate that our method outperforms 8\nstate-of-the-art backdoor defense baselines. We can reduce the attack success\nrate of advanced backdoor attacks by up to 98% while maintaining over 90% clean\ndata accuracy. Furthermore, our method has proven to be robust against adaptive\nbackdoor attacks. The code will be open-sourced upon publication.""]"
49,74,49_icl_demonstrations_incontext_demonstration,"['icl', 'demonstrations', 'incontext', 'demonstration', 'examples', 'learning', 'manyshot', 'selection', 'fewshot', 'tasks']","[""In-context Learning (ICL) has emerged as a powerful capability alongside the\ndevelopment of scaled-up large language models (LLMs). By instructing LLMs\nusing few-shot demonstrative examples, ICL enables them to perform a wide range\nof tasks without updating millions of parameters. However, the precise\ncontributions of demonstrations towards improving end-task performance have not\nbeen thoroughly investigated in recent analytical studies. In this paper, we\nempirically decompose the overall performance of ICL into three dimensions,\nlabel space, format, and discrimination, and we evaluate four general-purpose\nLLMs across a diverse range of tasks. Counter-intuitively, we find that the\ndemonstrations have a marginal impact on provoking discriminative knowledge of\nlanguage models. However, ICL exhibits significant efficacy in regulating the\nlabel space and format, which helps LLMs respond to desired label words. We\nthen demonstrate that this ability functions similar to detailed instructions\nfor LLMs to follow. We additionally provide an in-depth analysis of the\nmechanism of retrieval helping with ICL. Our findings demonstrate that\nretrieving the semantically similar examples notably boosts the model's\ndiscriminative capability. However, we also observe a trade-off in selecting\ngood in-context examples regarding label diversity."", 'In-context Learning (ICL) is an emerging few-shot learning paradigm on\nLanguage Models (LMs) with inner mechanisms un-explored. There are already\nexisting works describing the inner processing of ICL, while they struggle to\ncapture all the inference phenomena in large language models. Therefore, this\npaper proposes a comprehensive circuit to model the inference dynamics and try\nto explain the observed phenomena of ICL. In detail, we divide ICL inference\ninto 3 major operations: (1) Input Text Encode: LMs encode every input text (in\nthe demonstrations and queries) into linear representation in the hidden states\nwith sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge\nthe encoded representations of demonstrations with their corresponding label\ntokens to produce joint representations of labels and demonstrations. (3)\nFeature Retrieval and Copy: LMs search the joint representations of\ndemonstrations similar to the query representation on a task subspace, and copy\nthe searched representations into the query. Then, language model heads capture\nthese copied label representations to a certain extent and decode them into\npredicted labels. Through careful measurements, the proposed inference circuit\nsuccessfully captures and unifies many fragmented phenomena observed during the\nICL process, making it a comprehensive and practical explanation of the ICL\ninference process. Moreover, ablation analysis by disabling the proposed steps\nseriously damages the ICL performance, suggesting the proposed inference\ncircuit is a dominating mechanism. Additionally, we confirm and list some\nbypass mechanisms that solve ICL tasks in parallel with the proposed circuit.', ""Large language models (LLMs) exhibit remarkable in-context learning (ICL)\ncapabilities. However, the underlying working mechanism of ICL remains poorly\nunderstood. Recent research presents two conflicting views on ICL: One\nemphasizes the impact of similar examples in the demonstrations, stressing the\nneed for label correctness and more shots. The other attributes it to LLMs'\ninherent ability of task recognition, deeming label correctness and shot\nnumbers of demonstrations as not crucial. In this work, we provide a\nTwo-Dimensional Coordinate System that unifies both views into a systematic\nframework. The framework explains the behavior of ICL through two orthogonal\nvariables: whether similar examples are presented in the demonstrations\n(perception) and whether LLMs can recognize the task (cognition). We propose\nthe peak inverse rank metric to detect the task recognition ability of LLMs and\nstudy LLMs' reactions to different definitions of similarity. Based on these,\nwe conduct extensive experiments to elucidate how ICL functions across each\nquadrant on multiple representative classification tasks. Finally, we extend\nour analyses to generation tasks, showing that our coordinate system can also\nbe used to interpret ICL for generation tasks effectively.""]"
50,71,50_adversarial_safety_attacks_mllms,"['adversarial', 'safety', 'attacks', 'mllms', 'attack', 'vlms', 'multimodal', 'harmful', 'visual', 'transferability']","['Video-based multimodal large language models (V-MLLMs) have shown\nvulnerability to adversarial examples in video-text multimodal tasks. However,\nthe transferability of adversarial videos to unseen models--a common and\npractical real world scenario--remains unexplored. In this paper, we pioneer an\ninvestigation into the transferability of adversarial video samples across\nV-MLLMs. We find that existing adversarial attack methods face significant\nlimitations when applied in black-box settings for V-MLLMs, which we attribute\nto the following shortcomings: (1) lacking generalization in perturbing video\nfeatures, (2) focusing only on sparse key-frames, and (3) failing to integrate\nmultimodal information. To address these limitations and deepen the\nunderstanding of V-MLLM vulnerabilities in black-box scenarios, we introduce\nthe Image-to-Video MLLM (I2V-MLLM) attack. In I2V-MLLM, we utilize an\nimage-based multimodal model (IMM) as a surrogate model to craft adversarial\nvideo samples. Multimodal interactions and temporal information are integrated\nto disrupt video representations within the latent space, improving adversarial\ntransferability. In addition, a perturbation propagation technique is\nintroduced to handle different unknown frame sampling strategies. Experimental\nresults demonstrate that our method can generate adversarial examples that\nexhibit strong transferability across different V-MLLMs on multiple video-text\nmultimodal tasks. Compared to white-box attacks on these models, our black-box\nattacks (using BLIP-2 as surrogate model) achieve competitive performance, with\naverage attack success rates of 55.48% on MSVD-QA and 58.26% on MSRVTT-QA for\nVideoQA tasks, respectively. Our code will be released upon acceptance.', 'Multimodal Large Language Models (MLLMs) have demonstrated exceptional\nperformance in artificial intelligence by facilitating integrated understanding\nacross diverse modalities, including text, images, video, audio, and speech.\nHowever, their deployment in real-world applications raises significant\nconcerns about adversarial vulnerabilities that could compromise their safety\nand reliability. Unlike unimodal models, MLLMs face unique challenges due to\nthe interdependencies among modalities, making them susceptible to\nmodality-specific threats and cross-modal adversarial manipulations. This paper\nreviews the adversarial robustness of MLLMs, covering different modalities. We\nbegin with an overview of MLLMs and a taxonomy of adversarial attacks tailored\nto each modality. Next, we review key datasets and evaluation metrics used to\nassess the robustness of MLLMs. After that, we provide an in-depth review of\nattacks targeting MLLMs across different modalities. Our survey also identifies\ncritical challenges and suggests promising future research directions.', 'Vision-language models (VLMs) have significantly advanced autonomous driving\n(AD) by enhancing reasoning capabilities. However, these models remain highly\nvulnerable to adversarial attacks. While existing research has primarily\nfocused on general VLM attacks, the development of attacks tailored to the\nsafety-critical AD context has been largely overlooked. In this paper, we take\nthe first step toward designing adversarial attacks specifically targeting VLMs\nin AD, exposing the substantial risks these attacks pose within this critical\ndomain. We identify two unique challenges for effective adversarial attacks on\nAD VLMs: the variability of textual instructions and the time-series nature of\nvisual scenarios. To this end, we propose ADvLM, the first visual adversarial\nattack framework specifically designed for VLMs in AD. Our framework introduces\nSemantic-Invariant Induction, which uses a large language model to create a\ndiverse prompt library of textual instructions with consistent semantic\ncontent, guided by semantic entropy. Building on this, we introduce\nScenario-Associated Enhancement, an approach where attention mechanisms select\nkey frames and perspectives within driving scenarios to optimize adversarial\nperturbations that generalize across the entire scenario. Extensive experiments\non several AD VLMs over multiple benchmarks show that ADvLM achieves\nstate-of-the-art attack effectiveness. Moreover, real-world attack studies\nfurther validate its applicability and potential in practice.']"
51,68,51_watermarking_watermark_watermarks_text,"['watermarking', 'watermark', 'watermarks', 'text', 'watermarked', 'detection', 'steganography', 'robustness', 'attacks', 'embedding']","['In recent years, LLM watermarking has emerged as an attractive safeguard\nagainst AI-generated content, with promising applications in many real-world\ndomains. However, there are growing concerns that the current LLM watermarking\nschemes are vulnerable to expert adversaries wishing to reverse-engineer the\nwatermarking mechanisms. Prior work in ""breaking"" or ""stealing"" LLM watermarks\nmainly focuses on the distribution-modifying algorithm of Kirchenbauer et al.\n(2023), which perturbs the logit vector before sampling. In this work, we focus\non reverse-engineering the other prominent LLM watermarking scheme,\ndistortion-free watermarking (Kuditipudi et al. 2024), which preserves the\nunderlying token distribution by using a hidden watermarking key sequence. We\ndemonstrate that, even under a more sophisticated watermarking scheme, it is\npossible to ""compromise"" the LLM and carry out a ""spoofing"" attack.\nSpecifically, we propose a mixed integer linear programming framework that\naccurately estimates the secret key used for watermarking using only a few\nsamples of the watermarked dataset. Our initial findings challenge the current\ntheoretical claims on the robustness and usability of existing LLM watermarking\ntechniques.', 'Text watermarking for Large Language Models (LLMs) has made significant\nprogress in detecting LLM outputs and preventing misuse. Current watermarking\ntechniques offer high detectability, minimal impact on text quality, and\nrobustness to text editing. However, current researches lack investigation into\nthe imperceptibility of watermarking techniques in LLM services. This is\ncrucial as LLM providers may not want to disclose the presence of watermarks in\nreal-world scenarios, as it could reduce user willingness to use the service\nand make watermarks more vulnerable to attacks. This work is the first to\ninvestigate the imperceptibility of watermarked LLMs. We design an\nidentification algorithm called Water-Probe that detects watermarks through\nwell-designed prompts to the LLM. Our key motivation is that current\nwatermarked LLMs expose consistent biases under the same watermark key,\nresulting in similar differences across prompts under different watermark keys.\nExperiments show that almost all mainstream watermarking algorithms are easily\nidentified with our well-designed prompts, while Water-Probe demonstrates a\nminimal false positive rate for non-watermarked LLMs. Finally, we propose that\nthe key to enhancing the imperceptibility of watermarked LLMs is to increase\nthe randomness of watermark key selection. Based on this, we introduce the\nWater-Bag strategy, which significantly improves watermark imperceptibility by\nmerging multiple watermark keys.', 'Text watermarks in large language models (LLMs) are increasingly used to\ndetect synthetic text, mitigating misuse cases like fake news and academic\ndishonesty. While existing watermarking detection techniques primarily focus on\nclassifying entire documents as watermarked or not, they often neglect the\ncommon scenario of identifying individual watermark segments within longer,\nmixed-source documents. Drawing inspiration from plagiarism detection systems,\nwe propose two novel methods for partial watermark detection. First, we develop\na geometry cover detection framework aimed at determining whether there is a\nwatermark segment in long text. Second, we introduce an adaptive online\nlearning algorithm to pinpoint the precise location of watermark segments\nwithin the text. Evaluated on three popular watermarking techniques\n(KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves\nhigh accuracy, significantly outperforming baseline methods. Moreover, our\nframework is adaptable to other watermarking techniques, offering new insights\nfor precise watermark detection.']"
52,67,52_hate_speech_detection_content,"['hate', 'speech', 'detection', 'content', 'counterspeech', 'moderation', 'media', 'toxic', 'social', 'offensive']","['The issue of hate speech extends beyond the confines of the online realm. It\nis a problem with real-life repercussions, prompting most nations to formulate\nlegal frameworks that classify hate speech as a punishable offence. These legal\nframeworks differ from one country to another, contributing to the big chaos\nthat online platforms have to face when addressing reported instances of hate\nspeech. With the definitions of hate speech falling short in introducing a\nrobust framework, we turn our gaze onto hate speech laws. We consult the\nopinion of legal experts on a hate speech dataset and we experiment by\nemploying various approaches such as pretrained models both on hate speech and\nlegal data, as well as exploiting two large language models (Qwen2-7B-Instruct\nand Meta-Llama-3-70B). Due to the time-consuming nature of data acquisition for\nprosecutable hate speech, we use pseudo-labeling to improve our pretrained\nmodels. This study highlights the importance of amplifying research on\nprosecutable hate speech and provides insights into effective strategies for\ncombating hate speech within the parameters of legal frameworks. Our findings\nshow that legal knowledge in the form of annotations can be useful when\nclassifying prosecutable hate speech, yet more focus should be paid on the\ndifferences between the laws.', 'Large Language Models (LLMs) have raised increasing concerns about their\nmisuse in generating hate speech. Among all the efforts to address this issue,\nhate speech detectors play a crucial role. However, the effectiveness of\ndifferent detectors against LLM-generated hate speech remains largely unknown.\nIn this paper, we propose HateBench, a framework for benchmarking hate speech\ndetectors on LLM-generated hate speech. We first construct a hate speech\ndataset of 7,838 samples generated by six widely-used LLMs covering 34 identity\ngroups, with meticulous annotations by three labelers. We then assess the\neffectiveness of eight representative hate speech detectors on the\nLLM-generated dataset. Our results show that while detectors are generally\neffective in identifying LLM-generated hate speech, their performance degrades\nwith newer versions of LLMs. We also reveal the potential of LLM-driven hate\ncampaigns, a new threat that LLMs bring to the field of hate speech detection.\nBy leveraging advanced techniques like adversarial attacks and model stealing\nattacks, the adversary can intentionally evade the detector and automate hate\ncampaigns online. The most potent adversarial attack achieves an attack success\nrate of 0.966, and its attack efficiency can be further improved by\n$13-21\\times$ through model stealing attacks with acceptable attack\nperformance. We hope our study can serve as a call to action for the research\ncommunity and platform moderators to fortify defenses against these emerging\nthreats.', ""Hate speech is a harmful form of online expression, often manifesting as\nderogatory posts. It is a significant risk in digital environments. With the\nrise of Large Language Models (LLMs), there is concern about their potential to\nreplicate hate speech patterns, given their training on vast amounts of\nunmoderated internet data. Understanding how LLMs respond to hate speech is\ncrucial for their responsible deployment. However, the behaviour of LLMs\ntowards hate speech has been limited compared. This paper investigates the\nreactions of seven state-of-the-art LLMs (LLaMA 2, Vicuna, LLaMA 3, Mistral,\nGPT-3.5, GPT-4, and Gemini Pro) to hate speech. Through qualitative analysis,\nwe aim to reveal the spectrum of responses these models produce, highlighting\ntheir capacity to handle hate speech inputs. We also discuss strategies to\nmitigate hate speech generation by LLMs, particularly through fine-tuning and\nguideline guardrailing. Finally, we explore the models' responses to hate\nspeech framed in politically correct language.""]"
53,66,53_graph_gnns_graphs_node,"['graph', 'gnns', 'graphs', 'node', 'gnn', 'nodes', 'textattributed', 'neural', 'networks', 'tags']","[""Graph learning has attracted significant attention due to its widespread\nreal-world applications. Current mainstream approaches rely on text node\nfeatures and obtain initial node embeddings through shallow embedding learning\nusing GNNs, which shows limitations in capturing deep textual semantics. Recent\nadvances in Large Language Models (LLMs) have demonstrated superior\ncapabilities in understanding text semantics, transforming traditional text\nfeature processing. This paper proposes a novel framework that combines Graph\nTransformer architecture with LLM-enhanced node features. Specifically, we\nleverage LLMs to generate rich semantic representations of text nodes, which\nare then processed by a multi-head self-attention mechanism in the Graph\nTransformer to capture both local and global graph structural information. Our\nmodel utilizes the Transformer's attention mechanism to dynamically aggregate\nneighborhood information while preserving the semantic richness provided by LLM\nembeddings. Experimental results demonstrate that the LLM-enhanced node\nfeatures significantly improve the performance of graph learning models on node\nclassification tasks. This approach shows promising results across multiple\ngraph learning tasks, offering a practical direction for combining graph\nnetworks with language models."", 'Graph mining is an important area in data mining and machine learning that\ninvolves extracting valuable information from graph-structured data. In recent\nyears, significant progress has been made in this field through the development\nof graph neural networks (GNNs). However, GNNs are still deficient in\ngeneralizing to diverse graph data. Aiming to this issue, Large Language Models\n(LLMs) could provide new solutions for graph mining tasks with their superior\nsemantic understanding. In this review, we systematically review the\ncombination and application techniques of LLMs and GNNs and present a novel\ntaxonomy for research in this interdisciplinary field, which involves three\nmain categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.\nWithin this framework, we reveal the capabilities of LLMs in enhancing graph\nfeature extraction as well as improving the effectiveness of downstream tasks\nsuch as node classification, link prediction, and community detection. Although\nLLMs have demonstrated their great potential in handling graph-structured data,\ntheir high computational requirements and complexity remain challenges. Future\nresearch needs to continue to explore how to efficiently fuse LLMs and GNNs to\nachieve more powerful graph learning and reasoning capabilities and provide new\nimpetus for the development of graph mining techniques.', 'Graphs are a fundamental data structure for representing relationships in\nreal-world scenarios. With the success of Large Language Models (LLMs) across\nvarious natural language processing (NLP) tasks, there has been growing\ninterest in integrating LLMs for graph learning. However, applying LLMs to\ngraph-related tasks poses significant challenges, as these models are not\ninherently designed to capture the complex structural information present in\ngraphs. Existing approaches address this challenge through two strategies: the\nchain of tasks approach, which uses Graph Neural Networks (GNNs) to encode the\ngraph structure so that LLMs are relieved from understanding spatial positions;\nand Graph-to-Text Conversion, which translates graph structures into semantic\ntext representations that LLMs can process. Despite their progress, these\nmethods often struggle to fully preserve the topological information of graphs\nor require extensive computational resources, limiting their practical\napplicability.\n  In this work, we introduce Node Tokenizer for Large Language Models (NT-LLM),\na novel framework that efficiently encodes graph structures by selecting key\nnodes as anchors and representing each node based on its relative distance to\nthese anchors. This position-anchored encoding effectively captures the graph\ntopology, enabling enhanced reasoning capabilities in LLMs over graph data.\nAdditionally, we implement a task-specific tuning procedure to further improve\nstructural understanding within LLMs. Through extensive empirical evaluations,\nNT-LLM demonstrates significant performance improvements across a variety of\ngraph-related tasks.']"
54,65,54_federated_fl_clients_client,"['federated', 'fl', 'clients', 'client', 'finetuning', 'lora', 'communication', 'server', 'heterogeneity', 'aggregation']","['Significant advancements have been made by Large Language Models (LLMs) in\nthe domains of natural language understanding and automated content creation.\nHowever, they still face persistent problems, including substantial\ncomputational costs and inadequate availability of training data. The\ncombination of Federated Learning (FL) and LLMs (federated LLMs) offers a\nsolution by leveraging distributed data while protecting privacy, which\npositions it as an ideal choice for sensitive domains. However, Federated LLMs\nstill suffer from robustness challenges, including data heterogeneity,\nmalicious clients, and adversarial attacks, which greatly hinder their\napplications. We first introduce the robustness problems in federated LLMs, to\naddress these challenges, we propose FedEAT (Federated Embedding space\nAdversarial Training), a novel framework that applies adversarial training in\nthe embedding space of client LLM and employs a robust aggregation approach,\nspecifically geometric median aggregation, to enhance the robustness of\nFederated LLMs. Our experiments demonstrate that FedEAT effectively improves\nthe robustness of Federated LLMs with minimal performance loss.', 'Federated Learning (FL) is a recent model training paradigm in which client\ndevices collaboratively train a model without ever aggregating their data.\nCrucially, this scheme offers users potential privacy and security benefits by\nonly ever communicating updates to the model weights to a central server as\nopposed to traditional machine learning (ML) training which directly\ncommunicates and aggregates data. However, FL training suffers from statistical\nheterogeneity as clients may have differing local data distributions. Large\nlanguage models (LLMs) offer a potential solution to this issue of\nheterogeneity given that they have consistently been shown to be able to learn\non vast amounts of noisy data. While LLMs are a promising development for\nresolving the consistent issue of non-I.I.D. Clients in federated settings\nexacerbate two other bottlenecks in FL: limited local computing and expensive\ncommunication. This thesis aims to develop efficient training methods for LLMs\nin FL. To this end, we employ two critical techniques in enabling efficient\ntraining. First, we use low-rank adaptation (LoRA) to reduce the computational\nload of local model training. Second, we communicate sparse updates throughout\ntraining to significantly cut down on communication costs. Taken together, our\nmethod reduces communication costs by up to 10x over vanilla LoRA and up to 5x\nover more complex sparse LoRA baselines while achieving greater utility. We\nemphasize the importance of carefully applying sparsity and picking effective\nrank and sparsity configurations for federated LLM training.', 'Large language models are rapidly gaining popularity and have been widely\nadopted in real-world applications. While the quality of training data is\nessential, privacy concerns arise during data collection. Federated learning\noffers a solution by allowing multiple clients to collaboratively train LLMs\nwithout sharing local data. However, FL introduces new challenges, such as\nmodel convergence issues due to heterogeneous data and high communication\ncosts. A comprehensive study is required to address these challenges and guide\nfuture research. This paper surveys Federated learning for LLMs (FedLLM),\nhighlighting recent advances and future directions. We focus on two key\naspects: fine-tuning and prompt learning in a federated setting, discussing\nexisting work and associated research challenges. We finally propose potential\nresearch directions for federated LLMs, including pre-training and how LLMs can\nfurther enhance federated learning.']"
55,63,55_social_agents_simulations_simulation,"['social', 'agents', 'simulations', 'simulation', 'behavior', 'economic', 'cooperation', 'dynamics', 'society', 'human']","[""Understanding human behavior and society is a central focus in social\nsciences, with the rise of generative social science marking a significant\nparadigmatic shift. By leveraging bottom-up simulations, it replaces costly and\nlogistically challenging traditional experiments with scalable, replicable, and\nsystematic computational approaches for studying complex social dynamics.\nRecent advances in large language models (LLMs) have further transformed this\nresearch paradigm, enabling the creation of human-like generative social agents\nand realistic simulacra of society. In this paper, we propose AgentSociety, a\nlarge-scale social simulator that integrates LLM-driven agents, a realistic\nsocietal environment, and a powerful large-scale simulation engine. Based on\nthe proposed simulator, we generate social lives for over 10k agents,\nsimulating their 5 million interactions both among agents and between agents\nand their environment. Furthermore, we explore the potential of AgentSociety as\na testbed for computational social experiments, focusing on four key social\nissues: polarization, the spread of inflammatory messages, the effects of\nuniversal basic income policies, and the impact of external shocks such as\nhurricanes. These four issues serve as valuable cases for assessing\nAgentSociety's support for typical research methods -- such as surveys,\ninterviews, and interventions -- as well as for investigating the patterns,\ncauses, and underlying mechanisms of social issues. The alignment between\nAgentSociety's outcomes and real-world experimental results not only\ndemonstrates its ability to capture human behaviors and their underlying\nmechanisms, but also underscores its potential as an important platform for\nsocial scientists and policymakers."", 'The emergence of Large Language Models (LLMs) and advancements in Artificial\nIntelligence (AI) offer an opportunity for computational social science\nresearch at scale. Building upon prior explorations of LLM agent design, our\nwork introduces a simulated agent society where complex social relationships\ndynamically form and evolve over time. Agents are imbued with psychological\ndrives and placed in a sandbox survival environment. We conduct an evaluation\nof the agent society through the lens of Thomas Hobbes\'s seminal Social\nContract Theory (SCT). We analyze whether, as the theory postulates, agents\nseek to escape a brutish ""state of nature"" by surrendering rights to an\nabsolute sovereign in exchange for order and security. Our experiments unveil\nan alignment: Initially, agents engage in unrestrained conflict, mirroring\nHobbes\'s depiction of the state of nature. However, as the simulation\nprogresses, social contracts emerge, leading to the authorization of an\nabsolute sovereign and the establishment of a peaceful commonwealth founded on\nmutual cooperation. This congruence between our LLM agent society\'s\nevolutionary trajectory and Hobbes\'s theoretical account indicates LLMs\'\ncapability to model intricate social dynamics and potentially replicate forces\nthat shape human societies. By enabling such insights into group behavior and\nemergent societal phenomena, LLM-driven multi-agent simulations, while unable\nto simulate all the nuances of human behavior, may hold potential for advancing\nour understanding of social structures, group dynamics, and complex human\nsystems.', ""Large language models (LLMs) are increasingly used to model human social\nbehavior, with recent research exploring their ability to simulate social\ndynamics. Here, we test whether LLMs mirror human behavior in social dilemmas,\nwhere individual and collective interests conflict. Humans generally cooperate\nmore than expected in laboratory settings, showing less cooperation in\nwell-mixed populations but more in fixed networks. In contrast, LLMs tend to\nexhibit greater cooperation in well-mixed settings. This raises a key question:\nAre LLMs about to emulate human behavior in cooperative dilemmas on networks?\nIn this study, we examine networked interactions where agents repeatedly engage\nin the Prisoner's Dilemma within both well-mixed and structured network\nconfigurations, aiming to identify parallels in cooperative behavior between\nLLMs and humans. Our findings indicate critical distinctions: while humans tend\nto cooperate more within structured networks, LLMs display increased\ncooperation mainly in well-mixed environments, with limited adjustment to\nnetworked contexts. Notably, LLM cooperation also varies across model types,\nillustrating the complexities of replicating human-like social adaptability in\nartificial agents. These results highlight a crucial gap: LLMs struggle to\nemulate the nuanced, adaptive social strategies humans deploy in fixed\nnetworks. Unlike human participants, LLMs do not alter their cooperative\nbehavior in response to network structures or evolving social contexts, missing\nthe reciprocity norms that humans adaptively employ. This limitation points to\na fundamental need in future LLM design -- to integrate a deeper comprehension\nof social norms, enabling more authentic modeling of human-like cooperation and\nadaptability in networked environments.""]"
56,62,56_synthetic_data_augmentation_active,"['synthetic', 'data', 'augmentation', 'active', 'classification', 'diversity', 'label', 'learning', 'text', 'performance']","[""Supervised machine learning methods require large-scale training datasets to\nperform well in practice. Synthetic data has been showing great progress\nrecently and has been used as a complement to real data. However, there is yet\na great urge to assess the usability of synthetically generated data. To this\nend, we propose a novel UCB-based training procedure combined with a dynamic\nusability metric. Our proposed metric integrates low-level and high-level\ninformation from synthetic images and their corresponding real and synthetic\ndatasets, surpassing existing traditional metrics. By utilizing a UCB-based\ndynamic approach ensures continual enhancement of model learning. Unlike other\napproaches, our method effectively adapts to changes in the machine learning\nmodel's state and considers the evolving utility of training samples during the\ntraining process. We show that our metric is an effective way to rank synthetic\nimages based on their usability. Furthermore, we propose a new attribute-aware\nbandit pipeline for generating synthetic data by integrating a Large Language\nModel with Stable Diffusion. Quantitative results show that our approach can\nboost the performance of a wide range of supervised classifiers. Notably, we\nobserved an improvement of up to 10% in classification accuracy compared to\ntraditional approaches, demonstrating the effectiveness of our approach. Our\nsource code, datasets, and additional materials are publically available at\nhttps://github.com/A-Kerim/Synthetic-Data-Usability-2024."", ""Synthetic data has been proposed as a solution to address the issue of\nhigh-quality data scarcity in the training of large language models (LLMs).\nStudies have shown that synthetic data can effectively improve the performance\nof LLMs on downstream benchmarks. However, despite its potential benefits, our\nanalysis suggests that there may be inherent flaws in synthetic data. The\nuniform format of synthetic data can lead to pattern overfitting and cause\nsignificant shifts in the output distribution, thereby reducing the model's\ninstruction-following capabilities. Our work delves into these specific flaws\nassociated with question-answer (Q-A) pairs, a prevalent type of synthetic\ndata, and presents a method based on unlearning techniques to mitigate these\nflaws. The empirical results demonstrate the effectiveness of our approach,\nwhich can reverse the instruction-following issues caused by pattern\noverfitting without compromising performance on benchmarks at relatively low\ncost. Our work has yielded key insights into the effective use of synthetic\ndata, aiming to promote more robust and efficient LLM training."", 'Using Large Language Models (LLMs) to generate synthetic data for model\ntraining has become increasingly popular in recent years. While LLMs are\ncapable of producing realistic training data, the effectiveness of data\ngeneration is influenced by various factors, including the choice of prompt,\ntask complexity, and the quality, quantity, and diversity of the generated\ndata. In this work, we focus exclusively on using synthetic data for text\nclassification tasks. Specifically, we use natural language understanding (NLU)\nmodels trained on synthetic data to assess the quality of synthetic data from\ndifferent generation approaches. This work provides an empirical analysis of\nthe impact of these factors and offers recommendations for better data\ngeneration practices.']"
57,62,57_safety_alignment_harmful_safe,"['safety', 'alignment', 'harmful', 'safe', 'unsafe', 'helpfulness', 'content', 'merging', 'toxic', 'finetuning']","[""Fine-tuning large language models (LLMs) based on human preferences, commonly\nachieved through reinforcement learning from human feedback (RLHF), has been\neffective in improving their performance. However, maintaining LLM safety\nthroughout the fine-tuning process remains a significant challenge, as\nresolving conflicts between safety and helpfulness can be non-trivial.\nTypically, the safety alignment of LLM is trained on data with safety-related\ncategories. However, our experiments find that naively increasing the scale of\nsafety training data usually leads the LLMs to an ``overly safe'' state rather\nthan a ``truly safe'' state, boosting the refusal rate through extensive\nsafety-aligned data without genuinely understanding the requirements for safe\nresponses. Such an approach can inadvertently diminish the models' helpfulness.\nTo understand the phenomenon, we first investigate the role of safety data by\ncategorizing them into three different groups, and observe that each group\nbehaves differently as training data scales up. To boost the balance between\nsafety and helpfulness, we propose an Equilibrate RLHF framework including a\nFine-grained Data-centric (FDC) approach that achieves better safety alignment\neven with fewer training data, and an Adaptive Message-wise Alignment (AMA)\napproach, which selectively highlight the key segments through a gradient\nmasking strategy. Extensive experimental results demonstrate that our approach\nsignificantly enhances the safety alignment of LLMs while balancing safety and\nhelpfulness."", 'Fine-tuning large language models (LLMs) for downstream tasks is a widely\nadopted approach, but it often leads to safety degradation in safety-aligned\nLLMs. Currently, many solutions address this issue by incorporating additional\nsafety data, which can be impractical in many cases. In this paper, we address\nthe question: How can we improve downstream task performance while preserving\nsafety in LLMs without relying on additional safety data? We propose a simple\nand effective method that maintains the inherent safety of LLMs while enhancing\ntheir downstream task performance: merging the weights of pre- and\npost-fine-tuned safety-aligned models. Experimental results across various\ndownstream tasks, models, and merging methods demonstrate that this approach\neffectively mitigates safety degradation while improving downstream task\nperformance, offering a practical solution for adapting safety-aligned LLMs.', 'Ensuring the safe alignment of large language models (LLMs) with human values\nis critical as they become integral to applications like translation and\nquestion answering. Current alignment methods struggle with dynamic user\nintentions and complex objectives, making models vulnerable to generating\nharmful content. We propose Safety Arithmetic, a training-free framework\nenhancing LLM safety across different scenarios: Base models, Supervised\nfine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm\nDirection Removal to avoid harmful content and Safety Alignment to promote safe\nresponses. Additionally, we present NoIntentEdit, a dataset highlighting edit\ninstances that could compromise model safety if used unintentionally. Our\nexperiments show that Safety Arithmetic significantly improves safety measures,\nreduces over-safety, and maintains model utility, outperforming existing\nmethods in ensuring safe content generation.']"
58,61,58_prompt_prompts_optimization_engineering,"['prompt', 'prompts', 'optimization', 'engineering', 'instructions', 'prompting', 'tasks', 'automatic', 'performance', 'ape']","['Designing effective prompts is essential to guiding large language models\n(LLMs) toward desired responses. Automated prompt engineering aims to reduce\nreliance on manual effort by streamlining the design, refinement, and\noptimization of natural language prompts. This paper proposes an optimal\nlearning framework for automated prompt engineering, designed to sequentially\nidentify effective prompt features while efficiently allocating a limited\nevaluation budget. We introduce a feature-based method to express prompts,\nwhich significantly broadens the search space. Bayesian regression is employed\nto utilize correlations among similar prompts, accelerating the learning\nprocess. To efficiently explore the large space of prompt features for a high\nquality prompt, we adopt the forward-looking Knowledge-Gradient (KG) policy for\nsequential optimal learning. The KG policy is computed efficiently by solving\nmixed-integer second-order cone optimization problems, making it scalable and\ncapable of accommodating prompts characterized only through constraints. We\ndemonstrate that our method significantly outperforms a set of benchmark\nstrategies assessed on instruction induction tasks. The results highlight the\nadvantages of using the KG policy for prompt learning given a limited\nevaluation budget. Our framework provides a solution to deploying automated\nprompt engineering in a wider range applications where prompt evaluation is\ncostly.', 'Prompt optimization aims to search for effective prompts that enhance the\nperformance of large language models (LLMs). Although existing prompt\noptimization methods have discovered effective prompts, they often differ from\nsophisticated prompts carefully designed by human experts. Prompt design\nstrategies, representing best practices for improving prompt performance, can\nbe key to improving prompt optimization. Recently, a method termed the\nAutonomous Prompt Engineering Toolbox (APET) has incorporated various prompt\ndesign strategies into the prompt optimization process. In APET, the LLM is\nneeded to implicitly select and apply the appropriate strategies because prompt\ndesign strategies can have negative effects. This implicit selection may be\nsuboptimal due to the limited optimization capabilities of LLMs. This paper\nintroduces Optimizing Prompts with sTrategy Selection (OPTS), which implements\nexplicit selection mechanisms for prompt design. We propose three mechanisms,\nincluding a Thompson sampling-based approach, and integrate them into\nEvoPrompt, a well-known prompt optimizer. Experiments optimizing prompts for\ntwo LLMs, Llama-3-8B-Instruct and GPT-4o mini, were conducted using BIG-Bench\nHard. Our results show that the selection of prompt design strategies improves\nthe performance of EvoPrompt, and the Thompson sampling-based mechanism\nachieves the best overall results. Our experimental code is provided at\nhttps://github.com/shiralab/OPTS .', 'Prompt engineering has made significant contributions to the era of large\nlanguage models, yet its effectiveness depends on the skills of a prompt\nauthor. Automatic prompt optimization can support the prompt development\nprocess, but requires annotated data. This paper introduces $\\textit{iPrOp}$, a\nnovel Interactive Prompt Optimization system, to bridge manual prompt\nengineering and automatic prompt optimization. With human intervention in the\noptimization loop, $\\textit{iPrOp}$ offers users the flexibility to assess\nevolving prompts. We present users with prompt variations, selected instances,\nlarge language model predictions accompanied by corresponding explanations, and\nperformance metrics derived from a subset of the training data. This approach\nempowers users to choose and further refine the provided prompts based on their\nindividual preferences and needs. This system not only assists non-technical\ndomain experts in generating optimal prompts tailored to their specific tasks\nor domains, but also enables to study the intrinsic parameters that influence\nthe performance of prompt optimization. Our evaluation shows that our system\nhas the capability to generate improved prompts, leading to enhanced task\nperformance.']"
59,60,59_factchecking_claims_claim_verification,"['factchecking', 'claims', 'claim', 'verification', 'evidence', 'fact', 'factual', 'factcheckers', 'factscore', 'vietnamese']","['Claim verification is a task that involves assessing the truthfulness of a\ngiven claim based on multiple evidence pieces. Using large language models\n(LLMs) for claim verification is a promising way. However, simply feeding all\nthe evidence pieces to an LLM and asking if the claim is factual does not yield\ngood results. The challenge lies in the noisy nature of both the evidence and\nthe claim: evidence passages typically contain irrelevant information, with the\nkey facts hidden within the context, while claims often convey multiple aspects\nsimultaneously. To navigate this ""noisy crowd"" of information, we propose EACon\n(Evidence Abstraction and Claim Deconstruction), a framework designed to find\nkey information within evidence and verify each aspect of a claim separately.\nEACon first finds keywords from the claim and employs fuzzy matching to select\nrelevant keywords for each raw evidence piece. These keywords serve as a guide\nto extract and summarize critical information into abstracted evidence.\nSubsequently, EACon deconstructs the original claim into subclaims, which are\nthen verified against both abstracted and raw evidence individually. We\nevaluate EACon using two open-source LLMs on two challenging datasets. Results\ndemonstrate that EACon consistently and substantially improve LLMs\' performance\nin claim verification.', 'In the context of fact-checking, claims are often repeated across various\nplatforms and in different languages, which can benefit from a process that\nreduces this redundancy. While retrieving previously fact-checked claims has\nbeen investigated as a solution, the growing number of unverified claims and\nexpanding size of fact-checked databases calls for alternative, more efficient\nsolutions. A promising solution is to group claims that discuss the same\nunderlying facts into clusters to improve claim retrieval and validation.\nHowever, research on claim clustering is hindered by the lack of suitable\ndatasets. To bridge this gap, we introduce \\textit{MultiClaimNet}, a collection\nof three multilingual claim cluster datasets containing claims in 86 languages\nacross diverse topics. Claim clusters are formed automatically from\nclaim-matching pairs with limited manual intervention. We leverage two existing\nclaim-matching datasets to form the smaller datasets within\n\\textit{MultiClaimNet}. To build the larger dataset, we propose and validate an\napproach involving retrieval of approximate nearest neighbors to form candidate\nclaim pairs and an automated annotation of claim similarity using large\nlanguage models. This larger dataset contains 85.3K fact-checked claims written\nin 78 languages. We further conduct extensive experiments using various\nclustering techniques and sentence embedding models to establish baseline\nperformance. Our datasets and findings provide a strong foundation for scalable\nclaim clustering, contributing to efficient fact-checking pipelines.', 'Claim verification can be a challenging task. In this paper, we present a\nmethod to enhance the robustness and reasoning capabilities of automated claim\nverification through the extraction of short facts from evidence. Our novel\napproach, FactDetect, leverages Large Language Models (LLMs) to generate\nconcise factual statements from evidence and label these facts based on their\nsemantic relevance to the claim and evidence. The generated facts are then\ncombined with the claim and evidence. To train a lightweight supervised model,\nwe incorporate a fact-detection task into the claim verification process as a\nmultitasking approach to improve both performance and explainability. We also\nshow that augmenting FactDetect in the claim verification prompt enhances\nperformance in zero-shot claim verification using LLMs. Our method demonstrates\ncompetitive results in the supervised claim verification model by 15% on the F1\nscore when evaluated for challenging scientific claim verification datasets. We\nalso demonstrate that FactDetect can be augmented with claim and evidence for\nzero-shot prompting (AugFactDetect) in LLMs for verdict prediction. We show\nthat AugFactDetect outperforms the baseline with statistical significance on\nthree challenging scientific claim verification datasets with an average of\n17.3% performance gain compared to the best performing baselines.']"
60,60,60_forgetting_continual_cl_catastrophic,"['forgetting', 'continual', 'cl', 'catastrophic', 'learning', 'knowledge', 'cf', 'new', 'tasks', 'finetuning']","['The recent success of large language models (LLMs) trained on static,\npre-collected, general datasets has sparked numerous research directions and\napplications. One such direction addresses the non-trivial challenge of\nintegrating pre-trained LLMs into dynamic data distributions, task structures,\nand user preferences. Pre-trained LLMs, when tailored for specific needs, often\nexperience significant performance degradation in previous knowledge domains --\na phenomenon known as ""catastrophic forgetting"". While extensively studied in\nthe continual learning (CL) community, it presents new manifestations in the\nrealm of LLMs. In this survey, we provide a comprehensive overview of the\ncurrent research progress on LLMs within the context of CL. This survey is\nstructured into four main sections: we first describe an overview of\ncontinually learning LLMs, consisting of two directions of continuity: vertical\ncontinuity (or vertical continual learning), i.e., continual adaptation from\ngeneral to specific capabilities, and horizontal continuity (or horizontal\ncontinual learning), i.e., continual adaptation across time and domains\n(Section 3). We then summarize three stages of learning LLMs in the context of\nmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),\nand Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of\nevaluation protocols for continual learning with LLMs, along with the current\navailable data sources (Section 5). Finally, we discuss intriguing questions\npertaining to continual learning for LLMs (Section 6). The full list of papers\nexamined in this survey is available at\nhttps://github.com/Wang-ML-Lab/llm-continual-learning-survey.', 'Recently, foundation language models (LMs) have marked significant\nachievements in the domains of natural language processing (NLP) and computer\nvision (CV). Unlike traditional neural network models, foundation LMs obtain a\ngreat ability for transfer learning by acquiring rich commonsense knowledge\nthrough pre-training on extensive unsupervised datasets with a vast number of\nparameters. However, they still can not emulate human-like continuous learning\ndue to catastrophic forgetting. Consequently, various continual learning\n(CL)-based methodologies have been developed to refine LMs, enabling them to\nadapt to new tasks without forgetting previous knowledge. However, a systematic\ntaxonomy of existing approaches and a comparison of their performance are still\nlacking, which is the gap that our survey aims to fill. We delve into a\ncomprehensive review, summarization, and classification of the existing\nliterature on CL-based approaches applied to foundation language models, such\nas pre-trained language models (PLMs), large language models (LLMs) and\nvision-language models (VLMs). We divide these studies into offline CL and\nonline CL, which consist of traditional methods, parameter-efficient-based\nmethods, instruction tuning-based methods and continual pre-training methods.\nOffline CL encompasses domain-incremental learning, task-incremental learning,\nand class-incremental learning, while online CL is subdivided into hard task\nboundary and blurry task boundary settings. Additionally, we outline the\ntypical datasets and metrics employed in CL research and provide a detailed\nanalysis of the challenges and future work for LMs-based continual learning.', ""Recent advancements in Artificial Intelligence have led to the development of\nMultimodal Large Language Models (MLLMs). However, adapting these pre-trained\nmodels to dynamic data distributions and various tasks efficiently remains a\nchallenge. Fine-tuning MLLMs for specific tasks often causes performance\ndegradation in the model's prior knowledge domain, a problem known as\n'Catastrophic Forgetting'. While this issue has been well-studied in the\nContinual Learning (CL) community, it presents new challenges for MLLMs. This\nreview paper, the first of its kind in MLLM continual learning, presents an\noverview and analysis of 440 research papers in this area.The review is\nstructured into four sections. First, it discusses the latest research on\nMLLMs, covering model innovations, benchmarks, and applications in various\nfields. Second, it categorizes and overviews the latest studies on continual\nlearning, divided into three parts: non-large language models unimodal\ncontinual learning (Non-LLM Unimodal CL), non-large language models multimodal\ncontinual learning (Non-LLM Multimodal CL), and continual learning in large\nlanguage models (CL in LLM). The third section provides a detailed analysis of\nthe current state of MLLM continual learning research, including benchmark\nevaluations, architectural innovations, and a summary of theoretical and\nempirical studies.Finally, the paper discusses the challenges and future\ndirections of continual learning in MLLMs, aiming to inspire future research\nand development in the field. This review connects the foundational concepts,\ntheoretical insights, method innovations, and practical applications of\ncontinual learning for multimodal large models, providing a comprehensive\nunderstanding of the research progress and challenges in this field, aiming to\ninspire researchers in the field and promote the advancement of related\ntechnologies.""]"
61,58,61_saes_sae_sparse_steering,"['saes', 'sae', 'sparse', 'steering', 'icl', 'features', 'autoencoders', 'transformers', 'interpretability', 'feature']","['Steering vectors are a promising approach to control the behaviour of large\nlanguage models. However, their underlying mechanisms remain poorly understood.\nWhile sparse autoencoders (SAEs) may offer a potential method to interpret\nsteering vectors, recent findings show that SAE-reconstructed vectors often\nlack the steering properties of the original vectors. This paper investigates\nwhy directly applying SAEs to steering vectors yields misleading\ndecompositions, identifying two reasons: (1) steering vectors fall outside the\ninput distribution for which SAEs are designed, and (2) steering vectors can\nhave meaningful negative projections in feature directions, which SAEs are not\ndesigned to accommodate. These limitations hinder the direct use of SAEs for\ninterpreting steering vectors.', 'Sparse autoencoders (SAEs) are a useful tool for uncovering\nhuman-interpretable features in the activations of large language models\n(LLMs). While some expect SAEs to find the true underlying features used by a\nmodel, our research shows that SAEs trained on the same model and data,\ndiffering only in the random seed used to initialize their weights, identify\ndifferent sets of features. For example, in an SAE with 131K latents trained on\na feedforward network in Llama 3 8B, only 30% of the features were shared\nacross different seeds. We observed this phenomenon across multiple layers of\nthree different LLMs, two datasets, and several SAE architectures. While ReLU\nSAEs trained with the L1 sparsity loss showed greater stability across seeds,\nSAEs using the state-of-the-art TopK activation function were more\nseed-dependent, even when controlling for the level of sparsity. Our results\nsuggest that the set of features uncovered by an SAE should be viewed as a\npragmatically useful decomposition of activation space, rather than an\nexhaustive and universal list of features ""truly used"" by the model.', 'A recent line of work has shown promise in using sparse autoencoders (SAEs)\nto uncover interpretable features in neural network representations. However,\nthe simple linear-nonlinear encoding mechanism in SAEs limits their ability to\nperform accurate sparse inference. Using compressed sensing theory, we prove\nthat an SAE encoder is inherently insufficient for accurate sparse inference,\neven in solvable cases. We then decouple encoding and decoding processes to\nempirically explore conditions where more sophisticated sparse inference\nmethods outperform traditional SAE encoders. Our results reveal substantial\nperformance gains with minimal compute increases in correct inference of sparse\ncodes. We demonstrate this generalises to SAEs applied to large language\nmodels, where more expressive encoders achieve greater interpretability. This\nwork opens new avenues for understanding neural network representations and\nanalysing large language model activations.']"
62,58,62_temporal_events_event_timeline,"['temporal', 'events', 'event', 'timeline', 'reasoning', 'timesensitive', 'tpps', 'news', 'facts', 'forecasting']","['Temporal Knowledge Graph Forecasting (TKGF) aims to predict future events\nbased on the observed events in history. Recently, Large Language Models (LLMs)\nhave exhibited remarkable capabilities, generating significant research\ninterest in their application for reasoning over temporal knowledge graphs\n(TKGs). Existing LLM-based methods have integrated retrieved historical facts\nor static graph representations into LLMs. Despite the notable performance of\nLLM-based methods, they are limited by the insufficient modeling of temporal\npatterns and ineffective cross-modal alignment between graph and language,\nhindering the ability of LLMs to fully grasp the temporal and structural\ninformation in TKGs. To tackle these issues, we propose a novel framework\nTGL-LLM to integrate temporal graph learning into LLM-based temporal knowledge\ngraph model. Specifically, we introduce temporal graph learning to capture the\ntemporal and relational patterns and obtain the historical graph embedding.\nFurthermore, we design a hybrid graph tokenization to sufficiently model the\ntemporal patterns within LLMs. To achieve better alignment between graph and\nlanguage, we employ a two-stage training paradigm to finetune LLMs on\nhigh-quality and diverse data, thereby resulting in better performance.\nExtensive experiments on three real-world datasets show that our approach\noutperforms a range of state-of-the-art (SOTA) methods.', 'Automated fact verification plays an essential role in fostering trust in the\ndigital space. Despite the growing interest, the verification of temporal facts\nhas not received much attention in the community. Temporal fact verification\nbrings new challenges where cues of the temporal information need to be\nextracted and temporal reasoning involving various temporal aspects of the text\nmust be applied. In this work, we propose an end-to-end solution for temporal\nfact verification that considers the temporal information in claims to obtain\nrelevant evidence sentences and harness the power of large language model for\ntemporal reasoning. Recognizing that temporal facts often involve events, we\nmodel these events in the claim and evidence sentences. We curate two temporal\nfact datasets to learn time-sensitive representations that encapsulate not only\nthe semantic relationships among the events, but also their chronological\nproximity. This allows us to retrieve the top-k relevant evidence sentences and\nprovide the context for a large language model to perform temporal reasoning\nand outputs whether a claim is supported or refuted by the retrieved evidence\nsentences. Experiment results demonstrate that the proposed approach\nsignificantly enhances the accuracy of temporal claim verification, thereby\nadvancing current state-of-the-art in automated fact verification.', 'Automated fact verification plays an essential role in fostering trust in the\ndigital space. Despite the growing interest, the verification of temporal facts\nhas not received much attention in the community. Temporal fact verification\nbrings new challenges where cues of the temporal information need to be\nextracted and temporal reasoning involving various temporal aspects of the text\nmust be applied. In this work, we propose an end-to-end solution for temporal\nfact verification that considers the temporal information in claims to obtain\nrelevant evidence sentences and harness the power of large language model for\ntemporal reasoning. Recognizing that temporal facts often involve events, we\nmodel these events in the claim and evidence sentences. We curate two temporal\nfact datasets to learn time-sensitive representations that encapsulate not only\nthe semantic relationships among the events, but also their chronological\nproximity. This allows us to retrieve the top-k relevant evidence sentences and\nprovide the context for a large language model to perform temporal reasoning\nand outputs whether a claim is supported or refuted by the retrieved evidence\nsentences. Experiment results demonstrate that the proposed approach\nsignificantly enhances the accuracy of temporal claim verification, thereby\nadvancing current state-of-the-art in automated fact verification.']"
63,57,63_network_wireless_networks_6g,"['network', 'wireless', 'networks', '6g', 'communication', 'service', 'management', 'optimization', 'transmission', 'genai']","['Reinforcement learning (RL)-based large language models (LLMs), such as\nChatGPT, DeepSeek, and Grok-3, have gained significant attention for their\nexceptional capabilities in natural language processing and multimodal data\nunderstanding. Meanwhile, the rapid expansion of information services has\ndriven the growing need for intelligence, efficient, and adaptable wireless\nnetworks. Wireless networks require the empowerment of RL-based LLMs while\nthese models also benefit from wireless networks to broaden their application\nscenarios. Specifically, RL-based LLMs can enhance wireless communication\nsystems through intelligent resource allocation, adaptive network optimization,\nand real-time decision-making. Conversely, wireless networks provide a vital\ninfrastructure for the efficient training, deployment, and distributed\ninference of RL-based LLMs, especially in decentralized and edge computing\nenvironments. This mutual empowerment highlights the need for a deeper\nexploration of the interplay between these two domains. We first review recent\nadvancements in wireless communications, highlighting the associated challenges\nand potential solutions. We then discuss the progress of RL-based LLMs,\nfocusing on key technologies for LLM training, challenges, and potential\nsolutions. Subsequently, we explore the mutual empowerment between these two\nfields, highlighting key motivations, open challenges, and potential solutions.\nFinally, we provide insights into future directions, applications, and their\nsocietal impact to further explore this intersection, paving the way for\nnext-generation intelligent communication systems. Overall, this survey\nprovides a comprehensive overview of the relationship between RL-based LLMs and\nwireless networks, offering a vision where these domains empower each other to\ndrive innovations.', 'The sixth generation mobile communication standard (6G) can promote the\ndevelopment of Industrial Internet and Internet of Things (IoT). To achieve\ncomprehensive intelligent development of the network and provide customers with\nhigher quality personalized services. This paper proposes a network performance\noptimization and intelligent operation network architecture based on Large\nLanguage Model (LLM), aiming to build a comprehensive intelligent 6G network\nsystem. The Large Language Model, with more parameters and stronger learning\nability, can more accurately capture patterns and features in data, which can\nachieve more accurate content output and high intelligence and provide strong\nsupport for related research such as network data security, privacy protection,\nand health assessment. This paper also presents the design framework of a\nnetwork health assessment system based on LLM and focuses on its potential\napplication value, through the case of network health management system, it is\nfully demonstrated that the 6G intelligent network system based on LLM has\nimportant practical significance for the comprehensive realization of\nintelligence.', 'The transition to 6G networks promises unprecedented advancements in wireless\ncommunication, with increased data rates, ultra-low latency, and enhanced\ncapacity. However, the complexity of managing and optimizing these\nnext-generation networks presents significant challenges. The advent of large\nlanguage models (LLMs) has revolutionized various domains by leveraging their\nsophisticated natural language understanding capabilities. However, the\npractical application of LLMs in wireless network orchestration and management\nremains largely unexplored. Existing literature predominantly offers visionary\nperspectives without concrete implementations, leaving a significant gap in the\nfield. To address this gap, this paper presents NETORCHLLM, a wireless NETwork\nORCHestrator LLM framework that uses LLMs to seamlessly orchestrate diverse\nwireless-specific models from wireless communication communities using their\nlanguage understanding and generation capabilities. A comprehensive framework\nis introduced, demonstrating the practical viability of our approach and\nshowcasing how LLMs can be effectively harnessed to optimize dense network\noperations, manage dynamic environments, and improve overall network\nperformance. NETORCHLLM bridges the theoretical aspirations of prior research\nwith practical, actionable solutions, paving the way for future advancements in\nintegrating generative AI technologies within the wireless communications\nsector.']"
64,56,64_3d_scene_2d_point,"['3d', 'scene', '2d', 'point', 'objects', 'segmentation', 'spatial', 'scenes', 'object', 'clouds']","['Large 2D vision-language models (2D-LLMs) have gained significant attention\nby bridging Large Language Models (LLMs) with images using a simple projector.\nInspired by their success, large 3D point cloud-language models (3D-LLMs) also\nintegrate point clouds into LLMs. However, directly aligning point clouds with\nLLM requires expensive training costs, typically in hundreds of GPU-hours on\nA100, which hinders the development of 3D-LLMs. In this paper, we introduce\nMiniGPT-3D, an efficient and powerful 3D-LLM that achieves multiple SOTA\nresults while training for only 27 hours on one RTX 3090. Specifically, we\npropose to align 3D point clouds with LLMs using 2D priors from 2D-LLMs, which\ncan leverage the similarity between 2D and 3D visual information. We introduce\na novel four-stage training strategy for modality alignment in a cascaded way,\nand a mixture of query experts module to adaptively aggregate features with\nhigh efficiency. Moreover, we utilize parameter-efficient fine-tuning methods\nLoRA and Norm fine-tuning, resulting in only 47.8M learnable parameters, which\nis up to 260x fewer than existing methods. Extensive experiments show that\nMiniGPT-3D achieves SOTA on 3D object classification and captioning tasks, with\nsignificantly cheaper training costs. Notably, MiniGPT-3D gains an 8.12\nincrease on GPT-4 evaluation score for the challenging object captioning task\ncompared to ShapeLLM-13B, while the latter costs 160 total GPU-hours on 8 A800.\nWe are the first to explore the efficient 3D-LLM, offering new insights to the\ncommunity. Code and weights are available at\nhttps://github.com/TangYuan96/MiniGPT-3D.', 'Reasoning segmentation aims to segment target objects in complex scenes based\non human intent and spatial reasoning. While recent multimodal large language\nmodels (MLLMs) have demonstrated impressive 2D image reasoning segmentation,\nadapting these capabilities to 3D scenes remains underexplored. In this paper,\nwe introduce MLLM-For3D, a simple yet effective framework that transfers\nknowledge from 2D MLLMs to 3D scene understanding. Specifically, we utilize\nMLLMs to generate multi-view pseudo segmentation masks and corresponding text\nembeddings, then unproject 2D masks into 3D space and align them with the text\nembeddings. The primary challenge lies in the absence of 3D context and spatial\nconsistency across multiple views, causing the model to hallucinate objects\nthat do not exist and fail to target objects consistently. Training the 3D\nmodel with such irrelevant objects leads to performance degradation. To address\nthis, we introduce a spatial consistency strategy to enforce that segmentation\nmasks remain coherent in the 3D space, effectively capturing the geometry of\nthe scene. Moreover, we develop a Token-for-Query approach for multimodal\nsemantic alignment, enabling consistent identification of the same object\nacross different views. Extensive evaluations on various challenging indoor\nscene benchmarks demonstrate that, even without any labeled 3D training data,\nMLLM-For3D outperforms existing 3D reasoning segmentation methods, effectively\ninterpreting user intent, understanding 3D scenes, and reasoning about spatial\nrelationships.', ""The rapid advancement of Multimodal Large Language Models (MLLMs) has\nsignificantly impacted various multimodal tasks. However, these models face\nchallenges in tasks that require spatial understanding within 3D environments.\nEfforts to enhance MLLMs, such as incorporating point cloud features, have been\nmade, yet a considerable gap remains between the models' learned\nrepresentations and the inherent complexity of 3D scenes. This discrepancy\nlargely stems from the training of MLLMs on predominantly 2D data, which\nrestricts their effectiveness in comprehending 3D spaces. To address this\nissue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM,\nfor 3D scene understanding. By treating 3D scenes as dynamic videos and\nincorporating 3D position encoding into these representations, our Video-3D LLM\naligns video representations with real-world spatial contexts more accurately.\nIn addition, we have implemented a maximum coverage sampling technique to\noptimize the trade-off between computational cost and performance. Extensive\nexperiments demonstrate that our model achieves state-of-the-art performance on\nseveral 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer,\nScan2Cap, ScanQA, and SQA3D.""]"
65,56,65_materials_material_science_crystal,"['materials', 'material', 'science', 'crystal', 'properties', 'discovery', 'property', 'structures', 'design', 'synthesis']","['Materials discovery and design aim to find compositions and structures with\ndesirable properties over highly complex and diverse physical spaces.\nTraditional solutions, such as high-throughput simulations or machine learning,\noften rely on complex descriptors, which hinder generalizability and\ntransferability across different material systems. Moreover, These descriptors\nmay inadequately represent macro-scale material properties, which are\ninfluenced by structural imperfections and compositional variations in\nreal-world samples, thus limiting their practical applicability. To address\nthese challenges, we propose DARWIN 1.5, the largest open-source large language\nmodel tailored for materials science. By leveraging natural language as input,\nDARWIN eliminates the need for task-specific descriptors and enables a\nflexible, unified approach to material property prediction and discovery. Our\napproach integrates 6M material domain papers and 21 experimental datasets from\n49,256 materials across modalities while enabling cross-task knowledge\ntransfer. The enhanced model achieves up to 59.1% improvement in prediction\naccuracy over the base LLaMA-7B architecture and outperforms SOTA machine\nlearning approaches across 8 materials design tasks. These results establish\nLLMs as a promising foundation for developing versatile and scalable models in\nmaterials science.', ""Effectively representing materials as text has the potential to leverage the\nvast advancements of large language models (LLMs) for discovering new\nmaterials. While LLMs have shown remarkable success in various domains, their\napplication to materials science remains underexplored. A fundamental challenge\nis the lack of understanding of how to best utilize text-based representations\nfor materials modeling. This challenge is further compounded by the absence of\na comprehensive benchmark to rigorously evaluate the capabilities and\nlimitations of these text representations in capturing the complexity of\nmaterial systems. To address this gap, we propose MatText, a suite of\nbenchmarking tools and datasets designed to systematically evaluate the\nperformance of language models in modeling materials. MatText encompasses nine\ndistinct text-based representations for material systems, including several\nnovel representations. Each representation incorporates unique inductive biases\nthat capture relevant information and integrate prior physical knowledge about\nmaterials. Additionally, MatText provides essential tools for training and\nbenchmarking the performance of language models in the context of materials\nscience. These tools include standardized dataset splits for each\nrepresentation, probes for evaluating sensitivity to geometric factors, and\ntools for seamlessly converting crystal structures into text. Using MatText, we\nconduct an extensive analysis of the capabilities of language models in\nmodeling materials. Our findings reveal that current language models\nconsistently struggle to capture the geometric information crucial for\nmaterials modeling across all representations. Instead, these models tend to\nleverage local information, which is emphasized in some of our novel\nrepresentations. Our analysis underscores MatText's ability to reveal\nshortcomings of text-based methods for materials design."", ""As the application of large language models in various fields continues to\nexpand, materials science also ushers in opportunities for AI-driven\ninnovation. The traditional way of relying on manual search for materials\nscience-related information is now using artificial intelligence technology as\nan auxiliary tool to improve the efficiency of materials science research. To\naccelerate researchers' knowledge acquisition and intelligent decision-making\nsupport in materials science research, this paper proposes a large language\nmodel Polymetis model for a variety of materials fields, aiming to provide\nhighly professional knowledge answers in the field of materials, covering\nenergy materials, functional materials, alloy materials, physical chemistry,\nbiology, and other material directions. The model uses a dataset of about 2\nmillion material knowledge instructions, and in the process of building the\ndataset, we developed the Intelligent Extraction Large Model (IELM), which is\nspecially used to extract and form structured knowledge from scientific texts,\navoiding a large number of costs that need to be manually annotated, and\nimproving efficiency. We inject this data into the GLM4-9B model for learning\nto enhance its inference capabilities in a variety of material domains. In\naddition, we have introduced enhanced prompt strategies to ensure that the\nanswers to the model are more organized and comprehensive, providing efficient\nand comprehensive intelligent support for the diverse needs of materials\nscience exploration, and promoting the development of material science.""]"
66,53,66_privacy_private_dp_differential,"['privacy', 'private', 'dp', 'differential', 'privacypreserving', 'sensitive', 'data', 'homomorphic', 'differentially', 'guarantees']","['We present an approach for generating differentially private synthetic text\nusing large language models (LLMs), via private prediction. In the private\nprediction framework, we only require the output synthetic data to satisfy\ndifferential privacy guarantees. This is in contrast to approaches that train a\ngenerative model on potentially sensitive user-supplied source data and seek to\nensure the model itself is safe to release.\n  We prompt a pretrained LLM with source data, but ensure that next-token\npredictions are made with differential privacy guarantees. Previous work in\nthis paradigm reported generating a small number of examples (<10) at\nreasonable privacy levels, an amount of data that is useful only for downstream\nin-context learning or prompting. In contrast, we make changes that allow us to\ngenerate thousands of high-quality synthetic data points, greatly expanding the\nset of potential applications. Our improvements come from an improved privacy\nanalysis and a better private selection mechanism, which makes use of the\nequivalence between the softmax layer for sampling tokens in LLMs and the\nexponential mechanism. Furthermore, we introduce a novel use of public\npredictions via the sparse vector technique, in which we do not pay privacy\ncosts for tokens that are predictable without sensitive data; we find this to\nbe particularly effective for structured data.', ""Large language models (LLMs) have emerged as powerful tools for tackling\ncomplex tasks across diverse domains, but they also raise privacy concerns when\nfine-tuned on sensitive data due to potential memorization. While differential\nprivacy (DP) offers a promising solution by ensuring models are 'almost\nindistinguishable' with or without any particular privacy unit, current\nevaluations on LLMs mostly treat each example (text record) as the privacy\nunit. This leads to uneven user privacy guarantees when contributions per user\nvary. We therefore study user-level DP motivated by applications where it\nnecessary to ensure uniform privacy protection across users. We present a\nsystematic evaluation of user-level DP for LLM fine-tuning on natural language\ngeneration tasks. Focusing on two mechanisms for achieving user-level DP\nguarantees, Group Privacy and User-wise DP-SGD, we investigate design choices\nlike data selection strategies and parameter tuning for the best\nprivacy-utility tradeoff."", 'Machine learning (ML) models frequently rely on training data that may\ninclude sensitive or personal information, raising substantial privacy\nconcerns. Legislative frameworks such as the General Data Protection Regulation\n(GDPR) and the California Consumer Privacy Act (CCPA) have necessitated the\ndevelopment of strategies that preserve privacy while maintaining the utility\nof data. In this paper, we investigate the capability of Large Language Models\n(LLMs) to generate synthetic datasets integrated with Differential Privacy (DP)\nmechanisms, thereby enabling data-driven research and model training without\ndirect exposure of sensitive information. Our approach incorporates DP-based\nnoise injection methods, including Laplace and Gaussian distributions, into the\ndata generation process. We then evaluate the utility of these DP-enhanced\nsynthetic datasets by comparing the performance of ML models trained on them\nagainst models trained on the original data. To substantiate privacy\nguarantees, we assess the resilience of the generated synthetic data to\nmembership inference attacks and related threats. The experimental results\ndemonstrate that integrating DP within LLM-driven synthetic data generation\noffers a viable balance between privacy protection and data utility. This study\nprovides a foundational methodology and insight into the privacy-preserving\ncapabilities of LLMs, paving the way for compliant and effective ML research\nand applications.']"
67,53,67_game_games_strategic_agents,"['game', 'games', 'strategic', 'agents', 'gametheoretic', 'players', 'player', 'social', 'decisionmaking', 'interactions']","['Developing a consistent and reliable AI game master for text-based games is a\nchallenging task due to the limitations of large language models (LLMs) and the\ncomplexity of the game master\'s role. This paper presents a novel approach to\nenhance AI game masters by leveraging function calling in the context of the\ntable-top role-playing game ""Jim Henson\'s Labyrinth: The Adventure Game."" Our\nmethodology involves integrating game-specific controls through functions,\nwhich we show improves the narrative quality and state update consistency of\nthe AI game master. The experimental results, based on human evaluations and\nunit tests, demonstrate the effectiveness of our approach in enhancing gameplay\nexperience and maintaining coherence with the game state. This work contributes\nto the advancement of game AI and interactive storytelling, offering insights\ninto the design of more engaging and consistent AI-driven game masters.', 'The development of game agents holds a critical role in advancing towards\nArtificial General Intelligence. The progress of Large Language Models (LLMs)\noffers an unprecedented opportunity to evolve and empower game agents with\nhuman-like decision-making capabilities in complex computer game environments.\nThis paper provides a comprehensive overview of LLM-based game agents from a\nholistic viewpoint. First, we introduce the conceptual architecture of\nLLM-based game agents, centered around three core functional components:\nmemory, reasoning and in/output. Second, we survey existing representative\nLLM-based game agents documented in the literature with respect to\nmethodologies and adaptation agility across six genres of games, including\nadventure, communication, competition, cooperation, simulation, and crafting &\nexploration games. Finally, we present an outlook of future research and\ndevelopment directions in this burgeoning field. A curated list of relevant\npapers is maintained and made accessible at:\nhttps://github.com/git-disl/awesome-LLM-game-agent-papers.', ""Game-theoretic scenarios have become pivotal in evaluating the social\nintelligence of Large Language Model (LLM)-based social agents. While numerous\nstudies have explored these agents in such settings, there is a lack of a\ncomprehensive survey summarizing the current progress. To address this gap, we\nsystematically review existing research on LLM-based social agents within\ngame-theoretic scenarios. Our survey organizes the findings into three core\ncomponents: Game Framework, Social Agent, and Evaluation Protocol. The game\nframework encompasses diverse game scenarios, ranging from choice-focusing to\ncommunication-focusing games. The social agent part explores agents'\npreferences, beliefs, and reasoning abilities. The evaluation protocol covers\nboth game-agnostic and game-specific metrics for assessing agent performance.\nBy reflecting on the current research and identifying future research\ndirections, this survey provides insights to advance the development and\nevaluation of social agents in game-theoretic scenarios.""]"
68,53,68_moral_ethical_values_dilemmas,"['moral', 'ethical', 'values', 'dilemmas', 'ethics', 'judgments', 'misspellings', 'foundations', 'beliefs', 'llms']","['The rapid adoption of large language models (LLMs) has spurred extensive\nresearch into their encoded moral norms and decision-making processes. Much of\nthis research relies on prompting LLMs with survey-style questions to assess\nhow well models are aligned with certain demographic groups, moral beliefs, or\npolitical ideologies. While informative, the adherence of these approaches to\nrelatively superficial constructs tends to oversimplify the complexity and\nnuance underlying everyday moral dilemmas. We argue that auditing LLMs along\nmore detailed axes of human interaction is of paramount importance to better\nassess the degree to which they may impact human beliefs and actions. To this\nend, we evaluate LLMs on complex, everyday moral dilemmas sourced from the ""Am\nI the Asshole"" (AITA) community on Reddit, where users seek moral judgments on\neveryday conflicts from other community members. We prompted seven LLMs to\nassign blame and provide explanations for over 10,000 AITA moral dilemmas. We\nthen compared the LLMs\' judgments and explanations to those of Redditors and to\neach other, aiming to uncover patterns in their moral reasoning. Our results\ndemonstrate that large language models exhibit distinct patterns of moral\njudgment, varying substantially from human evaluations on the AITA subreddit.\nLLMs demonstrate moderate to high self-consistency but low inter-model\nagreement. Further analysis of model explanations reveals distinct patterns in\nhow models invoke various moral principles. These findings highlight the\ncomplexity of implementing consistent moral reasoning in artificial systems and\nthe need for careful evaluation of how different models approach ethical\njudgment. As LLMs continue to be used in roles requiring ethical\ndecision-making such as therapists and companions, careful evaluation is\ncrucial to mitigate potential biases and limitations.', 'Proper moral beliefs are fundamental for language models, yet assessing these\nbeliefs poses a significant challenge. This study introduces a novel\nthree-module framework to evaluate the moral beliefs of four prominent large\nlanguage models. Initially, we constructed a dataset containing 472 moral\nchoice scenarios in Chinese, derived from moral words. The decision-making\nprocess of the models in these scenarios reveals their moral principle\npreferences. By ranking these moral choices, we discern the varying moral\nbeliefs held by different language models. Additionally, through moral debates,\nwe investigate the firmness of these models to their moral choices. Our\nfindings indicate that English language models, namely ChatGPT and Gemini,\nclosely mirror moral decisions of the sample of Chinese university students,\ndemonstrating strong adherence to their choices and a preference for\nindividualistic moral beliefs. In contrast, Chinese models such as Ernie and\nChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their\nmoral choices and debates. This study also uncovers gender bias embedded within\nthe moral beliefs of all examined language models. Our methodology offers an\ninnovative means to assess moral beliefs in both artificial and human\nintelligence, facilitating a comparison of moral values across different\ncultures.', 'Recently, large foundation models, including large language models (LLMs) and\nlarge vision-language models (LVLMs), have become essential tools in critical\nfields such as law, finance, and healthcare. As these models increasingly\nintegrate into our daily life, it is necessary to conduct moral evaluation to\nensure that their outputs align with human values and remain within moral\nboundaries. Previous works primarily focus on LLMs, proposing moral datasets\nand benchmarks limited to text modality. However, given the rapid development\nof LVLMs, there is still a lack of multimodal moral evaluation methods. To\nbridge this gap, we introduce M$^3$oralBench, the first MultiModal Moral\nBenchmark for LVLMs. M$^3$oralBench expands the everyday moral scenarios in\nMoral Foundations Vignettes (MFVs) and employs the text-to-image diffusion\nmodel, SD3.0, to create corresponding scenario images. It conducts moral\nevaluation across six moral foundations of Moral Foundations Theory (MFT) and\nencompasses tasks in moral judgement, moral classification, and moral response,\nproviding a comprehensive assessment of model performance in multimodal moral\nunderstanding and reasoning. Extensive experiments on 10 popular open-source\nand closed-source LVLMs demonstrate that M$^3$oralBench is a challenging\nbenchmark, exposing notable moral limitations in current models. Our benchmark\nis publicly available.']"
69,52,69_test_tests_testing_unit,"['test', 'tests', 'testing', 'unit', 'software', 'cases', 'coverage', 'oracles', 'java', 'bugs']","['Implementing automated unit tests is an important but time-consuming activity\nin software development. To assist developers in this task, many techniques for\nautomating unit test generation have been developed. However, despite this\neffort, usable tools exist for very few programming languages. Moreover,\nstudies have found that automatically generated tests suffer poor readability\nand do not resemble developer-written tests. In this work, we present a\nrigorous investigation of how large language models (LLMs) can help bridge the\ngap. We describe a generic pipeline that incorporates static analysis to guide\nLLMs in generating compilable and high-coverage test cases. We illustrate how\nthe pipeline can be applied to different programming languages, specifically\nJava and Python, and to complex software requiring environment mocking. We\nconducted an empirical study to assess the quality of the generated tests in\nterms of code coverage and test naturalness -- evaluating them on standard as\nwell as enterprise Java applications and a large Python benchmark. Our results\ndemonstrate that LLM-based test generation, when guided by static analysis, can\nbe competitive with, and even outperform, state-of-the-art test-generation\ntechniques in coverage achieved while also producing considerably more natural\ntest cases that developers find easy to understand. We also present the results\nof a user study, conducted with 161 professional developers, that highlights\nthe naturalness characteristics of the tests generated by our approach.', 'Unit testing, crucial for ensuring the reliability of code modules, such as\nclasses and methods, is often overlooked by developers due to time constraints.\nAutomated test generation techniques have emerged to address this, but they\nfrequently lack readability and require significant developer intervention.\nLarge Language Models (LLMs), such as GPT and Mistral, have shown promise in\nsoftware engineering tasks, including test generation, but their overall\neffectiveness remains unclear. This study presents an extensive investigation\nof LLMs, evaluating the effectiveness of four models and five prompt\nengineering techniques for unit test generation. We analyze 216 300 tests\ngenerated by the selected advanced instruct-tuned LLMs for 690 Java classes\ncollected from diverse datasets. Our evaluation considers correctness,\nunderstandability, coverage, and test smell detection in the generated tests,\ncomparing them to a widely used automated testing tool, EvoSuite. While LLMs\ndemonstrate potential, improvements in test quality particularly in reducing\ncommon test smells are necessary. This study highlights the strengths and\nlimitations of LLM-generated tests compared to traditional methods, paving the\nway for further research on LLMs in test automation.', ""Unit tests represent the most basic level of testing within the software\ntesting lifecycle and are crucial to ensuring software correctness. Designing\nand creating unit tests is a costly and labor-intensive process that is ripe\nfor automation. Recently, Large Language Models (LLMs) have been applied to\nvarious aspects of software development, including unit test generation.\nAlthough several empirical studies evaluating LLMs' capabilities in test code\ngeneration exist, they primarily focus on simple scenarios, such as the\nstraightforward generation of unit tests for individual methods. These\nevaluations often involve independent and small-scale test units, providing a\nlimited view of LLMs' performance in real-world software development scenarios.\nMoreover, previous studies do not approach the problem at a suitable scale for\nreal-life applications. Generated unit tests are often evaluated via manual\nintegration into the original projects, a process that limits the number of\ntests executed and reduces overall efficiency. To address these gaps, we have\ndeveloped an approach for generating and evaluating more real-life complexity\ntest suites. Our approach focuses on class-level test code generation and\nautomates the entire process from test generation to test assessment. In this\nwork, we present AgoneTest: an automated system for generating test suites for\nJava projects and a comprehensive and principled methodology for evaluating the\ngenerated test suites. Starting from a state-of-the-art dataset (i.e.,\nMethods2Test), we built a new dataset for comparing human-written tests with\nthose generated by LLMs. Our key contributions include a scalable automated\nsoftware system, a new dataset, and a detailed methodology for evaluating test\nquality.""]"
70,50,70_verilog_hardware_design_hdl,"['verilog', 'hardware', 'design', 'hdl', 'hls', 'rtl', 'designs', 'code', 'generation', 'description']","['Large Language Models (LLMs) have recently shown promise in streamlining\nhardware design processes by encapsulating vast amounts of domain-specific\ndata. In addition, they allow users to interact with the design processes\nthrough natural language instructions, thus making hardware design more\naccessible to developers. However, effectively leveraging LLMs in hardware\ndesign necessitates providing domain-specific data during inference (e.g.,\nthrough in-context learning), fine-tuning, or pre-training. Unfortunately,\nexisting publicly available hardware datasets are often limited in size,\ncomplexity, or detail, which hinders the effectiveness of LLMs in hardware\ndesign tasks. To address this issue, we first propose a set of criteria for\ncreating high-quality hardware datasets that can effectively enhance\nLLM-assisted hardware design. Based on these criteria, we propose a\nMulti-Grained-Verilog (MG-Verilog) dataset, which encompasses descriptions at\nvarious levels of detail and corresponding code samples. To benefit the broader\nhardware design community, we have developed an open-source infrastructure that\nfacilitates easy access, integration, and extension of the dataset to meet\nspecific project needs. Furthermore, to fully exploit the potential of the\nMG-Verilog dataset, which varies in complexity and detail, we introduce a\nbalanced fine-tuning scheme. This scheme serves as a unique use case to\nleverage the diverse levels of detail provided by the dataset. Extensive\nexperiments demonstrate that the proposed dataset and fine-tuning scheme\nconsistently improve the performance of LLMs in hardware design tasks.', 'Recent advances in code generation have illuminated the potential of\nemploying large language models (LLMs) for general-purpose programming\nlanguages such as Python and C++, opening new opportunities for automating\nsoftware development and enhancing programmer productivity. The potential of\nLLMs in software programming has sparked significant interest in exploring\nautomated hardware generation and automation. Although preliminary endeavors\nhave been made to adopt LLMs in generating hardware description languages\n(HDLs), several challenges persist in this direction. First, the volume of\navailable HDL training data is substantially smaller compared to that for\nsoftware programming languages. Second, the pre-trained LLMs, mainly tailored\nfor software code, tend to produce HDL designs that are more error-prone.\nThird, the generation of HDL requires a significantly higher number of tokens\ncompared to software programming, leading to inefficiencies in cost and energy\nconsumption. To tackle these challenges, this paper explores leveraging LLMs to\ngenerate High-Level Synthesis (HLS)-based hardware design. Although code\ngeneration for domain-specific programming languages is not new in the\nliterature, we aim to provide experimental results, insights, benchmarks, and\nevaluation infrastructure to investigate the suitability of HLS over low-level\nHDLs for LLM-assisted hardware design generation. To achieve this, we first\nfinetune pre-trained models for HLS-based hardware generation, using a\ncollected dataset with text prompts and corresponding reference HLS designs. An\nLLM-assisted framework is then proposed to automate end-to-end hardware code\ngeneration, which also investigates the impact of chain-of-thought and feedback\nloops promoting techniques on HLS-design generation. Limited by the timeframe\nof this research, we plan to evaluate more advanced reasoning models in the\nfuture.', 'The ever-growing popularity of large language models (LLMs) has resulted in\ntheir increasing adoption for hardware design and verification. Prior research\nhas attempted to assess the capability of LLMs to automate digital hardware\ndesign by producing superior-quality Register Transfer Logic (RTL)\ndescriptions, particularly in Verilog. However, these tests have revealed that\nVerilog code production using LLMs at current state-of-the-art lack sufficient\nfunctional correctness to be practically viable, compared to automatic\ngeneration of programs in general-purpose programming languages such as C, C++,\nPython, etc. With this as the key insight, in this paper we assess the\nperformance of a two-stage software pipeline for automated Verilog RTL\ngeneration: LLM based automatic generation of annotated C++ code suitable for\nhigh-level synthesis (HLS), followed by HLS to generate Verilog RTL. We have\nbenchmarked the performance of our proposed scheme using the open-source\nVerilogEval dataset, for four different industry-scale LLMs, and the Vitis HLS\ntool. Our experimental results demonstrate that our two-step technique\nsubstantially outperforms previous proposed techniques of direct Verilog RTL\ngeneration by LLMs in terms of average functional correctness rates, reaching\nscore of 0.86 in pass@1 metric.']"
71,50,71_software_requirements_engineering_development,"['software', 'requirements', 'engineering', 'development', 'developers', 'uml', 'se', 'chatgpt', 're', 'user']","[""Many studies exploring the adoption of Large Language Model-based tools for\nsoftware development by junior developers have emerged in recent years. These\nstudies have sought to understand developers' perspectives about using those\ntools, a fundamental pillar for successfully adopting LLM-based tools in\nSoftware Engineering. The aim of this paper is to provide an overview of junior\nsoftware developers' perspectives and use of LLM-based tools for software\nengineering (LLM4SE). We conducted a systematic literature review (SLR)\nfollowing guidelines by Kitchenham et al. on 56 primary studies, applying the\ndefinition for junior software developers as software developers with equal or\nless than five years of experience, including Computer Science/Software\nEngineering students. We found that the majority of the studies focused on\ncomprehending the different aspects of integrating AI tools in SE. Only 8.9\\%\nof the studies provide a clear definition for junior software developers, and\nthere is no uniformity. Searching for relevant information is the most common\ntask using LLM tools. ChatGPT was the most common LLM tool present in the\nstudies (and experiments). A majority of the studies (83.9\\%) report both\npositive and negative perceptions about the impact of adopting LLM tools. We\nalso found and categorised advantages, challenges, and recommendations\nregarding LLM adoption. Our results indicate that developers are using LLMs not\njust for code generation, but also to improve their development skills.\nCritically, they are not just experiencing the benefits of adopting LLM tools,\nbut they are also aware of at least a few LLM limitations, such as the\ngeneration of wrong suggestions, potential data leaking, and AI hallucination.\nOur findings offer implications for software engineering researchers,\neducators, and developers."", 'Large language models (LLMs) like OpenAI ChatGPT, Google Gemini, and GitHub\nCopilot are rapidly gaining traction in the software industry, but their full\nimpact on software engineering remains insufficiently explored. Despite their\ngrowing adoption, there is a notable lack of formal, qualitative assessments of\nhow LLMs are applied in real-world software development contexts. To fill this\ngap, we conducted semi-structured interviews with sixteen early-adopter\nprofessional developers to explore their use of LLMs throughout various stages\nof the software development life cycle. Our investigation examines four\ndimensions: people - how LLMs affect individual developers and teams; process -\nhow LLMs alter software engineering workflows; product - LLM impact on software\nquality and innovation; and society - the broader socioeconomic and ethical\nimplications of LLM adoption. Thematic analysis of our data reveals that while\nLLMs have not fundamentally revolutionized the development process, they have\nsubstantially enhanced routine coding tasks, including code generation,\nrefactoring, and debugging. Developers reported the most effective outcomes\nwhen providing LLMs with clear, well-defined problem statements, indicating\nthat LLMs excel with decomposed problems and specific requirements.\nFurthermore, these early-adopters identified that LLMs offer significant value\nfor personal and professional development, aiding in learning new languages and\nconcepts. Early-adopters, highly skilled in software engineering and how LLMs\nwork, identified early and persisting challenges for software engineering, such\nas inaccuracies in generated content and the need for careful manual review\nbefore integrating LLM outputs into production environments. Our study provides\na nuanced understanding of how LLMs are shaping the landscape of software\ndevelopment, with their benefits, limitations, and ongoing implications.', 'Successful software projects depend on the quality of software requirements.\nCreating high-quality requirements is a crucial step toward successful software\ndevelopment. Effective support in this area can significantly reduce\ndevelopment costs and enhance the software quality. In this paper, we introduce\nand assess the capabilities of a Large Language Model (LLM) to evaluate the\nquality characteristics of software requirements according to the ISO 29148\nstandard. We aim to further improve the support of stakeholders engaged in\nrequirements engineering (RE). We show how an LLM can assess requirements,\nexplain its decision-making process, and examine its capacity to propose\nimproved versions of requirements. We conduct a study with software engineers\nto validate our approach. Our findings emphasize the potential of LLMs for\nimproving the quality of software requirements.']"
72,50,72_uncertainty_quantification_uq_conformal,"['uncertainty', 'quantification', 'uq', 'conformal', 'prediction', 'nlg', 'semantic', 'responses', 'probabilities', 'cp']","['Uncertainty quantification (UQ) in natural language generation (NLG) tasks\nremains an open challenge, exacerbated by the closed-source nature of the\nlatest large language models (LLMs). This study investigates applying conformal\nprediction (CP), which can transform any heuristic uncertainty notion into\nrigorous prediction sets, to black-box LLMs in open-ended NLG tasks. We\nintroduce a novel uncertainty measure based on self-consistency theory, and\nthen develop a conformal uncertainty criterion by integrating the uncertainty\ncondition aligned with correctness into the CP algorithm. Empirical evaluations\nindicate that our uncertainty measure outperforms prior state-of-the-art\nmethods. Furthermore, we achieve strict control over the correctness coverage\nrate utilizing 7 popular LLMs on 4 free-form NLG datasets, spanning\ngeneral-purpose and medical scenarios. Additionally, the calibrated prediction\nsets with small size further highlights the efficiency of our method in\nproviding trustworthy guarantees for practical open-ended NLG applications.', 'In this paper, we present a dynamic semantic clustering approach inspired by\nthe Chinese Restaurant Process, aimed at addressing uncertainty in the\ninference of Large Language Models (LLMs). We quantify uncertainty of an LLM on\na given query by calculating entropy of the generated semantic clusters.\nFurther, we propose leveraging the (negative) likelihood of these clusters as\nthe (non)conformity score within Conformal Prediction framework, allowing the\nmodel to predict a set of responses instead of a single output, thereby\naccounting for uncertainty in its predictions. We demonstrate the effectiveness\nof our uncertainty quantification (UQ) technique on two well known question\nanswering benchmarks, COQA and TriviaQA, utilizing two LLMs, Llama2 and\nMistral. Our approach achieves SOTA performance in UQ, as assessed by metrics\nsuch as AUROC, AUARC, and AURAC. The proposed conformal predictor is also shown\nto produce smaller prediction sets while maintaining the same probabilistic\nguarantee of including the correct response, in comparison to existing SOTA\nconformal prediction baseline.', 'Despite the massive advancements in large language models (LLMs), they still\nsuffer from producing plausible but incorrect responses. To improve the\nreliability of LLMs, recent research has focused on uncertainty quantification\nto predict whether a response is correct or not. However, most uncertainty\nquantification methods have been evaluated on single-labeled questions, which\nremoves data uncertainty: the irreducible randomness often present in user\nqueries, which can arise from factors like multiple possible answers. This\nlimitation may cause uncertainty quantification results to be unreliable in\npractical settings. In this paper, we investigate previous uncertainty\nquantification methods under the presence of data uncertainty. Our\ncontributions are two-fold: 1) proposing a new Multi-Answer Question Answering\ndataset, MAQA, consisting of world knowledge, mathematical reasoning, and\ncommonsense reasoning tasks to evaluate uncertainty quantification regarding\ndata uncertainty, and 2) assessing 5 uncertainty quantification methods of\ndiverse white- and black-box LLMs. Our findings show that previous methods\nrelatively struggle compared to single-answer settings, though this varies\ndepending on the task. Moreover, we observe that entropy- and consistency-based\nmethods effectively estimate model uncertainty, even in the presence of data\nuncertainty. We believe these observations will guide future work on\nuncertainty quantification in more realistic settings.']"
73,50,73_grading_questions_kt_students,"['grading', 'questions', 'kt', 'students', 'educational', 'student', 'rubrics', 'education', 'tracing', 'answers']","[""Large language models (LLMs) have demonstrated strong potential in performing\nautomatic scoring for constructed response assessments. While constructed\nresponses graded by humans are usually based on given grading rubrics, the\nmethods by which LLMs assign scores remain largely unclear. It is also\nuncertain how closely AI's scoring process mirrors that of humans or if it\nadheres to the same grading criteria. To address this gap, this paper uncovers\nthe grading rubrics that LLMs used to score students' written responses to\nscience tasks and their alignment with human scores. We also examine whether\nenhancing the alignments can improve scoring accuracy. Specifically, we prompt\nLLMs to generate analytic rubrics that they use to assign scores and study the\nalignment gap with human grading rubrics. Based on a series of experiments with\nvarious configurations of LLM settings, we reveal a notable alignment gap\nbetween human and LLM graders. While LLMs can adapt quickly to scoring tasks,\nthey often resort to shortcuts, bypassing deeper logical reasoning expected in\nhuman grading. We found that incorporating high-quality analytical rubrics\ndesigned to reflect human grading logic can mitigate this gap and enhance LLMs'\nscoring accuracy. These results underscore the need for a nuanced approach when\napplying LLMs in science education and highlight the importance of aligning LLM\noutputs with human expectations to ensure efficient and accurate automatic\nscoring."", 'This study examines the feasibility and potential advantages of using large\nlanguage models, in particular GPT-4o, to perform partial credit grading of\nlarge numbers of student written responses to introductory level physics\nproblems. Students were instructed to write down verbal explanations of their\nreasoning process when solving one conceptual and two numerical calculation\nproblems on in class exams. The explanations were then graded according to a\n3-item rubric with each item grades as binary (1 or 0). We first demonstrate\nthat machine grading using GPT-4o with no examples nor reference answer can\nreliably agree with human graders on 70%-80% of all cases, which is equal to or\nhigher than the level at which two human graders agree with each other. Two\nmethods are essential for achieving this level of accuracy: 1. Adding\nexplanation language to each rubric item that targets the errors of initial\nmachine grading. 2. Running the grading process 5 times and taking the most\nfrequent outcome. Next, we show that the variation in outcomes across 5 machine\ngrading attempts as measured by the Shannon Entropy can serve as a grading\nconfidence index, allowing a human instructor to identify ~40% of all\npotentially incorrect gradings by reviewing just 10 - 15% of all responses.\nFinally, we show that it is straightforward to use GPT-4o to write clear\nexplanations of the partial credit grading outcomes. Those explanations can be\nused as feedback for students, which will allow students to understand their\ngrades and raise different opinions when necessary. Almost all feedback\nmessages generated were rated 3 or above on a 5-point scale by two experienced\ninstructors. The entire grading and feedback generating process cost roughly $5\nper 100 student answers, which shows immense promise for automating\nlabor-intensive grading process by a combination of machine grading with human\ninput and supervision.', ""While large language models (LLMs) have been used for automated grading, they\nhave not yet achieved the same level of performance as humans, especially when\nit comes to grading complex questions. Existing research on this topic focuses\non a particular step in the grading procedure: grading using predefined\nrubrics. However, grading is a multifaceted procedure that encompasses other\ncrucial steps, such as grading rubrics design and post-grading review. There\nhas been a lack of systematic research exploring the potential of LLMs to\nenhance the entire grading~process.\n  In this paper, we propose an LLM-based grading system that addresses the\nentire grading procedure, including the following key components: 1) Developing\ngrading rubrics that not only consider the questions but also the student\nanswers, which can more accurately reflect students' performance. 2) Under the\nguidance of grading rubrics, providing accurate and consistent scores for each\nstudent, along with customized feedback. 3) Conducting post-grading review to\nbetter ensure accuracy and fairness. Additionally, we collected a new dataset\nnamed OS from a university operating system course and conducted extensive\nexperiments on both our new dataset and the widely used Mohler dataset.\nExperiments demonstrate the effectiveness of our proposed approach, providing\nsome new insights for developing automated grading systems based on LLMs.""]"
74,49,74_citation_citations_ecommerce_attribution,"['citation', 'citations', 'ecommerce', 'attribution', 'shopping', 'product', 'verifiability', 'sources', 'customers', 'finegrained']","['Unlike traditional citation analysis -- which assumes that all citations in a\npaper are equivalent -- citation context analysis considers the contextual\ninformation of individual citations. However, citation context analysis\nrequires creating large amounts of data through annotation, which hinders the\nwidespread use of this methodology. This study explored the applicability of\nLarge Language Models (LLMs) -- particularly ChatGPT -- to citation context\nanalysis by comparing LLMs and human annotation results. The results show that\nthe LLMs annotation is as good as or better than the human annotation in terms\nof consistency but poor in terms of predictive performance. Thus, having LLMs\nimmediately replace human annotators in citation context analysis is\ninappropriate. However, the annotation results obtained by LLMs can be used as\nreference information when narrowing the annotation results obtained by\nmultiple human annotators to one, or LLMs can be used as one of the annotators\nwhen it is difficult to prepare sufficient human annotators. This study\nprovides basic findings important for the future development of citation\ncontext analyses.', 'Enabling Large Language Models (LLMs) to generate citations in\nQuestion-Answering (QA) tasks is an emerging paradigm aimed at enhancing the\nverifiability of their responses when LLMs are utilizing external references to\ngenerate an answer. However, there is currently no unified framework to\nstandardize and fairly compare different citation generation methods, leading\nto difficulties in reproducing different methods and a comprehensive\nassessment. To cope with the problems above, we introduce \\name, an open-source\nand modular toolkit designed to facilitate the implementation and evaluation of\nexisting citation generation methods, while also fostering the development of\nnew approaches to improve citation quality in LLM outputs. This tool is highly\nextensible, allowing users to utilize 4 main modules and 14 components to\nconstruct a pipeline, evaluating an existing method or innovative designs. Our\nexperiments with two state-of-the-art LLMs and 11 citation generation baselines\ndemonstrate varying strengths of different modules in answer accuracy and\ncitation quality improvement, as well as the challenge of enhancing\ngranularity. Based on our analysis of the effectiveness of components, we\npropose a new method, self-RAG \\snippet, obtaining a balanced answer accuracy\nand citation quality. Citekit is released at\nhttps://github.com/SjJ1017/Citekit.', 'Retrieval-augmented generation (RAG) appears as a promising method to\nalleviate the ""hallucination"" problem in large language models (LLMs), since it\ncan incorporate external traceable resources for response generation. The\nessence of RAG in combating the hallucination issue lies in accurately\nattributing claims in responses to the corresponding retrieved documents.\nHowever, most of existing works focus on improving the quality of generated\nresponses from the LLM, while largely overlooked its ability to attribute\nsources accurately. In this study, we conduct a systematic analysis about the\ncapabilities of LLMs in generating citations within response generation, and\nfurther introduce a novel method to enhance their citation generation\nabilities. Specifically, we evaluate both the correctness and citation quality\nfor seven widely-used LLMs on two benchmark datasets. Meanwhile, we introduce\nnew citation evaluation metrics to eliminate the over-penalization of\nunnecessary and excessive citations in existing metrics. Furthermore, we\npropose a Generate-then-Refine method that completes relevant citations and\nremoves irrelevant ones without altering the response text. The results on\nWebGLM-QA, ASQA and ELI5 datasets show that our method substantially improves\nthe quality of citations in responses generated by LLMs.']"
75,49,75_audio_sound_acoustic_audiolanguage,"['audio', 'sound', 'acoustic', 'audiolanguage', 'clap', 'captioning', 'captions', 'music', 'tta', 'soundscape']","[""Understanding and explaining differences between audio recordings is crucial\nfor fields like audio forensics, quality assessment, and audio generation. This\ninvolves identifying and describing audio events, acoustic scenes, signal\ncharacteristics, and their emotional impact on listeners. This paper stands out\nas the first work to comprehensively study the task of explaining audio\ndifferences and then propose benchmark, baselines for the task. First, we\npresent two new datasets for audio difference explanation derived from the\nAudioCaps and Clotho audio captioning datasets. Using Large Language Models\n(LLMs), we generate three levels of difference explanations: (1) concise\ndescriptions of audio events and objects, (2) brief sentences about audio\nevents, acoustic scenes, and signal properties, and (3) comprehensive\nexplanations that include semantics and listener emotions. For the baseline, we\nuse prefix tuning where audio embeddings from two audio files are used to\nprompt a frozen language model. Our empirical analysis and ablation studies\nreveal that the naive baseline struggles to distinguish perceptually similar\nsounds and generate detailed tier 3 explanations. To address these limitations,\nwe propose ADIFF, which introduces a cross-projection module, position\ncaptioning, and a three-step training process to enhance the model's ability to\nproduce detailed explanations. We evaluate our model using objective metrics\nand human evaluation and show our model enhancements lead to significant\nimprovements in performance over naive baseline and SoTA Audio-Language Model\n(ALM) Qwen Audio. Lastly, we conduct multiple ablation studies to study the\neffects of cross-projection, language model parameters, position captioning,\nthird stage fine-tuning, and present our findings. Our benchmarks, findings,\nand strong baseline pave the way for nuanced and human-like explanations of\naudio differences."", 'Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online from here https://yyua8222.github.io/Sound-VECaps-demo/.', 'Large audio-language models (LALMs), built upon powerful Large Language\nModels (LLMs), have exhibited remarkable audio comprehension and reasoning\ncapabilities. However, the training of LALMs demands a large corpus of\naudio-language pairs, which requires substantial costs in both data collection\nand training resources. In this paper, we propose MATS, an audio-language\nmultimodal LLM designed to handle Multiple Audio task using solely Text-only\nSupervision. By leveraging pre-trained audio-language alignment models such as\nCLAP, we develop a text-only training strategy that projects the shared\naudio-language latent space into LLM latent space, endowing the LLM with audio\ncomprehension capabilities without relying on audio data during training. To\nfurther bridge the modality gap between audio and language embeddings within\nCLAP, we propose the Strongly-related noisy text with audio (Santa) mechanism.\nSanta maps audio embeddings into CLAP language embedding space while preserving\nessential information from the audio input. Extensive experiments demonstrate\nthat MATS, despite being trained exclusively on text data, achieves competitive\nperformance compared to recent LALMs trained on large-scale audio-language\npairs.']"
76,48,76_distillation_teacher_student_kd,"['distillation', 'teacher', 'student', 'kd', 'knowledge', 'arithmetic', 'kl', 'model', 'models', 'logits']","[""The widespread deployment of Large Language Models (LLMs) is hindered by the\nhigh computational demands, making knowledge distillation (KD) crucial for\ndeveloping compact smaller ones. However, the conventional KD methods endure\nthe distribution mismatch issue between the teacher and student models, leading\nto the poor performance of distillation. For instance, the widely-used KL-based\nmethods suffer the mode-averaging and mode-collapsing problems, since the\nmismatched probabitliy distribution between both models. Previous studies\nmainly optimize this issue via different distance calculations towards the\ndistribution of both models. Unfortunately, the distribution mismatch issue\nstill exists in the early stage of the distillation. Hence, to reduce the\nimpact of distribution mismatch, we propose a simple yet efficient method,\nnamed Warmup-Distill, which aligns the distillation of the student to that of\nthe teacher in advance of distillation. Specifically, we first detect the\ndistribution of the student model in practical scenarios with its internal\nknowledge, and then modify the knowledge with low probability via the teacher\nas the checker. Consequently, Warmup-Distill aligns the internal student's\nknowledge to that of the teacher, which expands the distribution of the student\nwith the teacher's, and assists the student model to learn better in the\nsubsequent distillation. Experiments on the seven benchmarks demonstrate that\nWarmup-Distill could provide a warmup student more suitable for distillation,\nwhich outperforms the vanilla student by as least +0.4 averaged score among all\nbenchmarks. Noteably, with the assistance of Warmup-Distill, the distillation\non the math task could yield a further improvement, at most +1.9% accuracy."", ""Knowledge distillation (KD) has shown great promise in transferring knowledge\nfrom larger teacher models to smaller student models. However, existing KD\nstrategies for large language models often minimize output distributions\nbetween student and teacher models indiscriminately for each token. This\noverlooks the imbalanced nature of tokens and their varying transfer\ndifficulties. In response, we propose a distillation strategy called\nSelf-Evolution KD. The core of this approach involves dynamically integrating\nteacher distribution and one-hot distribution of ground truth into the student\ndistribution as prior knowledge, which promotes the distillation process. It\nadjusts the ratio of prior knowledge based on token learning difficulty, fully\nleveraging the teacher model's potential. Experimental results show our method\nbrings an average improvement of approximately 1.4 SacreBLEU points across four\ntranslation directions in the WMT22 test sets. Further analysis indicates that\nthe improvement comes from better knowledge transfer from teachers, confirming\nour hypothesis."", 'Despite the advanced intelligence abilities of large language models (LLMs)\nin various applications, they still face significant computational and storage\ndemands. Knowledge Distillation (KD) has emerged as an effective strategy to\nimprove the performance of a smaller LLM (i.e., the student model) by\ntransferring knowledge from a high-performing LLM (i.e., the teacher model).\nPrevailing techniques in LLM distillation typically use a black-box model API\nto generate high-quality pretrained and aligned datasets, or utilize white-box\ndistillation by altering the loss function to better transfer knowledge from\nthe teacher LLM. However, these methods ignore the knowledge differences\nbetween the student and teacher LLMs across domains. This results in excessive\nfocus on domains with minimal performance gaps and insufficient attention to\ndomains with large gaps, reducing overall performance. In this paper, we\nintroduce a new LLM distillation framework called DDK, which dynamically\nadjusts the composition of the distillation dataset in a smooth manner\naccording to the domain performance differences between the teacher and student\nmodels, making the distillation process more stable and effective. Extensive\nevaluations show that DDK significantly improves the performance of student\nmodels, outperforming both continuously pretrained baselines and existing\nknowledge distillation methods by a large margin.']"
77,46,77_visualization_visualizations_nl2vis_analytics,"['visualization', 'visualizations', 'nl2vis', 'analytics', 'data', 'visual', 'va', 'users', 'color', 'user']","['We present a method for augmenting a Large Language Model (LLM) with a\ncombination of text and visual data to enable accurate question answering in\nvisualization of scientific data, making conversational visualization possible.\nLLMs struggle with tasks like visual data interaction, as they lack contextual\nvisual information. We address this problem by merging a text description of a\nvisualization and dataset with snapshots of the visualization. We extract their\nessential features into a structured text file, highly compact, yet descriptive\nenough to appropriately augment the LLM with contextual information, without\nany fine-tuning. This approach can be applied to any visualization that is\nalready finally rendered, as long as it is associated with some textual\ndescription.', 'Interactive visualization editors empower people to author visualizations\nwithout writing code, but do not guide them in the art and craft of effective\nvisual communication. In this paper, we explore the potential for using an\noff-the-shelf Large Language Model (LLM) to provide actionable and customized\nfeedback to visualization designers. Our implementation, called\nVISUALIZATIONARY, showcases how ChatGPT can be used in this manner using two\ncomponents: a preamble of visualization design guidelines and a suite of\nperceptual filters extracting salient metrics from a visualization image. We\npresent findings from a longitudinal user study involving 13 visualization\ndesigners - 6 novices, 4 intermediate ones, and 3 experts - authoring a new\nvisualization from scratch over the course of several days. Our results\nindicate that providing guidance in natural language using an LLM can aid even\nseasoned designers in refining their visualizations. All supplemental materials\naccompanying this paper are available at https://osf.io/v7hu8.', ""In this paper, we assess the visualization literacy of two prominent Large\nLanguage Models (LLMs): OpenAI's Generative Pretrained Transformers (GPT), the\nbackend of ChatGPT, and Google's Gemini, previously known as Bard, to establish\nbenchmarks for assessing their visualization capabilities. While LLMs have\nshown promise in generating chart descriptions, captions, and design\nsuggestions, their potential for evaluating visualizations remains\nunder-explored. Collecting data from humans for evaluations has been a\nbottleneck for visualization research in terms of both time and money, and if\nLLMs were able to serve, even in some limited role, as evaluators, they could\nbe a significant resource. To investigate the feasibility of using LLMs in the\nvisualization evaluation process, we explore the extent to which LLMs possess\nvisualization literacy -- a crucial factor for their effective utility in the\nfield. We conducted a series of experiments using a modified 53-item\nVisualization Literacy Assessment Test (VLAT) for GPT-4 and Gemini. Our\nfindings indicate that the LLMs we explored currently fail to achieve the same\nlevels of visualization literacy when compared to data from the general public\nreported in VLAT, and LLMs heavily relied on their pre-existing knowledge to\nanswer questions instead of utilizing the information provided by the\nvisualization when answering questions.""]"
78,46,78_proof_proofs_lean_theorem,"['proof', 'proofs', 'lean', 'theorem', 'formal', 'mathematical', 'proving', 'theorems', 'autoformalization', 'mathematics']","['Mathematical reasoning remains a significant challenge for Large Language\nModels (LLMs) due to hallucinations. When combined with formal proof assistants\nlike Lean, these hallucinations can be eliminated through rigorous\nverification, making theorem proving reliable. However, even with formal\nverification, LLMs still struggle with long proofs and complex mathematical\nformalizations. While Lean with LLMs offers valuable assistance with retrieving\nlemmas, generating tactics, or even complete proofs, it lacks a crucial\ncapability: providing a sense of proof progress. This limitation particularly\nimpacts the overall development efficiency in large formalization projects. We\nintroduce LeanProgress, a method that predicts the progress in the proof.\nTraining and evaluating our models made on a large corpus of Lean proofs from\nLean Workbook Plus and Mathlib4 and how many steps remain to complete it, we\nemploy data preprocessing and balancing techniques to handle the skewed\ndistribution of proof lengths. Our experiments show that LeanProgress achieves\nan overall prediction accuracy of 75.1\\% in predicting the amount of progress\nand, hence, the remaining number of steps. When integrated into a best-first\nsearch framework using Reprover, our method shows a 3.8\\% improvement on\nMathlib4 compared to baseline performances of 41.2\\%, particularly for longer\nproofs. These results demonstrate how proof progress prediction can enhance\nboth automated and interactive theorem proving, enabling users to make more\ninformed decisions about proof strategies.', 'Proof assistants like Lean have revolutionized mathematical proof\nverification, ensuring high accuracy and reliability. Although large language\nmodels (LLMs) show promise in mathematical reasoning, their advancement in\nformal theorem proving is hindered by a lack of training data. To address this\nissue, we introduce an approach to generate extensive Lean 4 proof data derived\nfrom high-school and undergraduate-level mathematical competition problems.\nThis approach involves translating natural language problems into formal\nstatements, filtering out low-quality statements, and generating proofs to\ncreate synthetic data. After fine-tuning the DeepSeekMath 7B model on this\nsynthetic dataset, which comprises 8 million formal statements with proofs, our\nmodel achieved whole-proof generation accuracies of 46.3% with 64 samples and\n52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at\n23.0% with 64 samples and a tree search reinforcement learning method at 41.0%.\nAdditionally, our model successfully proved 5 out of 148 problems in the Lean 4\nFormalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4\nfailed to prove any. These results demonstrate the potential of leveraging\nlarge-scale synthetic data to enhance theorem-proving capabilities in LLMs.\nBoth the synthetic dataset and the model will be made available to facilitate\nfurther research in this promising field.', 'Large Language Models (LLMs) have demonstrated significant potential in\ngenerating mathematical proofs. However, a persistent challenge is that LLMs\noccasionally make mistakes, while even a minor mistake can invalidate an entire\nproof. Proof assistants like Lean offer a great remedy. They are designed for\nverifying each step of a proof in a formal language, and in recent years\nresearchers have created AI models to generate proofs in their languages.\nHowever, the scarcity of large-scale datasets of Lean proofs restrict the\nperformance of such Automated Theorem Proving (ATP) models.\n  We developed LeanNavigator, a novel method for generating a large-scale\ndataset of Lean theorems and proofs by finding new ways to prove existing Lean\ntheorems. By leveraging an interactive Lean client and an efficient method for\nproof step generation, LeanNavigator efficiently produces new theorems with\ncorresponding proofs. Applying this approach to Mathlib4, we generated 4.7\nmillion theorems totaling 1 billion tokens, surpassing previous datasets by\nmore than an order of magnitude. Using this extensive dataset, we trained an AI\nmodel that outperforms the state-of-the-art ReProver model in theorem-proving\ntasks. These results confirm our hypothesis and demonstrate the critical role\nof large datasets in improving the performance of automated theorem provers.']"
79,46,79_privacy_policies_compliance_sensitive,"['privacy', 'policies', 'compliance', 'sensitive', 'risks', 'protection', 'pii', 'utility', 'policy', 'users']","[""Data protection and privacy is becoming increasingly crucial in the digital\nera. Numerous companies depend on third-party vendors and service providers to\ncarry out critical functions within their operations, encompassing tasks such\nas data handling and storage. However, this reliance introduces potential\nvulnerabilities, as these vendors' security measures and practices may not\nalways align with the standards expected by regulatory bodies. Businesses are\nrequired, often under the penalty of law, to ensure compliance with the\nevolving regulatory rules. Interpreting and implementing these regulations pose\nchallenges due to their complexity. Regulatory documents are extensive,\ndemanding significant effort for interpretation, while vendor-drafted privacy\npolicies often lack the detail required for full legal compliance, leading to\nambiguity. To ensure a concise interpretation of the regulatory requirements\nand compliance of organizational privacy policy with said regulations, we\npropose a Large Language Model (LLM) and Semantic Web based approach for\nprivacy compliance. In this paper, we develop the novel Privacy Policy\nCompliance Verification Knowledge Graph, PrivComp-KG. It is designed to\nefficiently store and retrieve comprehensive information concerning privacy\npolicies, regulatory frameworks, and domain-specific knowledge pertaining to\nthe legal landscape of privacy. Using Retrieval Augmented Generation, we\nidentify the relevant sections in a privacy policy with corresponding\nregulatory rules. This information about individual privacy policies is\npopulated into the PrivComp-KG. Combining this with the domain context and\nrules, the PrivComp-KG can be queried to check for compliance with privacy\npolicies by each vendor against relevant policy regulations. We demonstrate the\nrelevance of the PrivComp-KG, by verifying compliance of privacy policy\ndocuments for various organizations."", ""Recent advancements in generative large language models (LLMs) have enabled\nwider applicability, accessibility, and flexibility. However, their reliability\nand trustworthiness are still in doubt, especially for concerns regarding\nindividuals' data privacy. Great efforts have been made on privacy by building\nvarious evaluation benchmarks to study LLMs' privacy awareness and robustness\nfrom their generated outputs to their hidden representations. Unfortunately,\nmost of these works adopt a narrow formulation of privacy and only investigate\npersonally identifiable information (PII). In this paper, we follow the merit\nof the Contextual Integrity (CI) theory, which posits that privacy evaluation\nshould not only cover the transmitted attributes but also encompass the whole\nrelevant social context through private information flows. We present\nPrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted\nat legal compliance to cover well-annotated privacy and safety regulations,\nreal court cases, privacy policies, and synthetic data built from the official\ntoolkit to study LLMs' privacy and safety compliance. We evaluate the latest\nLLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our\nexperimental results suggest that though LLMs can effectively capture key CI\nparameters inside a given context, they still require further advancements for\nprivacy compliance."", ""The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights.""]"
80,46,80_activity_health_har_wearable,"['activity', 'health', 'har', 'wearable', 'sensor', 'sensing', 'sleep', 'recognition', 'activities', 'sensors']","['The astonishing success of Large Language Models (LLMs) in Natural Language\nProcessing (NLP) has spurred their use in many application domains beyond text\nanalysis, including wearable sensor-based Human Activity Recognition (HAR). In\nsuch scenarios, often sensor data are directly fed into an LLM along with text\ninstructions for the model to perform activity classification. Seemingly\nremarkable results have been reported for such LLM-based HAR systems when they\nare evaluated on standard benchmarks from the field. Yet, we argue, care has to\nbe taken when evaluating LLM-based HAR systems in such a traditional way. Most\ncontemporary LLMs are trained on virtually the entire (accessible) internet --\npotentially including standard HAR datasets. With that, it is not unlikely that\nLLMs actually had access to the test data used in such benchmark\nexperiments.The resulting contamination of training data would render these\nexperimental evaluations meaningless. In this paper we investigate whether LLMs\nindeed have had access to standard HAR datasets during training. We apply\nmemorization tests to LLMs, which involves instructing the models to extend\ngiven snippets of data. When comparing the LLM-generated output to the original\ndata we found a non-negligible amount of matches which suggests that the LLM\nunder investigation seems to indeed have seen wearable sensor data from the\nbenchmark datasets during training. For the Daphnet dataset in particular,\nGPT-4 is able to reproduce blocks of sensor readings. We report on our\ninvestigations and discuss potential implications on HAR research, especially\nwith regards to reporting results on experimental evaluation', 'Human Activity Recognition (HAR) using Inertial Measurement Unit (IMU)\nsensors is critical for applications in healthcare, safety, and industrial\nproduction. However, variations in activity patterns, device types, and sensor\nplacements create distribution gaps across datasets, reducing the performance\nof HAR models. To address this, we propose LanHAR, a novel system that\nleverages Large Language Models (LLMs) to generate semantic interpretations of\nsensor readings and activity labels for cross-dataset HAR. This approach not\nonly mitigates cross-dataset heterogeneity but also enhances the recognition of\nnew activities. LanHAR employs an iterative re-generation method to produce\nhigh-quality semantic interpretations with LLMs and a two-stage training\nframework that bridges the semantic interpretations of sensor readings and\nactivity labels. This ultimately leads to a lightweight sensor encoder suitable\nfor mobile deployment, enabling any sensor reading to be mapped into the\nsemantic interpretation space. Experiments on five public datasets demonstrate\nthat our approach significantly outperforms state-of-the-art methods in both\ncross-dataset HAR and new activity recognition. The source code will be made\npublicly available.', 'The proliferation of wearable technology enables the generation of vast\namounts of sensor data, offering significant opportunities for advancements in\nhealth monitoring, activity recognition, and personalized medicine. However,\nthe complexity and volume of this data present substantial challenges in data\nmodeling and analysis, which have been tamed with approaches spanning time\nseries modeling to deep learning techniques. The latest frontier in this domain\nis the adoption of Large Language Models (LLMs), such as GPT-4 and Llama, for\ndata analysis, modeling, understanding, and generation of human behavior\nthrough the lens of wearable sensor data. This survey explores current trends\nand challenges in applying LLMs for sensor-based human activity recognition and\nbehavior modeling. We discuss the nature of wearable sensors data, the\ncapabilities and limitations of LLMs to model them and their integration with\ntraditional machine learning techniques. We also identify key challenges,\nincluding data quality, computational requirements, interpretability, and\nprivacy concerns. By examining case studies and successful applications, we\nhighlight the potential of LLMs in enhancing the analysis and interpretation of\nwearable sensors data. Finally, we propose future directions for research,\nemphasizing the need for improved preprocessing techniques, more efficient and\nscalable models, and interdisciplinary collaboration. This survey aims to\nprovide a comprehensive overview of the intersection between wearable sensors\ndata and LLMs, offering insights into the current state and future prospects of\nthis emerging field.']"
81,42,81_biases_ai_sycophancy_cognitive,"['biases', 'ai', 'sycophancy', 'cognitive', 'bias', 'chatgpt', 'human', 'fce', 'conservation', 'nudging']","[""Recent studies have revealed that large language model (LLM)-powered\nconversational agents often exhibit `sycophancy', a tendency to adapt their\nresponses to align with user perspectives, even at the expense of factual\naccuracy. However, users' perceptions of LLM sycophancy and its interplay with\nother anthropomorphic features (e.g., friendliness) in shaping user trust\nremains understudied. To bridge this gap, we conducted a 2 (Sycophancy:\npresence vs. absence) x 2 (Friendliness: high vs. low) between-subjects\nexperiment (N = 224). Our study uncovered, for the first time, the intricate\ndynamics between LLM sycophancy and friendliness: When an LLM agent already\nexhibits a friendly demeanor, being sycophantic reduces perceived authenticity,\nthereby lowering user trust; Conversely, when the agent is less friendly,\naligning its responses with user opinions makes it appear more genuine, leading\nto higher user trust. Our findings entail profound implications for AI\npersuasion through exploiting human psychological tendencies and highlight the\nimperative for responsible designs in user-LLM agent interactions."", ""Warning: This paper contains examples of stereotypes and biases. Large\nLanguage Models (LLMs) exhibit considerable social biases, and various studies\nhave tried to evaluate and mitigate these biases accurately. Previous studies\nuse downstream tasks as prompts to examine the degree of social biases for\nevaluation and mitigation. While LLMs' output highly depends on prompts,\nprevious studies evaluating and mitigating bias have often relied on a limited\nvariety of prompts. In this paper, we investigate the sensitivity of LLMs when\nchanging prompt variations (task instruction and prompt, few-shot examples,\ndebias-prompt) by analyzing task performance and social bias of LLMs. Our\nexperimental results reveal that LLMs are highly sensitive to prompts to the\nextent that the ranking of LLMs fluctuates when comparing models for task\nperformance and social bias. Additionally, we show that LLMs have tradeoffs\nbetween performance and social bias caused by the prompts. Less bias from\nprompt setting may result in reduced performance. Moreover, the ambiguity of\ninstances is one of the reasons for this sensitivity to prompts in advanced\nLLMs, leading to various outputs. We recommend using diverse prompts, as in\nthis study, to compare the effects of prompts on social bias in LLMs."", ""Large Language Models (LLMs) are trained on large corpora written by humans\nand demonstrate high performance on various tasks. However, as humans are\nsusceptible to cognitive biases, which can result in irrational judgments, LLMs\ncan also be influenced by these biases, leading to irrational decision-making.\nFor example, changing the order of options in multiple-choice questions affects\nthe performance of LLMs due to order bias. In our research, we first conducted\nan extensive survey of existing studies examining LLMs' cognitive biases and\ntheir mitigation. The mitigation techniques in LLMs have the disadvantage that\nthey are limited in the type of biases they can apply or require lengthy inputs\nor outputs. We then examined the effectiveness of two mitigation methods for\nhumans, SoPro and AwaRe, when applied to LLMs, inspired by studies in\ncrowdsourcing. To test the effectiveness of these methods, we conducted\nexperiments on GPT-3.5 and GPT-4 to evaluate the influence of six biases on the\noutputs before and after applying these methods. The results demonstrate that\nwhile SoPro has little effect, AwaRe enables LLMs to mitigate the effect of\nthese biases and make more rational responses.""]"
82,41,82_smart_contracts_blockchain_contract,"['smart', 'contracts', 'blockchain', 'contract', 'vulnerabilities', 'auditing', 'solidity', 'erc', 'decentralized', 'security']","['Background: Health 3.0 allows decision making to be based on longitudinal\ndata from multiple institutions, from across the patient\'s healthcare journey.\nIn such a distributed setting, blockchain smart contracts can act as neutral\nintermediaries to implement trustworthy decision making.\n  Objective: In a distributed setting, transmitted data will be structured\nusing standards (such as HL7 FHIR) for semantic interoperability. In turn, the\nsmart contract will require interoperability with this standard, implement a\ncomplex communication setup (e.g., using oracles), and be developed using\nblockchain languages (e.g., Solidity). We propose the encoding of smart\ncontract logic using a high-level semantic Knowledge Graph, using concepts from\nthe domain standard. We then deploy this semantic KG on blockchain.\n  Methods: Off-chain, a code generation pipeline compiles the KG into a\nconcrete smart contract, which is then deployed on-chain. Our pipeline targets\nan intermediary bridge representation, which can be transpiled into a specific\nblockchain language. Our choice avoids on-chain rule engines, with\nunpredictable and likely higher computational cost; it is thus in line with the\neconomic rules of blockchain.\n  Results: We applied our code generation approach to generate smart contracts\nfor 3 health insurance cases from Medicare. We discuss the suitability of our\napproach - the need for a neutral intermediary - for a number of healthcare use\ncases. Our evaluation finds that the generated contracts perform well in terms\nof correctness and execution cost (""gas"") on blockchain.\n  Conclusions: We showed that it is feasible to automatically generate smart\ncontract code based on a semantic KG, in a way that respects the economic rules\nof blockchain. Future work includes studying the use of Large Language Models\n(LLM) in our approach, and evaluations on other blockchains.', 'The rise of blockchain technologies has greatly accelerated the development\nand deployment of smart contracts. However, their inherent vulnerabilities and\nsusceptibility to bugs have led to significant financial losses, underscoring\nthe challenges in securing smart contracts. While traditional auditing methods\nare crucial, they often fall short in addressing the increasing complexity and\nvolume of smart contracts. Recent advancements in Large Language Models (LLMs)\noffer promising solutions for enhancing software auditing by automatically\nidentifying security vulnerabilities. Despite their potential, the practical\napplication of these models is hindered by substantial computational demands.\nThis paper investigates the feasibility of using smaller, fine-tuned models to\nachieve comparable or even superior results in smart contract auditing. We\nintroduce the FTSmartAudit framework, which is designed to develop\ncost-effective, specialized models for smart contract auditing through the\nfine-tuning of LLMs. Our contributions include: (1) a single-task learning\nframework that streamlines data preparation, training, evaluation, and\ncontinuous learning; (2) a robust dataset generation method utilizing\ndomain-special knowledge distillation to produce high-quality datasets from\nadvanced models like GPT-4o; (3) an adaptive learning strategy to maintain\nmodel accuracy and robustness; (4) the proven effectiveness of fine-tuned\nmodels in detecting specific vulnerabilities and complex logical errors; and\n(5) a framework that can be extended to other domains requiring LLM solutions.\nOur experimental results demonstrate that smaller models can surpass\nstate-of-the-art commercial models and tools in detecting vulnerabilities in\nsmart contracts.', 'The immutable nature of blockchain technology, while revolutionary,\nintroduces significant security challenges, particularly in smart contracts.\nThese security issues can lead to substantial financial losses. Current tools\nand approaches often focus on specific types of vulnerabilities. However, a\ncomprehensive tool capable of detecting a wide range of vulnerabilities with\nhigh accuracy is lacking. This paper introduces LLM-SmartAudit, a novel\nframework leveraging the advanced capabilities of Large Language Models (LLMs)\nto detect and analyze vulnerabilities in smart contracts. Using a multi-agent\nconversational approach, LLM-SmartAudit employs a collaborative system with\nspecialized agents to enhance the audit process. To evaluate the effectiveness\nof LLM-SmartAudit, we compiled two distinct datasets: a labeled dataset for\nbenchmarking against traditional tools and a real-world dataset for assessing\npractical applications. Experimental results indicate that our solution\noutperforms all traditional smart contract auditing tools, offering higher\naccuracy and greater efficiency. Furthermore, our framework can detect complex\nlogic vulnerabilities that traditional tools have previously overlooked. Our\nfindings demonstrate that leveraging LLM agents provides a highly effective\nmethod for automated smart contract auditing.']"
83,41,83_personality_traits_personalities_big,"['personality', 'traits', 'personalities', 'big', 'psychological', 'trait', 'five', 'psychometrics', 'questionnaires', 'psychometric']","[""Large Language Models (LLMs) are increasingly used in everyday life and\nresearch. One of the most common use cases is conversational interactions,\nenabled by the language generation capabilities of LLMs. Just as between two\nhumans, a conversation between an LLM-powered entity and a human depends on the\npersonality of the conversants. However, measuring the personality of a given\nLLM is currently a challenge. This paper introduces the Language Model\nLinguistic Personality Assessment (LMLPA), a system designed to evaluate the\nlinguistic personalities of LLMs. Our system helps to understand LLMs' language\ngeneration capabilities by quantitatively assessing the distinct personality\ntraits reflected in their linguistic outputs. Unlike traditional human-centric\npsychometrics, the LMLPA adapts a personality assessment questionnaire,\nspecifically the Big Five Inventory, to align with the operational capabilities\nof LLMs, and also incorporates the findings from previous language-based\npersonality measurement literature. To mitigate sensitivity to the order of\noptions, our questionnaire is designed to be open-ended, resulting in textual\nanswers. Thus, the AI rater is needed to transform ambiguous personality\ninformation from text responses into clear numerical indicators of personality\ntraits. Utilising Principal Component Analysis and reliability validations, our\nfindings demonstrate that LLMs possess distinct personality traits that can be\neffectively quantified by the LMLPA. This research contributes to\nHuman-Computer Interaction and Human-Centered AI, providing a robust framework\nfor future studies to refine AI personality assessments and expand their\napplications in multiple areas, including education and manufacturing."", ""Personality psychologists have analyzed the relationship between personality\nand safety behaviors in human society. Although Large Language Models (LLMs)\ndemonstrate personality traits, the relationship between personality traits and\nsafety abilities in LLMs still remains a mystery. In this paper, we discover\nthat LLMs' personality traits are closely related to their safety abilities,\ni.e., toxicity, privacy, and fairness, based on the reliable MBTI-M scale.\nMeanwhile, the safety alignment generally increases various LLMs' Extraversion,\nSensing, and Judging traits. According to such findings, we can edit LLMs'\npersonality traits and improve their safety performance, e.g., inducing\npersonality from ISTJ to ISTP resulted in a relative improvement of\napproximately 43% and 10% in privacy and fairness performance, respectively.\nAdditionally, we find that LLMs with different personality traits are\ndifferentially susceptible to jailbreak. This study pioneers the investigation\nof LLM safety from a personality perspective, providing new insights into LLM\nsafety enhancement."", 'Personality recognition aims to identify the personality traits implied in\nuser data such as dialogues and social media posts. Current research\npredominantly treats personality recognition as a classification task, failing\nto reveal the supporting evidence for the recognized personality. In this\npaper, we propose a novel task named Explainable Personality Recognition,\naiming to reveal the reasoning process as supporting evidence of the\npersonality trait. Inspired by personality theories, personality traits are\nmade up of stable patterns of personality state, where the states are\nshort-term characteristic patterns of thoughts, feelings, and behaviors in a\nconcrete situation at a specific moment in time. We propose an explainable\npersonality recognition framework called Chain-of-Personality-Evidence (CoPE),\nwhich involves a reasoning process from specific contexts to short-term\npersonality states to long-term personality traits. Furthermore, based on the\nCoPE framework, we construct an explainable personality recognition dataset\nfrom dialogues, PersonalityEvd. We introduce two explainable personality state\nrecognition and explainable personality trait recognition tasks, which require\nmodels to recognize the personality state and trait labels and their\ncorresponding support evidence. Our extensive experiments based on Large\nLanguage Models on the two tasks show that revealing personality traits is very\nchallenging and we present some insights for future research. Our data and code\nare available at https://github.com/Lei-Sun-RUC/PersonalityEvd.']"
84,40,84_evolutionary_optimization_heuristics_algorithms,"['evolutionary', 'optimization', 'heuristics', 'algorithms', 'heuristic', 'search', 'problems', 'multiobjective', 'design', 'algorithm']","[""Combinatorial optimization problems often rely on heuristic algorithms to\ngenerate efficient solutions. However, the manual design of heuristics is\nresource-intensive and constrained by the designer's expertise. Recent advances\nin artificial intelligence, particularly large language models (LLMs), have\ndemonstrated the potential to automate heuristic generation through\nevolutionary frameworks. Recent works focus only on well-known combinatorial\noptimization problems like the traveling salesman problem and online bin\npacking problem when designing constructive heuristics. This study investigates\nwhether LLMs can effectively generate heuristics for niche, not yet broadly\nresearched optimization problems, using the unit-load pre-marshalling problem\nas an example case. We propose the Contextual Evolution of Heuristics (CEoH)\nframework, an extension of the Evolution of Heuristics (EoH) framework, which\nincorporates problem-specific descriptions to enhance in-context learning\nduring heuristic generation. Through computational experiments, we evaluate\nCEoH and EoH and compare the results. Results indicate that CEoH enables\nsmaller LLMs to generate high-quality heuristics more consistently and even\noutperform larger models. Larger models demonstrate robust performance with or\nwithout contextualized prompts. The generated heuristics exhibit scalability to\ndiverse instance configurations."", ""Evolutionary algorithms excel in solving complex optimization problems,\nespecially those with multiple objectives. However, their stochastic nature can\nsometimes hinder rapid convergence to the global optima, particularly in\nscenarios involving constraints. In this study, we employ a large language\nmodel (LLM) to enhance evolutionary search for solving constrained\nmulti-objective optimization problems. Our aim is to speed up the convergence\nof the evolutionary population. To achieve this, we finetune the LLM through\ntailored prompt engineering, integrating information concerning both objective\nvalues and constraint violations of solutions. This process enables the LLM to\ngrasp the relationship between well-performing and poorly performing solutions\nbased on the provided input data. Solution's quality is assessed based on their\nconstraint violations and objective-based performance. By leveraging the\nrefined LLM, it can be used as a search operator to generate superior-quality\nsolutions. Experimental evaluations across various test benchmarks illustrate\nthat LLM-aided evolutionary search can significantly accelerate the\npopulation's convergence speed and stands out competitively against\ncutting-edge evolutionary algorithms."", ""Designing optimization approaches, whether heuristic or meta-heuristic,\nusually demands extensive manual intervention and has difficulty generalizing\nacross diverse problem domains. The combination of Large Language Models (LLMs)\nand Evolutionary Algorithms (EAs) offers a promising new approach to overcome\nthese limitations and make optimization more automated. In this setup, LLMs act\nas dynamic agents that can generate, refine, and interpret optimization\nstrategies, while EAs efficiently explore complex solution spaces through\nevolutionary operators. Since this synergy enables a more efficient and\ncreative search process, we first conduct an extensive review of recent\nresearch on the application of LLMs in optimization. We focus on LLMs' dual\nfunctionality as solution generators and algorithm designers. Then, we\nsummarize the common and valuable designs in existing work and propose a novel\nLLM-EA paradigm for automated optimization. Furthermore, centered on this\nparadigm, we conduct an in-depth analysis of innovative methods for three key\ncomponents: individual representation, variation operators, and fitness\nevaluation. We address challenges related to heuristic generation and solution\nexploration, especially from the LLM prompts' perspective. Our systematic\nreview and thorough analysis of the paradigm can assist researchers in better\nunderstanding the current research and promoting the development of combining\nLLMs with EAs for automated optimization.""]"
85,40,85_motion_motions_hoi_gait,"['motion', 'motions', 'hoi', 'gait', 'gestures', 'gesture', 'body', 'movements', 'generation', 'human']","[""Human motion synthesis is a fundamental task in computer animation. Despite\nrecent progress in this field utilizing deep learning and motion capture data,\nexisting methods are always limited to specific motion categories,\nenvironments, and styles. This poor generalizability can be partially\nattributed to the difficulty and expense of collecting large-scale and\nhigh-quality motion data. At the same time, foundation models trained with\ninternet-scale image and text data have demonstrated surprising world knowledge\nand reasoning ability for various downstream tasks. Utilizing these foundation\nmodels may help with human motion synthesis, which some recent works have\nsuperficially explored. However, these methods didn't fully unveil the\nfoundation models' potential for this task and only support several simple\nactions and environments. In this paper, we for the first time, without any\nmotion data, explore open-set human motion synthesis using natural language\ninstructions as user control signals based on MLLMs across any motion task and\nenvironment. Our framework can be split into two stages: 1) sequential keyframe\ngeneration by utilizing MLLMs as a keyframe designer and animator; 2) motion\nfilling between keyframes through interpolation and motion tracking. Our method\ncan achieve general human motion synthesis for many downstream tasks. The\npromising results demonstrate the worth of mocap-free human motion synthesis\naided by MLLMs and pave the way for future research."", ""Generating lifelike human motions from descriptive texts has experienced\nremarkable research focus in the recent years, propelled by the emerging\nrequirements of digital humans.Despite impressive advances, existing approaches\nare often constrained by limited control modalities, task specificity, and\nfocus solely on body motion representations.In this paper, we present\nMotionGPT-2, a unified Large Motion-Language Model (LMLM) that addresses these\nlimitations. MotionGPT-2 accommodates multiple motion-relevant tasks and\nsupporting multimodal control conditions through pre-trained Large Language\nModels (LLMs). It quantizes multimodal inputs-such as text and single-frame\nposes-into discrete, LLM-interpretable tokens, seamlessly integrating them into\nthe LLM's vocabulary. These tokens are then organized into unified prompts,\nguiding the LLM to generate motion outputs through a\npretraining-then-finetuning paradigm. We also show that the proposed\nMotionGPT-2 is highly adaptable to the challenging 3D holistic motion\ngeneration task, enabled by the innovative motion discretization framework,\nPart-Aware VQVAE, which ensures fine-grained representations of body and hand\nmovements. Extensive experiments and visualizations validate the effectiveness\nof our method, demonstrating the adaptability of MotionGPT-2 across motion\ngeneration, motion captioning, and generalized motion completion tasks."", 'Recent advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural and contextually relevant text,\nenabling more human-like AI interactions. However, generating and understanding\ninteractive human-like motion, where multiple individuals engage in coordinated\nmovements, remains challenging due to the complexity of modeling these\ninteractions. Additionally, a unified and versatile model is needed to handle\ndiverse interactive scenarios, such as chat systems that dynamically adapt to\nuser instructions and assigned roles. To address these challenges, we introduce\nVIM, the Versatile Interactive Motion-language model, which integrates both\nlanguage and motion modalities to effectively understand, generate, and control\ninteractive motions in multi-turn conversational contexts. Unlike previous\nstudies that primarily focus on uni-directional tasks such as text-to-motion or\nmotion-to-text, VIM employs a unified architecture capable of simultaneously\nunderstanding and generating both motion and text modalities. Given the absence\nof an appropriate dataset to support this task, we introduce Inter-MT2, a\nlarge-scale instruction-tuning dataset containing 82.7K multi-turn interactive\nmotion instructions, covering 153K interactive motion samples. Inter-MT2 spans\ndiverse instructional scenarios, including motion editing, question answering,\nand story generation, leveraging off-the-shelf large language models and motion\ndiffusion models to construct a broad set of interactive motion instructions.\nWe extensively evaluate the versatility of VIM across multiple interactive\nmotion-related tasks, including motion-to-text, text-to-motion, reaction\ngeneration, motion editing, and reasoning about motion sequences.']"
86,40,86_cad_spatial_scene_3d,"['cad', 'spatial', 'scene', '3d', 'design', 'layout', 'layouts', 'scenes', 'geometric', 'objects']","[""This paper aims to design a unified Computer-Aided Design (CAD) generation\nsystem that can easily generate CAD models based on the user's inputs in the\nform of textual description, images, point clouds, or even a combination of\nthem. Towards this goal, we introduce the CAD-MLLM, the first system capable of\ngenerating parametric CAD models conditioned on the multimodal input.\nSpecifically, within the CAD-MLLM framework, we leverage the command sequences\nof CAD models and then employ advanced large language models (LLMs) to align\nthe feature space across these diverse multi-modalities data and CAD models'\nvectorized representations. To facilitate the model training, we design a\ncomprehensive data construction and annotation pipeline that equips each CAD\nmodel with corresponding multimodal data. Our resulting dataset, named\nOmni-CAD, is the first multimodal CAD dataset that contains textual\ndescription, multi-view images, points, and command sequence for each CAD\nmodel. It contains approximately 450K instances and their CAD construction\nsequences. To thoroughly evaluate the quality of our generated CAD models, we\ngo beyond current evaluation metrics that focus on reconstruction quality by\nintroducing additional metrics that assess topology quality and surface\nenclosure extent. Extensive experimental results demonstrate that CAD-MLLM\nsignificantly outperforms existing conditional generative methods and remains\nhighly robust to noises and missing points. The project page and more\nvisualizations can be found at: https://cad-mllm.github.io/"", 'Computer-Aided Design (CAD) models are typically constructed by sequentially\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\nand CAD operation sequences from 3D representations such as point clouds. In\nthis paper, we address this challenge through novel contributions across three\nlevels: CAD sequence representation, network design, and training dataset. In\nparticular, we represent CAD sketch-extrude sequences as Python code. The\nproposed CAD-Recode translates a point cloud into Python code that, when\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\npre-trained Large Language Models (LLMs) to Python code, we leverage a\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\nlightweight point cloud projector. CAD-Recode is trained on a procedurally\ngenerated dataset of one million CAD sequences. CAD-Recode significantly\noutperforms existing methods across the DeepCAD, Fusion360 and real-world CC3D\ndatasets. Furthermore, we show that our CAD Python code output is interpretable\nby off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\nfrom point clouds.', 'Computer-aided design (CAD) significantly enhances the efficiency, accuracy,\nand innovation of design processes by enabling precise 2D and 3D modeling,\nextensive analysis, and optimization. Existing methods for creating CAD models\nrely on latent vectors or point clouds, which are difficult to obtain and\ncostly to store. Recent advances in Multimodal Large Language Models (MLLMs)\nhave inspired researchers to use natural language instructions and images for\nCAD model construction. However, these models still struggle with inferring\naccurate 3D spatial location and orientation, leading to inaccuracies in\ndetermining the spatial 3D starting points and extrusion directions for\nconstructing geometries. This work introduces CAD-GPT, a CAD synthesis method\nwith spatial reasoning-enhanced MLLM that takes either a single image or a\ntextual description as input. To achieve precise spatial inference, our\napproach introduces a 3D Modeling Spatial Mechanism. This method maps 3D\nspatial positions and 3D sketch plane rotation angles into a 1D linguistic\nfeature space using a specialized spatial unfolding mechanism, while\ndiscretizing 2D sketch coordinates into an appropriate planar space to enable\nprecise determination of spatial starting position, sketch orientation, and 2D\nsketch coordinate translations. Extensive experiments demonstrate that CAD-GPT\nconsistently outperforms existing state-of-the-art methods in CAD model\nsynthesis, both quantitatively and qualitatively.']"
87,40,87_hallucinations_hallucination_lvlms_mllms,"['hallucinations', 'hallucination', 'lvlms', 'mllms', 'visual', 'image', 'object', 'lvlm', 'oh', 'mitigation']","['Hallucination poses a challenge to the deployment of large vision-language\nmodels (LVLMs) in applications. Unlike in large language models (LLMs),\nhallucination in LVLMs often arises from misalignments between visual inputs\nand textual outputs. This paper investigates the underlying mechanisms of\nhallucination, focusing on the unique structure of LVLMs that distinguishes\nthem from large language models (LLMs). We identify that hallucinations often\narise from the sensitivity of text decoders to vision inputs, a natural\nphenomenon when image encoders and text decoders are pre-trained separately.\nInspired by this, we introduce Visual and Textual Intervention (VTI), a novel\ntechnique designed to reduce hallucinations by steering latent space\nrepresentations during inference to enhance the stability of vision features.\nAs a task-agnostic test-time intervention, VTI can be easily applied to any\nproblem without additional cost. Extensive experiments demonstrate that it can\neffectively reduce hallucinations and outperform baseline methods across\nmultiple metrics, highlighting the critical role of vision feature stability in\nLVLMs.', ""Large Vision Language Models (LVLMs) are increasingly integral to healthcare\napplications, including medical visual question answering and imaging report\ngeneration. While these models inherit the robust capabilities of foundational\nLarge Language Models (LLMs), they also inherit susceptibility to\nhallucinations-a significant concern in high-stakes medical contexts where the\nmargin for error is minimal. However, currently, there are no dedicated methods\nor benchmarks for hallucination detection and evaluation in the medical field.\nTo bridge this gap, we introduce Med-HallMark, the first benchmark specifically\ndesigned for hallucination detection and evaluation within the medical\nmultimodal domain. This benchmark provides multi-tasking hallucination support,\nmultifaceted hallucination data, and hierarchical hallucination categorization.\nFurthermore, we propose the MediHall Score, a new medical evaluative metric\ndesigned to assess LVLMs' hallucinations through a hierarchical scoring system\nthat considers the severity and type of hallucination, thereby enabling a\ngranular assessment of potential clinical impacts. We also present\nMediHallDetector, a novel Medical LVLM engineered for precise hallucination\ndetection, which employs multitask training for hallucination detection.\nThrough extensive experimental evaluations, we establish baselines for popular\nLVLMs using our benchmark. The findings indicate that MediHall Score provides a\nmore nuanced understanding of hallucination impacts compared to traditional\nmetrics and demonstrate the enhanced performance of MediHallDetector. We hope\nthis work can significantly improve the reliability of LVLMs in medical\napplications. All resources of this work will be released soon."", 'The Large Visual Language Models (LVLMs) enhances user interaction and\nenriches user experience by integrating visual modality on the basis of the\nLarge Language Models (LLMs). It has demonstrated their powerful information\nprocessing and generation capabilities. However, the existence of\nhallucinations has limited the potential and practical effectiveness of LVLM in\nvarious fields. Although lots of work has been devoted to the issue of\nhallucination mitigation and correction, there are few reviews to summary this\nissue. In this survey, we first introduce the background of LVLMs and\nhallucinations. Then, the structure of LVLMs and main causes of hallucination\ngeneration are introduced. Further, we summary recent works on hallucination\ncorrection and mitigation. In addition, the available hallucination evaluation\nbenchmarks for LVLMs are presented from judgmental and generative perspectives.\nFinally, we suggest some future research directions to enhance the\ndependability and utility of LVLMs.']"
88,39,88_token_layers_representations_dimension,"['token', 'layers', 'representations', 'dimension', 'neural', 'topological', 'transformer', 'tokens', 'mathcalnc', 'input']","[""The Transformer architecture has emerged as the dominant choice for building\nlarge language models (LLMs). However, with new LLMs emerging on a frequent\nbasis, it is important to consider the potential value of architecture-agnostic\napproaches that can provide interpretability across a variety of architectures.\nDespite recent successes in the interpretability of LLMs, many existing\napproaches rely on complex methods that are often tied to a specific model\ndesign and come with a significant computational cost. To address these\nlimitations, we propose a novel technique, called NormXLogit, for assessing the\nsignificance of individual input tokens. This method operates based on the\ninput and output representations associated with each token. First, we\ndemonstrate that during the pre-training of LLMs, the norms of word embeddings\ncapture the importance of input tokens. Second, we reveal a significant\nrelationship between a token's importance and the extent to which its\nrepresentation can resemble the model's final prediction. Through extensive\nanalysis, we show that our approach consistently outperforms existing\ngradient-based methods in terms of faithfulness. Additionally, our method\nachieves better performance in layer-wise explanations compared to the most\nprominent architecture-specific methods."", 'We explore the topology of representation manifolds arising in autoregressive\nneural language models trained on raw text data. In order to study their\nproperties, we introduce tools from computational algebraic topology, which we\nuse as a basis for a measure of topological complexity, that we call\nperforation.\n  Using this measure, we study the evolution of topological structure in GPT\nbased large language models across depth and time during training. We then\ncompare these to gated recurrent models, and show that the latter exhibit more\ntopological complexity, with a distinct pattern of changes common to all\nnatural languages but absent from synthetically generated data. The paper\npresents a detailed analysis of the representation manifolds derived by these\nmodels based on studying the shapes of vector clouds induced by them as they\nare conditioned on sentences from corpora of natural language text.\n  The methods developed in this paper are novel in the field and based on\nmathematical apparatus that might be unfamiliar to the target audience. To help\nwith that we introduce the minimum necessary theory, and provide additional\nvisualizations in the appendices.\n  The main contribution of the paper is a striking observation about the\ntopological structure of the transformer as compared to LSTM based neural\narchitectures. It suggests that further research into mathematical properties\nof these neural networks is necessary to understand the operation of large\ntransformer language models. We hope this work inspires further explorations in\nthis direction within the NLP community.', 'We investigate the relationship between the geometry of token embeddings and\ntheir role in the next token prediction within transformer models. An important\naspect of this connection uses the notion of empirical measure, which encodes\nthe distribution of token point clouds across transformer layers and drives the\nevolution of token representations in the mean-field interacting picture. We\nuse metrics such as intrinsic dimension, neighborhood overlap, and cosine\nsimilarity to observationally probe these empirical measures across layers. To\nvalidate our approach, we compare these metrics to a dataset where the tokens\nare shuffled, which disrupts the syntactic and semantic structure. Our findings\nreveal a correlation between the geometric properties of token embeddings and\nthe cross-entropy loss of next token predictions, implying that prompts with\nhigher loss values have tokens represented in higher-dimensional spaces.']"
89,38,89_drug_molecular_molecules_molecule,"['drug', 'molecular', 'molecules', 'molecule', 'discovery', 'optimization', 'properties', 'chemical', 'binding', 'design']","['Recent advancements in large language models (LLMs) have demonstrated\nimpressive performance in molecular generation, which offers potential to\naccelerate drug discovery. However, the current LLMs overlook a critical\nrequirement for drug discovery: proposing a diverse set of molecules. This\ndiversity is essential for improving the chances of finding a viable drug, as\nit provides alternative molecules that may succeed where others fail in\nreal-world validations. Nevertheless, the LLMs often output structurally\nsimilar molecules. While decoding schemes like diverse beam search may enhance\ntextual diversity, this often does not align with molecular structural\ndiversity. In response, we propose a new method for fine-tuning molecular\ngenerative LLMs to autoregressively generate a set of structurally diverse\nmolecules, where each molecule is generated by conditioning on the previously\ngenerated molecules. Our approach consists of two stages: (1) supervised\nfine-tuning to adapt LLMs to autoregressively generate molecules in a sequence\nand (2) reinforcement learning to maximize structural diversity within the\ngenerated molecules. Our experiments show that the proposed approach enables\nLLMs to generate diverse molecules better than existing approaches for diverse\nsequence generation.', 'Significant interests have recently risen in leveraging sequence-based large\nlanguage models (LLMs) for drug design. However, most current applications of\nLLMs in drug discovery lack the ability to comprehend three-dimensional (3D)\nstructures, thereby limiting their effectiveness in tasks that explicitly\ninvolve molecular conformations. In this study, we introduced Token-Mol, a\ntoken-only 3D drug design model. This model encodes all molecular information,\nincluding 2D and 3D structures, as well as molecular property data, into\ntokens, which transforms classification and regression tasks in drug discovery\ninto probabilistic prediction problems, thereby enabling learning through a\nunified paradigm. Token-Mol is built on the transformer decoder architecture\nand trained using random causal masking techniques. Additionally, we proposed\nthe Gaussian cross-entropy (GCE) loss function to overcome the challenges in\nregression tasks, significantly enhancing the capacity of LLMs to learn\ncontinuous numerical values. Through a combination of fine-tuning and\nreinforcement learning (RL), Token-Mol achieves performance comparable to or\nsurpassing existing task-specific methods across various downstream tasks,\nincluding pocket-based molecular generation, conformation generation, and\nmolecular property prediction. Compared to existing molecular pre-trained\nmodels, Token-Mol exhibits superior proficiency in handling a wider range of\ndownstream tasks essential for drug design. Notably, our approach improves\nregression task accuracy by approximately 30% compared to similar token-only\nmethods. Token-Mol overcomes the precision limitations of token-only models and\nhas the potential to integrate seamlessly with general models such as ChatGPT,\npaving the way for the development of a universal artificial intelligence drug\ndesign model that facilitates rapid and high-quality drug design by experts.', 'Finetuning a Large Language Model (LLM) is crucial for generating results\ntowards specific objectives. This research delves into the realm of drug\noptimization and introduce a novel reinforcement learning algorithm to finetune\na drug optimization LLM-based generative model, enhancing the original drug\nacross target objectives, while retains the beneficial chemical properties of\nthe original drug. This work is comprised of two primary components: (1)\nDrugImprover: A framework tailored for improving robustness and efficiency in\ndrug optimization. It includes a LLM designed for drug optimization and a novel\nStructured Policy Optimization (SPO) algorithm, which is theoretically\ngrounded. This algorithm offers a unique perspective for fine-tuning the\nLLM-based generative model by aligning the improvement of the generated\nmolecule with the input molecule under desired objectives. (2) A dataset of 1\nmillion compounds, each with OEDOCK docking scores on 5 human proteins\nassociated with cancer cells and 24 binding sites from SARS-CoV-2 virus. We\nconduct a comprehensive evaluation of SPO and demonstrate its effectiveness in\nimproving the original drug across target properties. Our code and dataset will\nbe publicly available at: https://github.com/xuefeng-cs/DrugImproverGPT.']"
90,38,90_adversarial_attack_attacks_robustness,"['adversarial', 'attack', 'attacks', 'robustness', 'teaming', 'redteaming', 'red', 'prompts', 'defenses', 'vulnerabilities']","['The vulnerability of machine learning models in adversarial scenarios has\ngarnered significant interest in the academic community over the past decade,\nresulting in a myriad of attacks and defenses. However, while the community\nappears to be overtly successful in devising new attacks across new contexts,\nthe development of defenses has stalled. After a decade of research, we appear\nno closer to securing AI applications beyond additional training. Despite a\nlack of effective mitigations, AI development and its incorporation into\nexisting systems charge full speed ahead with the rise of generative AI and\nlarge language models. Will our ineffectiveness in developing solutions to\nadversarial threats further extend to these new technologies?\n  In this paper, we argue that overly permissive attack and overly restrictive\ndefensive threat models have hampered defense development in the ML domain.\nThrough the lens of adversarial evasion attacks against neural networks, we\ncritically examine common attack assumptions, such as the ability to bypass any\ndefense not explicitly built into the model. We argue that these flawed\nassumptions, seen as reasonable by the community based on paper acceptance,\nhave encouraged the development of adversarial attacks that map poorly to\nreal-world scenarios. In turn, new defenses evaluated against these very\nattacks are inadvertently required to be almost perfect and incorporated as\npart of the model. But do they need to? In practice, machine learning models\nare deployed as a small component of a larger system. We analyze adversarial\nmachine learning from a system security perspective rather than an AI\nperspective and its implications for emerging AI paradigms.', ""The field of textual adversarial defenses has gained considerable attention\nin recent years due to the increasing vulnerability of natural language\nprocessing (NLP) models to adversarial attacks, which exploit subtle\nperturbations in input text to deceive models. This paper introduces the\nDefensive Dual Masking (DDM) algorithm, a novel approach designed to enhance\nmodel robustness against such attacks. DDM utilizes a unique adversarial\ntraining strategy where [MASK] tokens are strategically inserted into training\nsamples to prepare the model to handle adversarial perturbations more\neffectively. During inference, potentially adversarial tokens are dynamically\nreplaced with [MASK] tokens to neutralize potential threats while preserving\nthe core semantics of the input. The theoretical foundation of our approach is\nexplored, demonstrating how the selective masking mechanism strengthens the\nmodel's ability to identify and mitigate adversarial manipulations. Our\nempirical evaluation across a diverse set of benchmark datasets and attack\nmechanisms consistently shows that DDM outperforms state-of-the-art defense\ntechniques, improving model accuracy and robustness. Moreover, when applied to\nLarge Language Models (LLMs), DDM also enhances their resilience to adversarial\nattacks, providing a scalable defense mechanism for large-scale NLP\napplications."", ""This position paper proposes a novel approach to advancing NLP security by\nleveraging Large Language Models (LLMs) as engines for generating diverse\nadversarial attacks. Building upon recent work demonstrating LLMs'\neffectiveness in creating word-level adversarial examples, we argue for\nexpanding this concept to encompass a broader range of attack types, including\nadversarial patches, universal perturbations, and targeted attacks. We posit\nthat LLMs' sophisticated language understanding and generation capabilities can\nproduce more effective, semantically coherent, and human-like adversarial\nexamples across various domains and classifier architectures. This paradigm\nshift in adversarial NLP has far-reaching implications, potentially enhancing\nmodel robustness, uncovering new vulnerabilities, and driving innovation in\ndefense mechanisms. By exploring this new frontier, we aim to contribute to the\ndevelopment of more secure, reliable, and trustworthy NLP systems for critical\napplications.""]"
91,37,91_zo_gradient_convergence_memory,"['zo', 'gradient', 'convergence', 'memory', 'optimizers', 'optimizer', 'adam', 'lowrank', 'memoryefficient', 'galore']","['The memory challenges associated with training Large Language Models (LLMs)\nhave become a critical concern, particularly when using the Adam optimizer. To\naddress this issue, numerous memory-efficient techniques have been proposed,\nwith GaLore standing out as a notable example designed to reduce the memory\nfootprint of optimizer states. However, these approaches do not alleviate the\nmemory burden imposed by activations, rendering them unsuitable for scenarios\ninvolving long context sequences or large mini-batches. Moreover, their\nconvergence properties are still not well-understood in the literature. In this\nwork, we introduce a Randomized Subspace Optimization framework for\npre-training and fine-tuning LLMs. Our approach decomposes the high-dimensional\ntraining problem into a series of lower-dimensional subproblems. At each\niteration, a random subspace is selected, and the parameters within that\nsubspace are optimized. This structured reduction in dimensionality allows our\nmethod to simultaneously reduce memory usage for both activations and optimizer\nstates. We establish comprehensive convergence guarantees and derive rates for\nvarious scenarios, accommodating different optimization strategies to solve the\nsubproblems. Extensive experiments validate the superior memory and\ncommunication efficiency of our method, achieving performance comparable to\nGaLore and Adam.', ""Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks."", 'Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the\nsuccess of large language models. However, they often require to maintain\noptimizer states throughout training, which can result in memory requirements\nseveral times greater than the model footprint. This overhead imposes\nconstraints on scalability and computational efficiency. Stochastic Gradient\nDescent (SGD), in contrast, is a stateless optimizer, as it does not track\nstate variables during training. Consequently, it achieves optimal memory\nefficiency. However, its capability in LLM training is limited (Zhao et al.,\n2024b). In this work, we show that pre-processing SGD in a stateless manner can\nachieve the same performance as the Adam optimizer for LLM training, while\ndrastically reducing the memory cost. Specifically, we propose to pre-process\nthe instantaneous stochastic gradients using normalization and whitening. We\nshow that normalization stabilizes gradient distributions, and whitening\ncounteracts the local curvature of the loss landscape. This results in SWAN\n(SGD with Whitening And Normalization), a stochastic optimizer that eliminates\nthe need to store any optimizer states. Empirically, SWAN has the same memory\nfootprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory\ncompared to Adam. In language modeling tasks, SWAN demonstrates comparable or\neven better performance than Adam: when pre-training the LLaMA model with 350M\nand 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation\nperplexity using half as many tokens.']"
92,37,92_music_musical_lyrics_texttomusic,"['music', 'musical', 'lyrics', 'texttomusic', 'notation', 'midi', 'symbolic', 'abc', 'generation', 'metadata']","['Recent years have seen many audio-domain text-to-music generation models that\nrely on large amounts of text-audio pairs for training. However,\nsymbolic-domain controllable music generation has lagged behind partly due to\nthe lack of a large-scale symbolic music dataset with extensive metadata and\ncaptions. In this work, we present MetaScore, a new dataset consisting of 963K\nmusical scores paired with rich metadata, including free-form user-annotated\ntags, collected from an online music forum. To approach text-to-music\ngeneration, we leverage a pretrained large language model (LLM) to generate\npseudo natural language captions from the metadata. With the LLM-enhanced\nMetaScore, we train a text-conditioned music generation model that learns to\ngenerate symbolic music from the pseudo captions, allowing control of\ninstruments, genre, composer, complexity and other free-form music descriptors.\nIn addition, we train a tag-conditioned system that supports a predefined set\nof tags available in MetaScore. Our experimental results show that both the\nproposed text-to-music and tags-to-music models outperform a baseline\ntext-to-music model in a listening test, while the text-based system offers a\nmore natural interface that allows free-form natural language prompts.', 'While many topics of the learning-based approach to automated music\ngeneration are under active research, musical form is under-researched. In\nparticular, recent methods based on deep learning models generate music that,\nat the largest time scale, lacks any structure. In practice, music longer than\none minute generated by such models is either unpleasantly repetitive or\ndirectionless. Adapting a recent music generation model, this paper proposes a\nnovel method to generate music with form. The experimental results show that\nthe proposed method can generate 2.5-minute-long music that is considered as\npleasant as the music used to train the model. The paper first reviews a recent\nmusic generation method based on language models (transformer architecture). We\ndiscuss why learning musical form by such models is infeasible. Then we discuss\nour proposed method and the experiments.', 'Music is essential in daily life, fulfilling emotional and entertainment\nneeds, and connecting us personally, socially, and culturally. A better\nunderstanding of music can enhance our emotions, cognitive skills, and cultural\nconnections. The rapid advancement of artificial intelligence (AI) has\nintroduced new ways to analyze music, aiming to replicate human understanding\nof music and provide related services. While the traditional models focused on\naudio features and simple tasks, the recent development of large language\nmodels (LLMs) and foundation models (FMs), which excel in various fields by\nintegrating semantic information and demonstrating strong reasoning abilities,\ncould capture complex musical features and patterns, integrate music with\nlanguage and incorporate rich musical, emotional and psychological knowledge.\nTherefore, they have the potential in handling complex music understanding\ntasks from a semantic perspective, producing outputs closer to human\nperception. This work, to our best knowledge, is one of the early reviews of\nthe intersection of AI techniques and music understanding. We investigated,\nanalyzed, and tested recent large-scale music foundation models in respect of\ntheir music comprehension abilities. We also discussed their limitations and\nproposed possible future directions, offering insights for researchers in this\nfield.']"
93,37,93_phishing_emails_scam_spam,"['phishing', 'emails', 'scam', 'spam', 'attacks', 'email', 'detection', 'scams', 'sms', 'scammers']","['Phishing attacks attempt to deceive users into stealing sensitive\ninformation, posing a significant cybersecurity threat. Advances in machine\nlearning (ML) and deep learning (DL) have led to the development of numerous\nphishing webpage detection solutions, but these models remain vulnerable to\nadversarial attacks. Evaluating their robustness against adversarial phishing\nwebpages is essential. Existing tools contain datasets of pre-designed phishing\nwebpages for a limited number of brands, and lack diversity in phishing\nfeatures.\n  To address these challenges, we develop PhishOracle, a tool that generates\nadversarial phishing webpages by embedding diverse phishing features into\nlegitimate webpages. We evaluate the robustness of three existing task-specific\nmodels -- Stack model, VisualPhishNet, and Phishpedia -- against\nPhishOracle-generated adversarial phishing webpages and observe a significant\ndrop in their detection rates. In contrast, a multimodal large language model\n(MLLM)-based phishing detector demonstrates stronger robustness against these\nadversarial attacks but still is prone to evasion. Our findings highlight the\nvulnerability of phishing detection models to adversarial attacks, emphasizing\nthe need for more robust detection approaches. Furthermore, we conduct a user\nstudy to evaluate whether PhishOracle-generated adversarial phishing webpages\ncan deceive users. The results show that many of these phishing webpages evade\nnot only existing detection models but also users. We also develop the\nPhishOracle web app, allowing users to input a legitimate URL, select relevant\nphishing features and generate a corresponding phishing webpage. All resources\nwill be made publicly available on GitHub.', ""Phishing remains a pervasive cyber threat, as attackers craft deceptive\nemails to lure victims into revealing sensitive information. While Artificial\nIntelligence (AI), particularly deep learning, has become a key component in\ndefending against phishing attacks, these approaches face critical limitations.\nThe scarcity of publicly available, diverse, and updated data, largely due to\nprivacy concerns, constrains their effectiveness. As phishing tactics evolve\nrapidly, models trained on limited, outdated data struggle to detect new,\nsophisticated deception strategies, leaving systems vulnerable to an\never-growing array of attacks. Addressing this gap is essential to\nstrengthening defenses in an increasingly hostile cyber landscape. To address\nthis gap, we propose the Phishing Evolution Network (PEN), a framework\nleveraging large language models (LLMs) and adversarial training mechanisms to\ncontinuously generate high quality and realistic diverse phishing samples, and\nanalyze features of LLM-provided phishing to understand evolving phishing\npatterns. We evaluate the quality and diversity of phishing samples generated\nby PEN and find that it produces over 80% realistic phishing samples,\neffectively expanding phishing datasets across seven dominant types. These\nPEN-generated samples enhance the performance of current phishing detectors,\nleading to a 40% improvement in detection accuracy. Additionally, the use of\nPEN significantly boosts model robustness, reducing detectors' sensitivity to\nperturbations by up to 60%, thereby decreasing attack success rates under\nadversarial conditions. When we analyze the phishing patterns that are used in\nLLM-generated phishing, the cognitive complexity and the tone of time\nlimitation are detected with statistically significant differences compared\nwith existing phishing."", 'The escalating threat of phishing emails has become increasingly\nsophisticated with the rise of Large Language Models (LLMs). As attackers\nexploit LLMs to craft more convincing and evasive phishing emails, it is\ncrucial to assess the resilience of current phishing defenses. In this study we\nconduct a comprehensive evaluation of traditional phishing detectors, such as\nGmail Spam Filter, Apache SpamAssassin, and Proofpoint, as well as machine\nlearning models like SVM, Logistic Regression, and Naive Bayes, in identifying\nboth traditional and LLM-rephrased phishing emails. We also explore the\nemerging role of LLMs as phishing detection tools, a method already adopted by\ncompanies like NTT Security Holdings and JPMorgan Chase. Our results reveal\nnotable declines in detection accuracy for rephrased emails across all\ndetectors, highlighting critical weaknesses in current phishing defenses. As\nthe threat landscape evolves, our findings underscore the need for stronger\nsecurity controls and regulatory oversight on LLM-generated content to prevent\nits misuse in creating advanced phishing attacks. This study contributes to the\ndevelopment of more effective Cyber Threat Intelligence (CTI) by leveraging\nLLMs to generate diverse phishing variants that can be used for data\naugmentation, harnessing the power of LLMs to enhance phishing detection, and\npaving the way for more robust and adaptable threat detection systems.']"
94,37,94_table_tables_tabular_spreadsheet,"['table', 'tables', 'tabular', 'spreadsheet', 'tableqa', 'data', 'enterprise', 'spreadsheets', 'tabfact', 'reasoning']","['We explore generating factual and accurate tables from the parametric\nknowledge of large language models (LLMs). While LLMs have demonstrated\nimpressive capabilities in recreating knowledge bases and generating free-form\ntext, we focus on generating structured tabular data, which is crucial in\ndomains like finance and healthcare. We examine the table generation abilities\nof four state-of-the-art LLMs: GPT-3.5, GPT-4, Llama2-13B, and Llama2-70B,\nusing three prompting methods for table generation: (a) full-table, (b)\nrow-by-row; (c) cell-by-cell. For evaluation, we introduce a novel benchmark,\nWikiTabGen which contains 100 curated Wikipedia tables. Tables are further\nprocessed to ensure their factual correctness and manually annotated with short\nnatural language descriptions. Our findings reveal that table generation\nremains a challenge, with GPT-4 reaching the highest accuracy at 19.6%. Our\ndetailed analysis sheds light on how various table properties, such as size,\ntable popularity, and numerical content, influence generation performance. This\nwork highlights the unique challenges in LLM-based table generation and\nprovides a solid evaluation framework for future research. Our code, prompts\nand data are all publicly available:\nhttps://github.com/analysis-bots/WikiTabGen', 'Table reasoning is a challenging task that requires understanding both\nnatural language questions and structured tabular data. Large language models\n(LLMs) have shown impressive capabilities in natural language understanding and\ngeneration, but they often struggle with large tables due to their limited\ninput length. In this paper, we propose TabSQLify, a novel method that\nleverages text-to-SQL generation to decompose tables into smaller and relevant\nsub-tables, containing only essential information for answering questions or\nverifying statements, before performing the reasoning task. In our\ncomprehensive evaluation on four challenging datasets, our approach\ndemonstrates comparable or superior performance compared to prevailing methods\nreliant on full tables as input. Moreover, our method can reduce the input\ncontext length significantly, making it more scalable and efficient for\nlarge-scale table reasoning applications. Our method performs remarkably well\non the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the\nTabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass\nother LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can\nreduce the table size significantly alleviating the computational load on LLMs\nwhen handling large tables without compromising performance.', 'The ubiquity and value of tables as semi-structured data across various\ndomains necessitate advanced methods for understanding their complexity and\nvast amounts of information. Despite the impressive capabilities of large\nlanguage models (LLMs) in advancing the natural language understanding\nfrontier, their application to large-scale tabular data presents significant\nchallenges, specifically regarding table size and complex intricate\nrelationships. Existing works have shown promise with small-scale tables but\noften flounder when tasked with the complex reasoning required by larger,\ninterconnected tables found in real-world scenarios. To address this gap, we\nintroduce ""Tree-of-Table"", a novel approach designed to enhance LLMs\' reasoning\ncapabilities over large and complex tables. Our method employs Table\nCondensation and Decomposition to distill and reorganize relevant data into a\nmanageable format, followed by the construction of a hierarchical Table-Tree\nthat facilitates tree-structured reasoning. Through a meticulous Table-Tree\nExecution process, we systematically unravel the tree-structured reasoning\nchain to derive the solutions. Experiments across diverse datasets, including\nWikiTQ, TableFact, FeTaQA, and BIRD, demonstrate that Tree-of-Table sets a new\nbenchmark with superior performance, showcasing remarkable efficiency and\ngeneralization capabilities in large-scale table reasoning.']"
95,37,95_topic_esg_ncs_topics,"['topic', 'esg', 'ncs', 'topics', 'climate', 'modeling', 'carbon', 'environmental', 'coimpacts', 'sustainability']","['Topic modeling has become a crucial method for analyzing text data,\nparticularly for extracting meaningful insights from large collections of\ndocuments. However, the output of these models typically consists of lists of\nkeywords that require manual interpretation for precise labeling. This study\nexplores the use of Large Language Models (LLMs) to automate and enhance topic\nlabeling by generating more meaningful and contextually appropriate labels.\nAfter applying BERTopic for topic modeling, we explore different approaches to\nselect keywords and document summaries within each topic, which are then fed\ninto an LLM to generate labels. Each approach prioritizes different aspects,\nsuch as dominant themes or diversity, to assess their impact on label quality.\nAdditionally, recognizing the lack of quantitative methods for evaluating topic\nlabels, we propose a novel metric that measures how semantically representative\na label is of all documents within a topic.', 'Topic models are used to identify and group similar themes in a set of\ndocuments. Recent advancements in deep learning based neural topic models has\nreceived significant research interest. In this paper, an approach is proposed\nthat further enhances topic modeling performance by utilizing a pre-trained\nlarge language model (LLM) to generate summaries of documents before inputting\nthem into the topic model. Few shot prompting is used to generate summaries of\ndifferent lengths to compare their impact on topic modeling. This approach is\nparticularly effective for larger documents because it helps capture the most\nessential information while reducing noise and irrelevant details that could\nobscure the overall theme. Additionally, it is observed that datasets exhibit\nan optimal summary length that leads to improved topic modeling performance.\nThe proposed method yields better topic diversity and comparable coherence\nvalues compared to previous models.', 'Topic modeling is a widely used technique for uncovering thematic structures\nfrom large text corpora. However, most topic modeling approaches e.g. Latent\nDirichlet Allocation (LDA) struggle to capture nuanced semantics and contextual\nunderstanding required to accurately model complex narratives. Recent\nadvancements in this area include methods like BERTopic, which have\ndemonstrated significantly improved topic coherence and thus established a new\nstandard for benchmarking. In this paper, we present a novel approach, the\nQualitative Insights Tool (QualIT) that integrates large language models (LLMs)\nwith existing clustering-based topic modeling approaches. Our method leverages\nthe deep contextual understanding and powerful language generation capabilities\nof LLMs to enrich the topic modeling process using clustering. We evaluate our\napproach on a large corpus of news articles and demonstrate substantial\nimprovements in topic coherence and topic diversity compared to baseline topic\nmodeling techniques. On the 20 ground-truth topics, our method shows 70% topic\ncoherence (vs 65% & 57% benchmarks) and 95.5% topic diversity (vs 85% & 72%\nbenchmarks). Our findings suggest that the integration of LLMs can unlock new\nopportunities for topic modeling of dynamic and complex text data, as is common\nin talent management research contexts.']"
96,36,96_dna_gene_genomic_singlecell,"['dna', 'gene', 'genomic', 'singlecell', 'biological', 'rna', 'sequences', 'cell', 'expression', 'multiomics']","['The application of deep learning methods, particularly foundation models, in\nbiological research has surged in recent years. These models can be text-based\nor trained on underlying biological data, especially omics data of various\ntypes. However, comparing the performance of these models consistently has\nproven to be a challenge due to differences in training data and downstream\ntasks. To tackle this problem, we developed an architecture-agnostic\nbenchmarking approach that, instead of evaluating the models directly,\nleverages entity representation vectors from each model and trains simple\npredictive models for each benchmarking task. This ensures that all types of\nmodels are evaluated using the same input and output types. Here we focus on\ngene properties collected from professionally curated bioinformatics databases.\nThese gene properties are categorized into five major groups: genomic\nproperties, regulatory functions, localization, biological processes, and\nprotein properties. Overall, we define hundreds of tasks based on these\ndatabases, which include binary, multi-label, and multi-class classification\ntasks. We apply these benchmark tasks to evaluate expression-based models,\nlarge language models, protein language models, DNA-based models, and\ntraditional baselines. Our findings suggest that text-based models and protein\nlanguage models generally outperform expression-based models in genomic\nproperties and regulatory functions tasks, whereas expression-based models\ndemonstrate superior performance in localization tasks. These results should\naid in the development of more informed artificial intelligence strategies for\nbiological understanding and therapeutic discovery. To ensure the\nreproducibility and transparency of our findings, we have made the source code\nand benchmark data publicly accessible for further investigation and expansion\nat github.com/BiomedSciAI/gene-benchmark.', 'Advancements in DNA sequencing technologies have significantly improved our\nability to decode genomic sequences. However, the prediction and interpretation\nof these sequences remain challenging due to the intricate nature of genetic\nmaterial. Large language models (LLMs) have introduced new opportunities for\nbiological sequence analysis. Recent developments in genomic language models\nhave underscored the potential of LLMs in deciphering DNA sequences.\nNonetheless, existing models often face limitations in robustness and\napplication scope, primarily due to constraints in model structure and training\ndata scale. To address these limitations, we present GENERator, a generative\ngenomic foundation model featuring a context length of 98k base pairs (bp) and\n1.2B parameters. Trained on an expansive dataset comprising 386B bp of\neukaryotic DNA, the GENERator demonstrates state-of-the-art performance across\nboth established and newly proposed benchmarks. The model adheres to the\ncentral dogma of molecular biology, accurately generating protein-coding\nsequences that translate into proteins structurally analogous to known\nfamilies. It also shows significant promise in sequence optimization,\nparticularly through the prompt-responsive generation of enhancer sequences\nwith specific activity profiles. These capabilities position the GENERator as a\npivotal tool for genomic research and biotechnological advancement, enhancing\nour ability to interpret and predict complex biological systems and enabling\nprecise genomic interventions. Implementation details and supplementary\nresources are available at https://github.com/GenerTeam/GENERator.', 'Biological sequences encode fundamental instructions for the building blocks\nof life, in the form of DNA, RNA, and proteins. Modeling these sequences is key\nto understand disease mechanisms and is an active research area in\ncomputational biology. Recently, Large Language Models have shown great promise\nin solving certain biological tasks but current approaches are limited to a\nsingle sequence modality (DNA, RNA, or protein). Key problems in genomics\nintrinsically involve multiple modalities, but it remains unclear how to adapt\ngeneral-purpose sequence models to those cases. In this work we propose a\nmulti-modal model that connects DNA, RNA, and proteins by leveraging\ninformation from different pre-trained modality-specific encoders. We\ndemonstrate its capabilities by applying it to the largely unsolved problem of\npredicting how multiple RNA transcript isoforms originate from the same gene\n(i.e. same DNA sequence) and map to different transcription expression levels\nacross various human tissues. We show that our model, dubbed IsoFormer, is able\nto accurately predict differential transcript expression, outperforming\nexisting methods and leveraging the use of multiple modalities. Our framework\nalso achieves efficient transfer knowledge from the encoders pre-training as\nwell as in between modalities. We open-source our model, paving the way for new\nmulti-modal gene expression approaches.']"
97,35,97_roleplaying_character_characters_rplas,"['roleplaying', 'character', 'characters', 'rplas', 'rpas', 'dialogue', 'persona', 'agent', 'agents', 'coser']","['Recent advancements in large language models (LLMs) have significantly\nboosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI\nsystems designed to simulate assigned personas. By harnessing multiple advanced\nabilities of LLMs, including in-context learning, instruction following, and\nsocial intelligence, RPLAs achieve a remarkable sense of human likeness and\nvivid role-playing performance. RPLAs can mimic a wide range of personas,\nranging from historical figures and fictional characters to real-life\nindividuals. Consequently, they have catalyzed numerous AI applications, such\nas emotional companions, interactive video games, personalized assistants and\ncopilots, and digital clones. In this paper, we conduct a comprehensive survey\nof this field, illustrating the evolution and recent progress in RPLAs\nintegrating with cutting-edge LLM technologies. We categorize personas into\nthree types: 1) Demographic Persona, which leverages statistical stereotypes;\n2) Character Persona, focused on well-established figures; and 3)\nIndividualized Persona, customized through ongoing user interactions for\npersonalized services. We begin by presenting a comprehensive overview of\ncurrent methodologies for RPLAs, followed by the details for each persona type,\ncovering corresponding data sourcing, agent construction, and evaluation.\nAfterward, we discuss the fundamental risks, existing limitations, and future\nprospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI\napplications, which reflects practical user demands that shape and drive RPLA\nresearch. Through this work, we aim to establish a clear taxonomy of RPLA\nresearch and applications, and facilitate future research in this critical and\never-evolving field, and pave the way for a future where humans and RPLAs\ncoexist in harmony.', ""Character-based dialogue (aka role-playing) enables users to freely customize\ncharacters for interaction, which often relies on LLMs, raising the need to\nevaluate LLMs' character customization capability. However, existing benchmarks\nfail to ensure a robust evaluation as they often only involve a single\ncharacter category or evaluate limited dimensions. Moreover, the sparsity of\ncharacter features in responses makes feature-focused generative evaluation\nboth ineffective and inefficient. To address these issues, we propose\nCharacterBench, the largest bilingual generative benchmark, with 22,859\nhuman-annotated samples covering 3,956 characters from 25 detailed character\ncategories. We define 11 dimensions of 6 aspects, classified as sparse and\ndense dimensions based on whether character features evaluated by specific\ndimensions manifest in each response. We enable effective and efficient\nevaluation by crafting tailored queries for each dimension to induce\ncharacters' responses related to specific dimensions. Further, we develop\nCharacterJudge model for cost-effective and stable evaluations. Experiments\nshow its superiority over SOTA automatic judges (e.g., GPT-4) and our\nbenchmark's potential to optimize LLMs' character customization. Our repository\nis at https://github.com/thu-coai/CharacterBench."", ""Large language models (LLMs) have demonstrated impressive performance and\nspurred numerous AI applications, in which role-playing agents (RPAs) are\nparticularly popular, especially for fictional characters. The prerequisite for\nthese RPAs lies in the capability of LLMs to understand characters from\nfictional works. Previous efforts have evaluated this capability via basic\nclassification tasks or characteristic imitation, failing to capture the\nnuanced character understanding with LLMs. In this paper, we propose evaluating\nLLMs' character understanding capability via the character profiling task,\ni.e., summarizing character profiles from corresponding materials, a widely\nadopted yet understudied practice for RPA development. Specifically, we\nconstruct the CroSS dataset from literature experts and assess the generated\nprofiles by comparing them with ground truth references and evaluating their\napplicability in downstream tasks. Our experiments, which cover various\nsummarization methods and LLMs, have yielded promising results. These results\nstrongly validate the character understanding capability of LLMs. Resources are\navailable at https://github.com/Joanna0123/character_profiling.""]"
98,35,98_persuasive_persuasion_social_influence,"['persuasive', 'persuasion', 'social', 'influence', 'arguments', 'what', 'misinformation', 'ai', 'economic', 'pbt']","[""LLMs are increasingly being used in workflows involving generating content to\nbe consumed by humans (e.g., marketing) and also in directly interacting with\nhumans (e.g., through chatbots). The development of such systems that are\ncapable of generating verifiably persuasive messages presents both\nopportunities and challenges for society. On the one hand, such systems could\npositively impact domains like advertising and social good, such as addressing\ndrug addiction, and on the other, they could be misused for spreading\nmisinformation and shaping political opinions. To channel LLMs' impact on\nsociety, we need to develop systems to measure and benchmark their\npersuasiveness. With this motivation, we introduce PersuasionBench and\nPersuasionArena, the first large-scale benchmark and arena containing a battery\nof tasks to measure the persuasion ability of generative models automatically.\nWe investigate to what extent LLMs know and leverage linguistic patterns that\ncan help them generate more persuasive language. Our findings indicate that the\npersuasiveness of LLMs correlates positively with model size, but smaller\nmodels can also be made to have a higher persuasiveness than much larger\nmodels. Notably, targeted training using synthetic and natural datasets\nsignificantly enhances smaller models' persuasive capabilities, challenging\nscale-dependent assumptions. Our findings carry key implications for both model\ndevelopers and policymakers. For instance, while the EU AI Act and California's\nSB-1047 aim to regulate AI models based on the number of floating point\noperations, we demonstrate that simple metrics like this alone fail to capture\nthe full scope of AI's societal impact. We invite the community to explore and\ncontribute to PersuasionArena and PersuasionBench, available at\nhttps://bit.ly/measure-persuasion, to advance our understanding of AI-driven\npersuasion and its societal implications."", ""We are exposed to much information trying to influence us, such as teaser\nmessages, debates, politically framed news, and propaganda - all of which use\npersuasive language. With the recent interest in Large Language Models (LLMs),\nwe study the ability of LLMs to produce persuasive text. As opposed to prior\nwork which focuses on particular domains or types of persuasion, we conduct a\ngeneral study across various domains to measure and benchmark to what degree\nLLMs produce persuasive language - both when explicitly instructed to rewrite\ntext to be more or less persuasive and when only instructed to paraphrase. We\nconstruct the new dataset Persuasive-Pairs of pairs of a short text and its\nrewrite by an LLM to amplify or diminish persuasive language. We multi-annotate\nthe pairs on a relative scale for persuasive language: a valuable resource in\nitself, and for training a regression model to score and benchmark persuasive\nlanguage, including for new LLMs across domains. In our analysis, we find that\ndifferent 'personas' in LLaMA3's system prompt change persuasive language\nsubstantially, even when only instructed to paraphrase."", ""Large Language Models (LLMs) demonstrate persuasive capabilities that rival\nhuman-level persuasion. While these capabilities can be used for social good,\nthey also present risks of potential misuse. Moreover, LLMs' susceptibility to\npersuasion raises concerns about alignment with ethical principles. To study\nthese dynamics, we introduce Persuade Me If You Can (PMIYC), an automated\nframework for evaluating persuasion through multi-agent interactions. Here,\nPersuader agents engage in multi-turn conversations with the Persuadee agents,\nallowing us to measure LLMs' persuasive effectiveness and their susceptibility\nto persuasion. We conduct comprehensive evaluations across diverse LLMs,\nensuring each model is assessed against others in both subjective and\nmisinformation contexts. We validate the efficacy of our framework through\nhuman evaluations and show alignment with prior work. PMIYC offers a scalable\nalternative to human annotation for studying persuasion in LLMs. Through PMIYC,\nwe find that Llama-3.3-70B and GPT-4o exhibit similar persuasive effectiveness,\noutperforming Claude 3 Haiku by 30%. However, GPT-4o demonstrates over 50%\ngreater resistance to persuasion for misinformation compared to Llama-3.3-70B.\nThese findings provide empirical insights into the persuasive dynamics of LLMs\nand contribute to the development of safer AI systems.""]"
99,33,99_contextual_coherence_structured_token,"['contextual', 'coherence', 'structured', 'token', 'embeddings', 'dependencies', 'computational', 'hierarchical', 'lexical', 'linguistic']","['Contextual adaptation in token embeddings plays a central role in determining\nhow well language models maintain coherence and retain semantic relationships\nover extended text sequences. Static embeddings often impose constraints on\nlexical flexibility, leading to suboptimal performance when faced with complex\nsentence structures or domain-specific terminology shifts. To address this\nlimitation, a structured approach was developed for dynamically reconfiguring\ntoken embeddings through continuous geometric transformations, ensuring that\nrepresentations evolved in response to evolving discourse structures. A\nmanifold-based transformation mechanism was integrated to regulate lexical\npositioning, allowing embeddings to undergo controlled shifts while preserving\nlinguistic relationships across varying textual contexts. Empirical evaluations\ndemonstrated that embedding reconfiguration contributed to reductions in\nperplexity, improved lexical coherence, and enhanced sentence-level continuity,\nparticularly in structured and domain-adaptive text generation tasks.\nComparative analyses of embedding drift indicated that dynamically restructured\nrepresentations maintained stronger contextual consistency, reducing\nmisalignment in token dependencies while preserving fluency in language\nmodeling outputs. Computational overhead assessments confirmed that while\ntraining complexity increased due to the iterative refinement of embeddings,\ninference remained efficient, ensuring practical feasibility for real-time\ngeneration. Evaluations across multiple datasets further demonstrated that\ndynamically modulated embeddings exhibited broader lexical diversity, reducing\nrepetitive token patterns and enabling a more adaptable representation learning\nprocess.', 'Representation learning plays a central role in structuring internal\nembeddings to capture the statistical properties of language, influencing the\ncoherence and contextual consistency of generated text. Statistical Coherence\nAlignment is introduced as a method to enforce structured token representations\nthrough tensor field convergence, guiding embeddings to reflect statistical\ndependencies inherent in linguistic data. A mathematical framework is\nestablished to quantify coherence alignment, integrating a loss function that\noptimizes representational consistency across training iterations. Empirical\nevaluations demonstrate that applying coherence constraints improves\nperplexity, enhances classification accuracy, and refines rare word embeddings,\ncontributing to a more stable representation space. Comparative analyses with\nbaseline models reveal that the proposed method fosters a more interpretable\ninternal structure, ensuring that embeddings retain contextual dependencies\nwhile mitigating representation collapse. The impact on coherence score\ndistributions suggests that the alignment mechanism strengthens semantic\nintegrity across diverse linguistic constructs, leading to a more balanced\norganization of learned embeddings. Computational assessments indicate that\nwhile the method introduces additional memory and training costs, the\nstructured optimization process justifies the trade-offs in applications\nrequiring heightened contextual fidelity. Experimental results validate the\neffectiveness of coherence alignment in optimizing token representations,\nproviding insights into how statistical dependencies can be leveraged to\nimprove language model training.', 'The integration of structured hierarchical embeddings into transformer-based\narchitectures introduces a refined approach to lexical representation, ensuring\nthat multi-scale semantic relationships are preserved without compromising\ncomputational efficiency. A projection mechanism that maps tokens onto a\nstructured manifold provides improved lexical alignment, enhancing the\nadaptability of word representations across diverse linguistic tasks. The\nstructured encoding framework ensures that hierarchical embeddings maintain\ncoherence across varying abstraction levels, allowing for stable transitions\nbetween localized syntactic features and global semantic structures.\nExperimental evaluations indicate that hierarchical embeddings consistently\noutperform conventional token representations, improving accuracy in linguistic\nbenchmarks while maintaining lower computational overhead. Comparative analysis\nacross multiple domains highlights the ability of hierarchical embeddings to\nretain contextual consistency, particularly in specialized language\napplications where structured lexical alignment is essential. Statistical\nassessments further demonstrate that hierarchical embeddings exhibit enhanced\nrobustness under perturbation conditions, ensuring that linguistic structures\nremain stable across adversarial text modifications. The integration of\nhierarchical projections with transformer attention mechanisms enables improved\ncontextual adaptation, ensuring that token representations are dynamically\nadjusted based on varying linguistic distributions. The refined hierarchical\norganization of embeddings provides greater interpretability in lexical\nmodeling, facilitating enhanced generalization capabilities across diverse text\nprocessing tasks.']"
100,33,100_edge_devices_mobile_cloud,"['edge', 'devices', 'mobile', 'cloud', 'computing', 'servers', 'resource', 'latency', 'distributed', 'inference']","['Large Language Models (LLMs) have achieved remarkable success in serving\nend-users with human-like intelligence. However, LLMs demand high computational\nresources, making it challenging to deploy them to satisfy various performance\nobjectives, such as meeting the resource constraints on edge devices close to\nend-users or achieving high accuracy with ample resources. In this paper, we\nintroduce CE-CoLLM, a novel cloud-edge collaboration framework that supports\nefficient and adaptive LLM inference for end-users at the edge with two modes,\n(1) low-latency edge standalone inference and (2) highly accurate cloud-edge\ncollaborative inference. First, we show that the inherent high communication\ncosts for transmitting LLM contextual information between the edge and cloud\ndominate the overall latency, making it inefficient and costly to deploy LLMs\nusing cloud-edge collaboration. Second, we propose several critical techniques\nto address this challenge, including early-exit mechanism, cloud context\nmanager, and quantization in cloud-edge collaboration to enable not only\nlow-latency standalone edge inference but also efficient and adaptive\ncloud-edge collaborative inference for LLMs. Third, we perform comprehensive\nexperimental analysis, which demonstrates that CE-CoLLM significantly reduces\ninference time by up to 13.81% and cloud computation costs by up to 84.55%\ncompared to the popular cloud-based LLM deployment, while maintaining\ncomparable model accuracy. The proposed approach effectively shifts the\ncomputational load to the edge, reduces the communication overhead, scales\nefficiently with multiple edge clients, and provides reliable LLM deployment\nusing cloud-edge collaboration.', 'The combination of Federated Learning (FL), Multimodal Large Language Models\n(MLLMs), and edge-cloud computing enables distributed and real-time data\nprocessing while preserving privacy across edge devices and cloud\ninfrastructure. However, the deployment of MLLMs in FL environments with\nresource-constrained edge devices presents significant challenges, including\nresource management, communication overhead, and non-IID data. To address these\nchallenges, we propose a novel hybrid framework wherein MLLMs are deployed on\nedge devices equipped with sufficient resources and battery life, while the\nmajority of training occurs in the cloud. To identify suitable edge devices for\ndeployment, we employ Particle Swarm Optimization (PSO), and Ant Colony\nOptimization (ACO) is utilized to optimize the transmission of model updates\nbetween edge and cloud nodes. This proposed swarm intelligence-based framework\naims to enhance the efficiency of MLLM training by conducting extensive\ntraining in the cloud and fine-tuning at the edge, thereby reducing energy\nconsumption and communication costs. Our experimental results show that the\nproposed method significantly improves system performance, achieving an\naccuracy of 92%, reducing communication cost by 30%, and enhancing client\nparticipation compared to traditional FL methods. These results make the\nproposed approach highly suitable for large-scale edge-cloud computing systems.', 'Large language models (LLMs) have shown great potential in natural language\nprocessing and content generation. However, current LLMs heavily rely on cloud\ncomputing, leading to prolonged latency, high bandwidth cost, and privacy\nconcerns. Edge computing is promising to address such concerns by deploying\nLLMs on edge devices, closer to data sources. Some works try to leverage model\nquantization to reduce the model size to fit the resource-constraint edge\ndevices, but they lead to accuracy loss. Other works use cloud-edge\ncollaboration, suffering from unstable network connections. In this work, we\nleverage collaborative edge computing to facilitate the collaboration among\nedge devices and cloud servers for jointly performing efficient LLM inference.\nWe propose a general framework to partition the LLM model into shards and\ndeploy on distributed devices. To achieve efficient LLM inference, we formulate\nan adaptive joint device selection and model partition problem and design an\nefficient dynamic programming algorithm to optimize the inference latency and\nthroughput, respectively. Experiments of Llama2 serial models on a\nheterogeneous physical prototype demonstrate that EdgeShard achieves up to 50%\nlatency reduction and 2x throughput improvement over baseline methods.']"
101,33,101_scaling_size_laws_law,"['scaling', 'size', 'laws', 'law', 'optimal', 'training', 'vocabulary', 'loss', 'hyperparameters', 'sizes']","['Scaling laws for large language models (LLMs) predict model performance based\non parameters like size and training data. However, differences in training\nconfigurations and data processing across model families lead to significant\nvariations in benchmark performance, making it difficult for a single scaling\nlaw to generalize across all LLMs. On the other hand, training family-specific\nscaling laws requires training models of varying sizes for every family. In\nthis work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a\nnovel scaling law that leverages publicly available benchmark data and assumes\nLLM performance is driven by low-dimensional latent skills, such as reasoning\nand instruction following. These latent skills are influenced by computational\nresources like model size and training tokens but with varying efficiencies\nacross model families. Sloth exploits correlations across benchmarks to provide\nmore accurate and interpretable predictions while alleviating the need to train\nmultiple LLMs per family. We present both theoretical results on parameter\nidentification and empirical evaluations on 12 prominent benchmarks, from Open\nLLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance\nefficiently and offers insights into scaling behaviors for complex downstream\ntasks and increased test-time compute.', 'Neural scaling laws have driven significant advancements in machine learning,\nparticularly in domains like language modeling and computer vision. However,\nthe exploration of neural scaling laws within robotics has remained relatively\nunderexplored, despite the growing adoption of foundation models in this field.\nThis paper represents the first comprehensive study to quantify neural scaling\nlaws for Robot Foundation Models (RFMs) and Large Language Models (LLMs) in\nrobotics tasks. Through a meta-analysis of 327 research papers, we investigate\nhow data size, model size, and compute resources influence downstream\nperformance across a diverse set of robotic tasks. Consistent with previous\nscaling law research, our results reveal that the performance of robotic models\nimproves with increased resources, following a power-law relationship.\nPromisingly, the improvement in robotic task performance scales notably faster\nthan language tasks. This suggests that, while performance on downstream\nrobotic tasks today is often moderate-to-poor, increased data and compute are\nlikely to signficantly improve performance in the future. Also consistent with\nprevious scaling law research, we also observe the emergence of new robot\ncapabilities as models scale.', 'Scaling laws guide the development of large language models (LLMs) by\noffering estimates for the optimal balance of model size, tokens, and compute.\nMore recently, loss-to-loss scaling laws that relate losses across pretraining\ndatasets and downstream tasks have emerged as a powerful tool for understanding\nand improving LLM performance. In this work, we investigate which factors most\nstrongly influence loss-to-loss scaling. Our experiments reveal that the\npretraining data and tokenizer determine the scaling trend. In contrast, model\nsize, optimization hyperparameters, and even significant architectural\ndifferences, such as between transformer-based models like Llama and\nstate-space models like Mamba, have limited impact. Consequently, practitioners\nshould carefully curate suitable pretraining datasets for optimal downstream\nperformance, while architectures and other settings can be freely optimized for\ntraining efficiency.']"
102,31,102_repair_apr_program_bug,"['repair', 'apr', 'program', 'bug', 'bugs', 'localization', 'patch', 'fixing', 'fault', 'defects4j']","['Recently, multiple Automated Program Repair (APR) techniques based on Large\nLanguage Models (LLMs) have been proposed to enhance the repair performance.\nWhile these techniques mainly focus on the single-line or hunk-level repair,\nthey face significant challenges in real-world application due to the limited\nrepair task scope and costly statement-level fault localization. However, the\nmore practical function-level APR, which broadens the scope of APR task to fix\nentire buggy functions and requires only cost-efficient function-level fault\nlocalization, remains underexplored. In this paper, we conduct the first\ncomprehensive study of LLM-based function-level APR including investigating the\neffect of the few-shot learning mechanism and the auxiliary repair-relevant\ninformation. Specifically, we adopt six widely-studied LLMs and construct a\nbenchmark in both the Defects4J 1.2 and 2.0 datasets. Our study demonstrates\nthat LLMs with zero-shot learning are already powerful function-level APR\ntechniques, while applying the few-shot learning mechanism leads to disparate\nrepair performance. Moreover, we find that directly applying the auxiliary\nrepair-relevant information to LLMs significantly increases function-level\nrepair performance. Inspired by our findings, we propose an LLM-based\nfunction-level APR technique, namely SRepair, which adopts a dual-LLM framework\nto leverage the power of the auxiliary repair-relevant information for\nadvancing the repair performance. The evaluation results demonstrate that\nSRepair can correctly fix 300 single-function bugs in the Defects4J dataset,\nlargely surpassing all previous APR techniques by at least 85%, without the\nneed for the costly statement-level fault location information. Furthermore,\nSRepair successfully fixes 32 multi-function bugs in the Defects4J dataset,\nwhich is the first time achieved by any APR technique ever to our best\nknowledge.', 'Automated program repair (APR) is designed to automate the process of\nbug-fixing. In recent years, thanks to the rapid development of large language\nmodels (LLMs), automated repair has achieved remarkable progress. Advanced APR\ntechniques powered by conversational LLMs, most notably ChatGPT, have exhibited\nimpressive repair abilities and gained increasing popularity due to the\ncapabilities of the underlying LLMs in providing repair feedback and performing\niterative patch improvement. Despite the superiority, conversational APR\ntechniques still fail to repair a large number of bugs. For example, a\nstate-of-the-art conversational technique ChatRepair does not correctly repair\nover half of the single-function bugs in the Defects4J dataset. To understand\nthe effectiveness and failures of conversational LLM-based repair and provide\npossible directions for improvement, we studied the exemplary ChatRepair with a\nfocus on comparing the effectiveness of its cloze-style and full function\nrepair strategies, assessing its key iterative component for patch improvement,\nand analyzing the repair failures. Our study has led to a series of findings,\nwhich we believe provide key implications for future research.', ""Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR.""]"
103,31,103_event_extraction_events_relations,"['event', 'extraction', 'events', 'relations', 'eae', 'documentlevel', 'mentions', 'argument', 'episode', 'oded']","['In the field of Natural Language Processing (NLP), Large Language Models\n(LLMs) have shown great potential in document-level event extraction tasks, but\nexisting methods face challenges in the design of prompts. To address this\nissue, we propose an optimization strategy called ""Definition-driven\nDocument-level Event Extraction (DDEE)."" By adjusting the length of the prompt\nand enhancing the clarity of heuristics, we have significantly improved the\nevent extraction performance of LLMs. We used data balancing techniques to\nsolve the long-tail effect problem, enhancing the model\'s generalization\nability for event types. At the same time, we refined the prompt to ensure it\nis both concise and comprehensive, adapting to the sensitivity of LLMs to the\nstyle of prompts. In addition, the introduction of structured heuristic methods\nand strict limiting conditions has improved the precision of event and argument\nrole extraction. These strategies not only solve the prompt engineering\nproblems of LLMs in document-level event extraction but also promote the\ndevelopment of event extraction technology, providing new research perspectives\nfor other tasks in the NLP field.', 'Extracting event relations that deviate from known schemas has proven\nchallenging for previous methods based on multi-class classification, MASK\nprediction, or prototype matching. Recent advancements in large language models\nhave shown impressive performance through instruction tuning. Nevertheless, in\nthe task of event relation extraction, instruction-based methods face several\nchallenges: there are a vast number of inference samples, and the relations\nbetween events are non-sequential. To tackle these challenges, we present an\nimproved instruction-based event relation extraction framework named\nMAQInstruct. Firstly, we transform the task from extracting event relations\nusing given event-event instructions to selecting events using given\nevent-relation instructions, which reduces the number of samples required for\ninference. Then, by incorporating a bipartite matching loss, we reduce the\ndependency of the instruction-based method on the generation sequence. Our\nexperimental results demonstrate that MAQInstruct significantly improves the\nperformance of event relation extraction across multiple LLMs.', 'Events refer to specific occurrences, incidents, or happenings that take\nplace under a particular background. Event reasoning aims to infer events\naccording to certain relations and predict future events. The cutting-edge\ntechniques for event reasoning play a crucial role in various natural language\nprocessing applications. Large language models (LLMs) have made significant\nadvancements in event reasoning owing to their wealth of knowledge and\nreasoning capabilities. However, smaller instruction-tuned models currently in\nuse do not consistently demonstrate exceptional proficiency in managing these\ntasks. This discrepancy arises from the absence of explicit modeling of events\nand the interconnections of them within their instruction data. Consequently,\nthese models face challenges in comprehending event structures and semantics\nwhile struggling to bridge the gap between their interpretations and human\nunderstanding of events. Additionally, their limitations in grasping event\nrelations lead to constrained event reasoning abilities to effectively deduce\nand incorporate pertinent event knowledge. In this paper, we propose\nEvent-Oriented Instruction Tuning (EvIT) to train our LLM. Specifically, we\nfirst propose a novel structure named event quadruple which contains the\nstructure and semantics of events and is complete in the event representation.\nWe then design event-relation learning based on the structures. We encapsulate\nthe learning into the instruction-tuning formulation to better stimulate the\nevent reasoning capacity of our model. We design a heuristic unsupervised\nmethod to mine event quadruple from a large-scale corpus. At last, we finetune\na Llama model on our Event-Oriented Instruction Tuning. We conduct extensive\nexperiments on event reasoning tasks on several datasets. Automatic and human\nevaluations demonstrate EvIT achieves competitive performances on event\nreasoning.']"
104,31,104_test_testing_rest_api,"['test', 'testing', 'rest', 'api', 'code', 'software', 'dev', 'cases', 'apis', 'autoresttest']","['In the contemporary landscape of technological advancements, the automation\nof manual processes is crucial, compelling the demand for huge datasets to\neffectively train and test machines. This research paper is dedicated to the\nexploration and implementation of an automated approach to generate test cases\nspecifically using Large Language Models. The methodology integrates the use of\nOpen AI to enhance the efficiency and effectiveness of test case generation for\ntraining and evaluating Large Language Models. This formalized approach with\nLLMs simplifies the testing process, making it more efficient and\ncomprehensive. Leveraging natural language understanding, LLMs can\nintelligently formulate test cases that cover a broad range of REST API\nproperties, ensuring comprehensive testing. The model that is developed during\nthe research is trained using manually collected postman test cases or\ninstances for various Rest APIs. LLMs enhance the creation of Postman test\ncases by automating the generation of varied and intricate test scenarios.\nPostman test cases offer streamlined automation, collaboration, and dynamic\ndata handling, providing a user-friendly and efficient approach to API testing\ncompared to traditional test cases. Thus, the model developed not only conforms\nto current technological standards but also holds the promise of evolving into\nan idea of substantial importance in future technological advancements.', 'API testing has increasing demands for software companies. Prior API testing\ntools were aware of certain types of dependencies that needed to be concise\nbetween operations and parameters. However, their approaches, which are mostly\ndone manually or using heuristic-based algorithms, have limitations due to the\ncomplexity of these dependencies. In this paper, we present KAT (Katalon API\nTesting), a novel AI-driven approach that leverages the large language model\nGPT in conjunction with advanced prompting techniques to autonomously generate\ntest cases to validate RESTful APIs. Our comprehensive strategy encompasses\nvarious processes to construct an operation dependency graph from an OpenAPI\nspecification and to generate test scripts, constraint validation scripts, test\ncases, and test data. Our evaluation of KAT using 12 real-world RESTful\nservices shows that it can improve test coverage, detect more undocumented\nstatus codes, and reduce false positives in these services in comparison with a\nstate-of-the-art automated test generation tool. These results indicate the\neffectiveness of using the large language model for generating test scripts and\ndata for API testing.', ""As modern web services increasingly rely on REST APIs, their thorough testing\nhas become crucial. Furthermore, the advent of REST API documentation\nlanguages, such as the OpenAPI Specification, has led to the emergence of many\nblack-box REST API testing tools. However, these tools often focus on\nindividual test elements in isolation (e.g., APIs, parameters, values),\nresulting in lower coverage and less effectiveness in fault detection. To\naddress these limitations, we present AutoRestTest, the first black-box tool to\nadopt a dependency-embedded multi-agent approach for REST API testing that\nintegrates multi-agent reinforcement learning (MARL) with a semantic property\ndependency graph (SPDG) and Large Language Models (LLMs). Our approach treats\nREST API testing as a separable problem, where four agents -- API, dependency,\nparameter, and value agents -- collaborate to optimize API exploration. LLMs\nhandle domain-specific value generation, the SPDG model simplifies the search\nspace for dependencies using a similarity score between API operations, and\nMARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest\non 12 real-world REST services shows that it outperforms the four leading\nblack-box REST API testing tools, including those assisted by RESTGPT (which\ngenerates realistic test inputs using LLMs), in terms of code coverage,\noperation coverage, and fault detection. Notably, AutoRestTest is the only tool\nable to trigger an internal server error in the Spotify service. Our ablation\nstudy illustrates that each component of AutoRestTest -- the SPDG, the LLM, and\nthe agent-learning mechanism -- contributes to its overall effectiveness.""]"
105,30,105_fake_news_detection_misinformation,"['fake', 'news', 'detection', 'misinformation', 'credibility', 'spread', 'factagent', 'media', 'social', 'comments']","['The advent of large language models (LLMs) has revolutionized online content\ncreation, making it much easier to generate high-quality fake news. This misuse\nthreatens the integrity of our digital environment and ethical standards.\nTherefore, understanding the motivations and mechanisms behind LLM-generated\nfake news is crucial. In this study, we analyze the creation of fake news from\na social psychology perspective and develop a comprehensive LLM-based\ntheoretical framework, LLM-Fake Theory. We introduce a novel pipeline that\nautomates the generation of fake news using LLMs, thereby eliminating the need\nfor manual annotation. Utilizing this pipeline, we create a theoretically\ninformed Machine-generated Fake news dataset, MegaFake, derived from the\nGossipCop dataset. We conduct comprehensive analyses to evaluate our MegaFake\ndataset. We believe that our dataset and insights will provide valuable\ncontributions to future research focused on the detection and governance of\nfake news in the era of LLMs.', 'Fake news poses a significant threat to the integrity of information\necosystems and public trust. The advent of Large Language Models (LLMs) holds\nconsiderable promise for transforming the battle against fake news. Generally,\nLLMs represent a double-edged sword in this struggle. One major concern is that\nLLMs can be readily used to craft and disseminate misleading information on a\nlarge scale. This raises the pressing questions: Can LLMs easily generate\nbiased fake news? Do all LLMs have this capability? Conversely, LLMs offer\nvaluable prospects for countering fake news, thanks to their extensive\nknowledge of the world and robust reasoning capabilities. This leads to other\ncritical inquiries: Can we use LLMs to detect fake news, and do they outperform\ntypical detection models? In this paper, we aim to address these pivotal\nquestions by exploring the performance of various LLMs. Our objective is to\nexplore the capability of various LLMs in effectively combating fake news,\nmarking this as the first investigation to analyze seven such models. Our\nresults reveal that while some models adhere strictly to safety protocols,\nrefusing to generate biased or misleading content, other models can readily\nproduce fake news across a spectrum of biases. Additionally, our results show\nthat larger models generally exhibit superior detection abilities and that\nLLM-generated fake news are less likely to be detected than human-written ones.\nFinally, our findings demonstrate that users can benefit from LLM-generated\nexplanations in identifying fake news.', 'With the rise of AI-generated content spewed at scale from large language\nmodels (LLMs), genuine concerns about the spread of fake news have intensified.\nThe perceived ability of LLMs to produce convincing fake news at scale poses\nnew challenges for both human and automated fake news detection systems. To\naddress this gap, this paper presents the findings from a university-level\ncompetition that aimed to explore how LLMs can be used by humans to create fake\nnews, and to assess the ability of human annotators and AI models to detect it.\nA total of 110 participants used LLMs to create 252 unique fake news stories,\nand 84 annotators participated in the detection tasks. Our findings indicate\nthat LLMs are ~68% more effective at detecting real news than humans. However,\nfor fake news detection, the performance of LLMs and humans remains comparable\n(~60% accuracy). Additionally, we examine the impact of visual elements (e.g.,\npictures) in news on the accuracy of detecting fake news stories. Finally, we\nalso examine various strategies used by fake news creators to enhance the\ncredibility of their AI-generated content. This work highlights the increasing\ncomplexity of detecting AI-generated fake news, particularly in collaborative\nhuman-AI settings.']"
106,30,106_geospatial_spatial_code_geographic,"['geospatial', 'spatial', 'code', 'geographic', 'vr', 'gis', 'objects', 'generation', 'data', 'geoentity']","['Large language models (LLMs) have shown promising results in learning and\ncontextualizing information from different forms of data. Recent advancements\nin foundational models, particularly those employing self-attention mechanisms,\nhave significantly enhanced our ability to comprehend the semantics of diverse\ndata types. One such area that could highly benefit from multi-modality is in\nunderstanding geospatial data, which inherently has multiple modalities.\nHowever, current Natural Language Processing (NLP) mechanisms struggle to\neffectively address geospatial queries. Existing pre-trained LLMs are\ninadequately equipped to meet the unique demands of geospatial data, lacking\nthe ability to retrieve precise spatio-temporal data in real-time, thus leading\nto significantly reduced accuracy in answering complex geospatial queries. To\naddress these limitations, we introduce Geode--a pioneering system designed to\ntackle zero-shot geospatial question-answering tasks with high precision using\nspatio-temporal data retrieval. Our approach represents a significant\nimprovement in addressing the limitations of current LLM models, demonstrating\nremarkable improvement in geospatial question-answering abilities compared to\nexisting state-of-the-art pre-trained models.', 'Large language models (LLMs) are being used in data science code generation\ntasks, but they often struggle with complex sequential tasks, leading to\nlogical errors. Their application to geospatial data processing is particularly\nchallenging due to difficulties in incorporating complex data structures and\nspatial constraints, effectively utilizing diverse function calls, and the\ntendency to hallucinate less-used geospatial libraries. To tackle these\nproblems, we introduce GeoAgent, a new interactive framework designed to help\nLLMs handle geospatial data processing more effectively. GeoAgent pioneers the\nintegration of a code interpreter, static analysis, and Retrieval-Augmented\nGeneration (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,\noffering a novel approach to geospatial data processing. In addition, we\ncontribute a new benchmark specifically designed to evaluate the LLM-based\napproach in geospatial tasks. This benchmark leverages a variety of Python\nlibraries and includes both single-turn and multi-turn tasks such as data\nacquisition, data analysis, and visualization. By offering a comprehensive\nevaluation among diverse geospatial contexts, this benchmark sets a new\nstandard for developing LLM-based approaches in geospatial data analysis tasks.\nOur findings suggest that relying solely on knowledge of LLM is insufficient\nfor accurate geospatial task programming, which requires coherent multi-step\nprocesses and multiple function calls. Compared to the baseline LLMs, the\nproposed GeoAgent has demonstrated superior performance, yielding notable\nimprovements in function calls and task completion. In addition, these results\noffer valuable insights for the future development of LLM agents in automatic\ngeospatial data analysis task programming.', 'Software development support tools have been studied for a long time, with\nrecent approaches using Large Language Models (LLMs) for code generation. These\nmodels can generate Python code for data science and machine learning\napplications. LLMs are helpful for software engineers because they increase\nproductivity in daily work. An LLM can also serve as a ""mentor"" for\ninexperienced software developers, and be a viable learning support.\nHigh-quality code generation with LLMs can also be beneficial in geospatial\ndata science. However, this domain poses different challenges, and code\ngeneration LLMs are typically not evaluated on geospatial tasks. Here, we show\nhow we constructed an evaluation benchmark for code generation models, based on\na selection of geospatial tasks. We categorised geospatial tasks based on their\ncomplexity and required tools. Then, we created a dataset with tasks that test\nmodel capabilities in spatial reasoning, spatial data processing, and\ngeospatial tools usage. The dataset consists of specific coding problems that\nwere manually created for high quality. For every problem, we proposed a set of\ntest scenarios that make it possible to automatically check the generated code\nfor correctness. In addition, we tested a selection of existing code generation\nLLMs for code generation in the geospatial domain. We share our dataset and\nreproducible evaluation code on a public GitHub repository, arguing that this\ncan serve as an evaluation benchmark for new LLMs in the future. Our dataset\nwill hopefully contribute to the development new models capable of solving\ngeospatial coding tasks with high accuracy. These models will enable the\ncreation of coding assistants tailored for geospatial applications.']"
107,30,107_energy_power_emissions_environmental,"['energy', 'power', 'emissions', 'environmental', 'carbon', 'sustainable', 'consumption', 'ai', 'sustainability', 'electricity']","['Prominent works in the field of Natural Language Processing have long\nattempted to create new innovative models by improving upon previous model\ntraining approaches, altering model architecture, and developing more in-depth\ndatasets to better their performance. However, with the quickly advancing field\nof NLP comes increased greenhouse gas emissions, posing concerns over the\nenvironmental damage caused by training LLMs. Gaining a comprehensive\nunderstanding of the various costs, particularly those pertaining to\nenvironmental aspects, that are associated with artificial intelligence serves\nas the foundational basis for ensuring safe AI models. Currently,\ninvestigations into the CO2 emissions of AI models remain an emerging area of\nresearch, and as such, in this paper, we evaluate the CO2 emissions of\nwell-known large language models, which have an especially high carbon\nfootprint due to their significant amount of model parameters. We argue for the\ntraining of LLMs in a way that is responsible and sustainable by suggesting\nmeasures for reducing carbon emissions. Furthermore, we discuss how the choice\nof hardware affects CO2 emissions by contrasting the CO2 emissions during model\ntraining for two widely used GPUs. Based on our results, we present the\nbenefits and drawbacks of our proposed solutions and make the argument for the\npossibility of training more environmentally safe AI models without sacrificing\ntheir robustness and performance.', ""Human-produced emissions are growing at an alarming rate, causing already\nobservable changes in the climate and environment in general. Each year global\ncarbon dioxide emissions hit a new record, and it is reported that 0.5% of\ntotal US greenhouse gas emissions are attributed to data centres as of 2021.\nThe release of ChatGPT in late 2022 sparked social interest in Large Language\nModels (LLMs), the new generation of Language Models with a large number of\nparameters and trained on massive amounts of data. Currently, numerous\ncompanies are releasing products featuring various LLMs, with many more models\nin development and awaiting release. Deep Learning research is a competitive\nfield, with only models that reach top performance attracting attention and\nbeing utilized. Hence, achieving better accuracy and results is often the first\npriority, while the model's efficiency and the environmental impact of the\nstudy are neglected. However, LLMs demand substantial computational resources\nand are very costly to train, both financially and environmentally. It becomes\nessential to raise awareness and promote conscious decisions about algorithmic\nand hardware choices. Providing information on training time, the approximate\ncarbon dioxide emissions and power consumption would assist future studies in\nmaking necessary adjustments and determining the compatibility of available\ncomputational resources with model requirements. In this study, we infused T5\nLLM with external knowledge and fine-tuned the model for Question-Answering\ntask. Furthermore, we calculated and reported the approximate environmental\nimpact for both steps. The findings demonstrate that the smaller models may not\nalways be sustainable options, and increased training does not always imply\nbetter performance. The most optimal outcome is achieved by carefully\nconsidering both performance and efficiency factors."", 'Large Language Models (LLMs) have transformed numerous domains by providing\nadvanced capabilities in natural language understanding, generation, and\nreasoning. Despite their groundbreaking applications across industries such as\nresearch, healthcare, and creative media, their rapid adoption raises critical\nconcerns regarding sustainability. This survey paper comprehensively examines\nthe environmental, economic, and computational challenges associated with LLMs,\nfocusing on energy consumption, carbon emissions, and resource utilization in\ndata centers. By synthesizing insights from existing literature, this work\nexplores strategies such as resource-efficient training, sustainable deployment\npractices, and lifecycle assessments to mitigate the environmental impacts of\nLLMs. Key areas of emphasis include energy optimization, renewable energy\nintegration, and balancing performance with sustainability. The findings aim to\nguide researchers, practitioners, and policymakers in developing actionable\nstrategies for sustainable AI systems, fostering a responsible and\nenvironmentally conscious future for artificial intelligence.']"
108,30,108_tool_tools_retrieval_tooluse,"['tool', 'tools', 'retrieval', 'tooluse', 'external', 'invocation', 'usage', 'learning', 'documentation', 'mtubench']","['Large language models (LLMs) have demonstrated exceptional reasoning\ncapabilities, enabling them to solve various complex problems. Recently, this\nability has been applied to the paradigm of tool learning. Tool learning\ninvolves providing examples of tool usage and their corresponding functions,\nallowing LLMs to formulate plans and demonstrate the process of invoking and\nexecuting each tool. LLMs can address tasks that they cannot complete\nindependently, thereby enhancing their potential across different tasks.\nHowever, this approach faces two key challenges. First, redundant error\ncorrection leads to unstable planning and long execution time. Additionally,\ndesigning a correct plan among multiple tools is also a challenge in tool\nlearning. To address these issues, we propose Tool-Planner, a task-processing\nframework based on toolkits. Tool-Planner groups tools based on the API\nfunctions with the same function into a toolkit and allows LLMs to implement\nplanning across the various toolkits. When a tool error occurs, the language\nmodel can reselect and adjust tools based on the toolkit. Experiments show that\nour approach demonstrates a high pass and win rate across different datasets\nand optimizes the planning scheme for tool learning in models such as GPT-4 and\nClaude 3, showcasing the potential of our method. Our code is public at\nhttps://github.com/OceannTwT/Tool-Planner', 'By augmenting Large Language Models (LLMs) with external tools, their\ncapacity to solve complex problems has been significantly enhanced. However,\ndespite ongoing advancements in the parsing capabilities of LLMs, incorporating\nall available tools simultaneously in the prompt remains impractical due to the\nvast number of external tools. Consequently, it is essential to provide LLMs\nwith a precise set of tools tailored to the specific task, considering both\nquantity and quality. Current tool retrieval methods primarily focus on\nrefining the ranking list of tools and directly packaging a fixed number of\ntop-ranked tools as the tool set. However, these approaches often fail to equip\nLLMs with the optimal set of tools prior to execution, since the optimal number\nof tools for different tasks could be different, resulting in inefficiencies\nsuch as redundant or unsuitable tools, which impede immediate access to the\nmost relevant tools. This paper addresses the challenge of recommending precise\ntoolsets for LLMs. We introduce the problem of tool recommendation, define its\nscope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.\nPTR captures an initial, concise set of tools by leveraging historical tool\nbundle usage and dynamically adjusts the tool set by performing tool matching,\nculminating in a multi-view-based tool addition. Additionally, we present a new\ndataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness\nof tool recommendation for LLMs. We further validate our design choices through\ncomprehensive experiments, demonstrating promising accuracy across two open\nbenchmarks and our RecTools dataset.', ""Tool learning aims to enhance and expand large language models' (LLMs)\ncapabilities with external tools, which has gained significant attention\nrecently. Current methods have shown that LLMs can effectively handle a certain\namount of tools through in-context learning or fine-tuning. However, in\nreal-world scenarios, the number of tools is typically extensive and\nirregularly updated, emphasizing the necessity for a dedicated tool retrieval\ncomponent. Tool retrieval is nontrivial due to the following challenges: 1)\ncomplex user instructions and tool descriptions; 2) misalignment between tool\nretrieval and tool usage models. To address the above issues, we propose to\nenhance tool retrieval with iterative feedback from the large language model.\nSpecifically, we prompt the tool usage model, i.e., the LLM, to provide\nfeedback for the tool retriever model in multi-round, which could progressively\nimprove the tool retriever's understanding of instructions and tools and reduce\nthe gap between the two standalone components. We build a unified and\ncomprehensive benchmark to evaluate tool retrieval models. The extensive\nexperiments indicate that our proposed approach achieves advanced performance\nin both in-domain evaluation and out-of-domain evaluation.""]"
109,29,109_anomaly_detection_anomalies_fault,"['anomaly', 'detection', 'anomalies', 'fault', 'series', 'tsad', 'diagnosis', 'time', 'industrial', 'pyod']","[""Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 88.6% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries."", 'Recent studies have shown the ability of large language models to perform a\nvariety of tasks, including time series forecasting. The flexible nature of\nthese models allows them to be used for many applications. In this paper, we\npresent a novel study of large language models used for the challenging task of\ntime series anomaly detection. This problem entails two aspects novel for LLMs:\nthe need for the model to identify part of the input sequence (or multiple\nparts) as anomalous; and the need for it to work with time series data rather\nthan the traditional text input. We introduce sigllm, a framework for time\nseries anomaly detection using large language models. Our framework includes a\ntime-series-to-text conversion module, as well as end-to-end pipelines that\nprompt language models to perform time series anomaly detection. We investigate\ntwo paradigms for testing the abilities of large language models to perform the\ndetection task. First, we present a prompt-based detection method that directly\nasks a language model to indicate which elements of the input are anomalies.\nSecond, we leverage the forecasting capability of a large language model to\nguide the anomaly detection process. We evaluated our framework on 11 datasets\nspanning various sources and 10 pipelines. We show that the forecasting method\nsignificantly outperformed the prompting method in all 11 datasets with respect\nto the F1 score. Moreover, while large language models are capable of finding\nanomalies, state-of-the-art deep learning models are still superior in\nperformance, achieving results 30% better than large language models.', 'For data-constrained, complex and dynamic industrial environments, there is a\ncritical need for transferable and multimodal methodologies to enhance anomaly\ndetection and therefore, prevent costs associated with system failures.\nTypically, traditional PdM approaches are not transferable or multimodal. This\nwork examines the use of Large Language Models (LLMs) for anomaly detection in\ncomplex and dynamic manufacturing systems. The research aims to improve the\ntransferability of anomaly detection models by leveraging Large Language Models\n(LLMs) and seeks to validate the enhanced effectiveness of the proposed\napproach in data-sparse industrial applications. The research also seeks to\nenable more collaborative decision-making between the model and plant operators\nby allowing for the enriching of input series data with semantics.\nAdditionally, the research aims to address the issue of concept drift in\ndynamic industrial settings by integrating an adaptability mechanism. The\nliterature review examines the latest developments in LLM time series tasks\nalongside associated adaptive anomaly detection methods to establish a robust\ntheoretical framework for the proposed architecture. This paper presents a\nnovel model framework (AAD-LLM) that doesn\'t require any training or finetuning\non the dataset it is applied to and is multimodal. Results suggest that anomaly\ndetection can be converted into a ""language"" task to deliver effective,\ncontext-aware detection in data-constrained industrial applications. This work,\ntherefore, contributes significantly to advancements in anomaly detection\nmethodologies.']"
110,29,110_contamination_test_benchmarks_android,"['contamination', 'test', 'benchmarks', 'android', 'apps', 'ncf', 'evaluation', 'benchmark', 'apis', 'api']","['Public benchmarks play an essential role in the evaluation of large language\nmodels. However, data contamination can lead to inflated performance, rendering\nthem unreliable for model comparison. It is therefore crucial to detect\ncontamination and estimate its impact on measured performance. Unfortunately,\nexisting detection methods can be easily evaded and fail to quantify\ncontamination. To overcome these limitations, we propose a novel definition of\ncontamination as artificially inflated and non-generalizing benchmark\nperformance instead of the inclusion of benchmark samples in the training data.\nThis perspective enables us to detect any model with inflated performance,\ni.e., performance that does not generalize to rephrased samples, synthetic\nsamples from the same distribution, or different benchmarks for the same task.\nBased on this insight, we develop ConStat, a statistical method that reliably\ndetects and quantifies contamination by comparing performance between a primary\nand reference benchmark relative to a set of reference models. We demonstrate\nthe effectiveness of ConStat in an extensive evaluation of diverse model\narchitectures, benchmarks, and contamination scenarios and find high levels of\ncontamination in multiple popular models including Mistral, Llama, Yi, and the\ntop-3 Open LLM Leaderboard models.', 'Data contamination has garnered increased attention in the era of large\nlanguage models (LLMs) due to the reliance on extensive internet-derived\ntraining corpora. The issue of training corpus overlap with evaluation\nbenchmarks--referred to as contamination--has been the focus of significant\nrecent research. This body of work aims to identify contamination, understand\nits impacts, and explore mitigation strategies from diverse perspectives.\nHowever, comprehensive studies that provide a clear pathway from foundational\nconcepts to advanced insights are lacking in this nascent field. Therefore, we\npresent a comprehensive survey in the field of data contamination, laying out\nthe key issues, methodologies, and findings to date, and highlighting areas in\nneed of further research and development. In particular, we begin by examining\nthe effects of data contamination across various stages and forms. We then\nprovide a detailed analysis of current contamination detection methods,\ncategorizing them to highlight their focus, assumptions, strengths, and\nlimitations. We also discuss mitigation strategies, offering a clear guide for\nfuture research. This survey serves as a succinct overview of the most recent\nadvancements in data contamination research, providing a straightforward guide\nfor the benefit of future research endeavors.', 'As large language models achieve increasingly impressive results, questions\narise about whether such performance is from generalizability or mere data\nmemorization. Thus, numerous data contamination detection methods have been\nproposed. However, these approaches are often validated with traditional\nbenchmarks and early-stage LLMs, leaving uncertainty about their effectiveness\nwhen evaluating state-of-the-art LLMs on the contamination of more challenging\nbenchmarks. To address this gap and provide a dual investigation of SOTA LLM\ncontamination status and detection method robustness, we evaluate five\ncontamination detection approaches with four state-of-the-art LLMs across eight\nchallenging datasets often used in modern LLM evaluation. Our analysis reveals\nthat (1) Current methods have non-trivial limitations in their assumptions and\npractical applications; (2) Notable difficulties exist in detecting\ncontamination introduced during instruction fine-tuning with answer\naugmentation; and (3) Limited consistencies between SOTA contamination\ndetection techniques. These findings highlight the complexity of contamination\ndetection in advanced LLMs and the urgent need for further research on robust\nand generalizable contamination evaluation. Our code is available at\nhttps://github.com/vsamuel2003/data-contamination.']"
111,28,111_relevance_judgments_ir_collections,"['relevance', 'judgments', 'ir', 'collections', 'trec', 'holes', 'retrieval', 'test', 'umbrela', 'search']","['Test collections play a vital role in evaluation of information retrieval\n(IR) systems. Obtaining a diverse set of user queries for test collection\nconstruction can be challenging, and acquiring relevance judgments, which\nindicate the appropriateness of retrieved documents to a query, is often costly\nand resource-intensive. Generating synthetic datasets using Large Language\nModels (LLMs) has recently gained significant attention in various\napplications. In IR, while previous work exploited the capabilities of LLMs to\ngenerate synthetic queries or documents to augment training data and improve\nthe performance of ranking models, using LLMs for constructing synthetic test\ncollections is relatively unexplored. Previous studies demonstrate that LLMs\nhave the potential to generate synthetic relevance judgments for use in the\nevaluation of IR systems. In this paper, we comprehensively investigate whether\nit is possible to use LLMs to construct fully synthetic test collections by\ngenerating not only synthetic judgments but also synthetic queries. In\nparticular, we analyse whether it is possible to construct reliable synthetic\ntest collections and the potential risks of bias such test collections may\nexhibit towards LLM-based models. Our experiments indicate that using LLMs it\nis possible to construct synthetic test collections that can reliably be used\nfor retrieval evaluation.', 'Using Large Language Models (LLMs) for relevance assessments offers promising\nopportunities to improve Information Retrieval (IR), Natural Language\nProcessing (NLP), and related fields. Indeed, LLMs hold the promise of allowing\nIR experimenters to build evaluation collections with a fraction of the manual\nhuman labor currently required. This could help with fresh topics on which\nthere is still limited knowledge and could mitigate the challenges of\nevaluating ranking systems in low-resource scenarios, where it is challenging\nto find human annotators. Given the fast-paced recent developments in the\ndomain, many questions concerning LLMs as assessors are yet to be answered.\nAmong the aspects that require further investigation, we can list the impact of\nvarious components in a relevance judgment generation pipeline, such as the\nprompt used or the LLM chosen.\n  This paper benchmarks and reports on the results of a large-scale automatic\nrelevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where\ndifferent relevance assessment approaches were proposed. In detail, we release\nand benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track\nrelevance judgments produced by eight international teams who participated in\nthe challenge. Given their diverse nature, these automatically generated\nrelevance judgments can help the community not only investigate systematic\nbiases caused by LLMs but also explore the effectiveness of ensemble models,\nanalyze the trade-offs between different models and human assessors, and\nadvance methodologies for improving automated evaluation techniques. The\nreleased resource is available at the following link:\nhttps://llm4eval.github.io/LLMJudge-benchmark/', 'Building test collections for Information Retrieval evaluation has\ntraditionally been a resource-intensive and time-consuming task, primarily due\nto the dependence on manual relevance judgments. While various cost-effective\nstrategies have been explored, the development of such collections remains a\nsignificant challenge. In this paper, we present GenTREC , the first test\ncollection constructed entirely from documents generated by a Large Language\nModel (LLM), eliminating the need for manual relevance judgments. Our approach\nis based on the assumption that documents generated by an LLM are inherently\nrelevant to the prompts used for their generation. Based on this heuristic, we\nutilized existing TREC search topics to generate documents. We consider a\ndocument relevant only to the prompt that generated it, while other\ndocument-topic pairs are treated as non-relevant. To introduce realistic\nretrieval challenges, we also generated non-relevant documents, ensuring that\nIR systems are tested against a diverse and robust set of materials. The\nresulting GenTREC collection comprises 96,196 documents, 300 topics, and 18,964\nrelevance ""judgments"". We conducted extensive experiments to evaluate GenTREC\nin terms of document quality, relevance judgment accuracy, and evaluation\nreliability. Notably, our findings indicate that the ranking of IR systems\nusing GenTREC is compatible with the evaluations conducted using traditional\nTREC test collections, particularly for P@100, MAP, and RPrec metrics. Overall,\nour results show that our proposed approach offers a promising, low-cost\nalternative for IR evaluation, significantly reducing the burden of building\nand maintaining future IR evaluation resources.']"
112,28,112_agents_attack_security_attacks,"['agents', 'attack', 'security', 'attacks', 'malicious', 'agent', 'vulnerabilities', 'injection', 'defense', 'multiagent']","[""As Large Language Models (LLMs) grow increasingly powerful, multi-agent\nsystems are becoming more prevalent in modern AI applications. Most safety\nresearch, however, has focused on vulnerabilities in single-agent LLMs. These\ninclude prompt injection attacks, where malicious prompts embedded in external\ncontent trick the LLM into executing unintended or harmful actions,\ncompromising the victim's application. In this paper, we reveal a more\ndangerous vector: LLM-to-LLM prompt injection within multi-agent systems. We\nintroduce Prompt Infection, a novel attack where malicious prompts\nself-replicate across interconnected agents, behaving much like a computer\nvirus. This attack poses severe threats, including data theft, scams,\nmisinformation, and system-wide disruption, all while propagating silently\nthrough the system. Our extensive experiments demonstrate that multi-agent\nsystems are highly susceptible, even when agents do not publicly share all\ncommunications. To address this, we propose LLM Tagging, a defense mechanism\nthat, when combined with existing safeguards, significantly mitigates infection\nspread. This work underscores the urgent need for advanced security measures as\nmulti-agent LLM systems become more widely adopted."", ""Although LLM-based agents, powered by Large Language Models (LLMs), can use\nexternal tools and memory mechanisms to solve complex real-world tasks, they\nmay also introduce critical security vulnerabilities. However, the existing\nliterature does not comprehensively evaluate attacks and defenses against\nLLM-based agents. To address this, we introduce Agent Security Bench (ASB), a\ncomprehensive framework designed to formalize, benchmark, and evaluate the\nattacks and defenses of LLM-based agents, including 10 scenarios (e.g.,\ne-commerce, autonomous driving, finance), 10 agents targeting the scenarios,\nover 400 tools, 27 different types of attack/defense methods, and 7 evaluation\nmetrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory\npoisoning attack, a novel Plan-of-Thought backdoor attack, 4 mixed attacks, and\n11 corresponding defenses across 13 LLM backbones. Our benchmark results reveal\ncritical vulnerabilities in different stages of agent operation, including\nsystem prompt, user prompt handling, tool usage, and memory retrieval, with the\nhighest average attack success rate of 84.30\\%, but limited effectiveness shown\nin current defenses, unveiling important works to be done in terms of agent\nsecurity for the community. We also introduce a new metric to evaluate the\nagents' capability to balance utility and security. Our code can be found at\nhttps://github.com/agiresearch/ASB."", ""Recently, autonomous agents built on large language models (LLMs) have\nexperienced significant development and are being deployed in real-world\napplications. These agents can extend the base LLM's capabilities in multiple\nways. For example, a well-built agent using GPT-3.5-Turbo as its core can\noutperform the more advanced GPT-4 model by leveraging external components.\nMore importantly, the usage of tools enables these systems to perform actions\nin the real world, moving from merely generating text to actively interacting\nwith their environment. Given the agents' practical applications and their\nability to execute consequential actions, it is crucial to assess potential\nvulnerabilities. Such autonomous systems can cause more severe damage than a\nstandalone language model if compromised. While some existing research has\nexplored harmful actions by LLM agents, our study approaches the vulnerability\nfrom a different perspective. We introduce a new type of attack that causes\nmalfunctions by misleading the agent into executing repetitive or irrelevant\nactions. We conduct comprehensive evaluations using various attack methods,\nsurfaces, and properties to pinpoint areas of susceptibility. Our experiments\nreveal that these attacks can induce failure rates exceeding 80\\% in multiple\nscenarios. Through attacks on implemented and deployable agents in multi-agent\nscenarios, we accentuate the realistic risks associated with these\nvulnerabilities. To mitigate such attacks, we propose self-examination\ndetection methods. However, our findings indicate these attacks are difficult\nto detect effectively using LLMs alone, highlighting the substantial risks\nassociated with this vulnerability.""]"
113,28,113_explanations_faithfulness_counterfactual_explanation,"['explanations', 'faithfulness', 'counterfactual', 'explanation', 'selfexplanations', 'fmms', 'faithful', 'rationales', 'metrics', 'shap']","['We propose a large language model explainability technique for obtaining\nfaithful natural language explanations by grounding the explanations in a\nreasoning process. When converted to a sequence of tokens, the outputs of the\nreasoning process can become part of the model context and later be decoded to\nnatural language as the model produces either the final answer or the\nexplanation. To improve the faithfulness of the explanations, we propose to use\na joint predict-explain approach, in which the answers and explanations are\ninferred directly from the reasoning sequence, without the explanations being\ndependent on the answers and vice versa. We demonstrate the plausibility of the\nproposed technique by achieving a high alignment between answers and\nexplanations in several problem domains, observing that language models often\nsimply copy the partial decisions from the reasoning sequence into the final\nanswers or explanations. Furthermore, we show that the proposed use of\nreasoning can also improve the quality of the answers.', ""Large Language Models (LLMs) are capable of generating persuasive Natural\nLanguage Explanations (NLEs) to justify their answers. However, the\nfaithfulness of these explanations should not be readily trusted at face value.\nRecent studies have proposed various methods to measure the faithfulness of\nNLEs, typically by inserting perturbations at the explanation or feature level.\nWe argue that these approaches are neither comprehensive nor correctly designed\naccording to the established definition of faithfulness. Moreover, we highlight\nthe risks of grounding faithfulness findings on out-of-distribution samples. In\nthis work, we leverage a causal mediation technique called activation patching,\nto measure the faithfulness of an explanation towards supporting the explained\nanswer. Our proposed metric, Causal Faithfulness quantifies the consistency of\ncausal attributions between explanations and the corresponding model outputs as\nthe indicator of faithfulness. We experimented across models varying from 2B to\n27B parameters and found that models that underwent alignment tuning tend to\nproduce more faithful and plausible explanations. We find that Causal\nFaithfulness is a promising improvement over existing faithfulness tests by\ntaking into account the model's internal computations and avoiding out of\ndistribution concerns that could otherwise undermine the validity of\nfaithfulness assessments. We release the code in\n\\url{https://github.com/wj210/Causal-Faithfulness}"", 'As machine learning becomes more widespread and is used in more critical\napplications, it\'s important to provide explanations for these models, to\nprevent unintended behavior. Unfortunately, many current interpretability\nmethods struggle with faithfulness. Therefore, this Ph.D. thesis investigates\nthe question ""How to provide and ensure faithful explanations for complex\ngeneral-purpose neural NLP models?"" The main thesis is that we should develop\nnew paradigms in interpretability. This is achieved by first developing solid\nfaithfulness metrics and then applying the lessons learned from this\ninvestigation to develop new paradigms. The two new paradigms explored are\nfaithfulness measurable models (FMMs) and self-explanations. The idea in\nself-explanations is to have large language models explain themselves, we\nidentify that current models are not capable of doing this consistently.\nHowever, we suggest how this could be achieved. The idea of FMMs is to create\nmodels that are designed such that measuring faithfulness is cheap and precise.\nThis makes it possible to optimize an explanation towards maximum faithfulness,\nwhich makes FMMs designed to be explained. We find that FMMs yield explanations\nthat are near theoretical optimal in terms of faithfulness. Overall, from all\ninvestigations of faithfulness, results show that post-hoc and intrinsic\nexplanations are by default model and task-dependent. However, this was not the\ncase when using FMMs, even with the same post-hoc explanation methods. This\nshows, that even simple modifications to the model, such as randomly masking\nthe training dataset, as was done in FMMs, can drastically change the situation\nand result in consistently faithful explanations. This answers the question of\nhow to provide and ensure faithful explanations.']"
114,27,114_ocr_document_handwritten_documents,"['ocr', 'document', 'handwritten', 'documents', 'layout', 'recognition', 'text', 'optical', 'extraction', 'lmrpa']","['Multimodal large language models (MLLMs) have shown impressive capabilities\nacross various domains, excelling in processing and understanding information\nfrom multiple modalities. Despite the rapid progress made previously,\ninsufficient OCR ability hinders MLLMs from excelling in text-related tasks. In\nthis paper, we present \\textbf{Ocean-OCR}, a 3B MLLM with state-of-the-art\nperformance on various OCR scenarios and comparable understanding ability on\ngeneral tasks. We employ Native Resolution ViT to enable variable resolution\ninput and utilize a substantial collection of high-quality OCR datasets to\nenhance the model performance. We demonstrate the superiority of Ocean-OCR\nthrough comprehensive experiments on open-source OCR benchmarks and across\nvarious OCR scenarios. These scenarios encompass document understanding, scene\ntext recognition, and handwritten recognition, highlighting the robust OCR\ncapabilities of Ocean-OCR. Note that Ocean-OCR is the first MLLM to outperform\nprofessional OCR models such as TextIn and PaddleOCR.', ""Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge to reduce hallucinations and incorporate\nup-to-date information without retraining. As an essential part of RAG,\nexternal knowledge bases are commonly built by extracting structured data from\nunstructured PDF documents using Optical Character Recognition (OCR). However,\ngiven the imperfect prediction of OCR and the inherent non-uniform\nrepresentation of structured data, knowledge bases inevitably contain various\nOCR noises. In this paper, we introduce OHRBench, the first benchmark for\nunderstanding the cascading impact of OCR on RAG systems. OHRBench includes\n8,561 carefully selected unstructured document images from seven real-world RAG\napplication domains, along with 8,498 Q&A pairs derived from multimodal\nelements in documents, challenging existing OCR solutions used for RAG. To\nbetter understand OCR's impact on RAG systems, we identify two primary types of\nOCR noise: Semantic Noise and Formatting Noise and apply perturbation to\ngenerate a set of structured data with varying degrees of each OCR noise. Using\nOHRBench, we first conduct a comprehensive evaluation of current OCR solutions\nand reveal that none is competent for constructing high-quality knowledge bases\nfor RAG systems. We then systematically evaluate the impact of these two noise\ntypes and demonstrate the trend relationship between the degree of OCR noise\nand RAG performance. Our OHRBench, including PDF documents, Q&As, and the\nground truth structured data are released at:\nhttps://github.com/opendatalab/OHR-Bench"", 'Automating high-volume unstructured data processing is essential for\noperational efficiency. Optical Character Recognition (OCR) is critical but\noften struggles with accuracy and efficiency in complex layouts and ambiguous\ntext. These challenges are especially pronounced in large-scale tasks requiring\nboth speed and precision. This paper introduces LMV-RPA, a Large Model\nVoting-based Robotic Process Automation system to enhance OCR workflows.\nLMV-RPA integrates outputs from OCR engines such as Paddle OCR, Tesseract OCR,\nEasy OCR, and DocTR with Large Language Models (LLMs) like LLaMA 3 and\nGemini-1.5-pro. Using a majority voting mechanism, it processes OCR outputs\ninto structured JSON formats, improving accuracy, particularly in complex\nlayouts. The multi-phase pipeline processes text extracted by OCR engines\nthrough LLMs, combining results to ensure the most accurate outputs. LMV-RPA\nachieves 99 percent accuracy in OCR tasks, surpassing baseline models with 94\npercent, while reducing processing time by 80 percent. Benchmark evaluations\nconfirm its scalability and demonstrate that LMV-RPA offers a faster, more\nreliable, and efficient solution for automating large-scale document processing\ntasks.']"
115,27,115_translation_code_program_programs,"['translation', 'code', 'program', 'programs', 'specifications', 'formal', 'dafny', 'correctness', 'invariants', 'programming']","['Large language models (LLMs) are increasingly being used for the task of\nautomated code translation, which has important real-world applications.\nHowever, most existing approaches use only the source code of a program as an\ninput to an LLM, and do not consider the different kinds of specifications that\ncan be extracted from a program. In this paper, we propose SpecTra, a\nmulti-stage approach that uses a novel self-consistency filter to first\ngenerate high-quality static specifications, test cases, and natural language\ndescriptions from a given program, and then uses these along with the source\ncode to improve the quality of LLM-generated translations. We evaluate SpecTra\non three code translation tasks - C to Rust, C to Go, and JavaScript to\nTypeScript - and show that it can enhance the performance of six popular LLMs\non these tasks by up to 10 percentage points and a relative improvement of\n26\\%. Our research suggests that generating high-quality specifications could\nbe a promising and efficient way to improve the performance of LLMs for code\ntranslation. We make our code and data available, anonymized for review.', ""Code translation transforms programs from one programming language (PL) to\nanother. Several rule-based transpilers have been designed to automate code\ntranslation between different pairs of PLs. However, the rules can become\nobsolete as the PLs evolve and cannot generalize to other PLs. Recent studies\nhave explored the automation of code translation using Large Language Models\n(LLMs). One key observation is that such techniques may work well for crafted\nbenchmarks but fail to generalize to the scale and complexity of real-world\nprojects with dependencies, custom types, PL-specific features, etc.\n  We propose AlphaTrans, a neuro-symbolic approach to automate repository-level\ncode translation. AlphaTrans translates both source and test code, and employs\nmultiple levels of validation to ensure the translation preserves the\nfunctionality of the source program. To break down the problem for LLMs,\nAlphaTrans leverages program analysis to decompose the program into fragments\nand translates them in the reverse call order. We leveraged AlphaTrans to\ntranslate ten real-world open-source projects consisting of <836, 8575, 2719>\nclasses, methods, and tests. AlphaTrans breaks down these projects into 17874\nfragments and translates the entire repository. 96.40% of the translated\nfragments are syntactically correct, and AlphaTrans validates the translations'\nruntime behavior and functional correctness for 27.03% and 25.14% of fragments.\nOn average, the integrated translation and validation take 34 hours to\ntranslate a project, showing its scalability in practice. For the incorrect\ntranslations, AlphaTrans generates a report including existing translation,\nstack trace, test errors, or assertion failures. We provided these artifacts to\ntwo developers to fix the translation bugs in four projects. They were able to\nfix the issues in 20.1 hours on average and achieve all passing tests."", 'Verification-aware programming languages such as Dafny and F* provide means\nto formally specify and prove properties of a program. Although the problem of\nchecking an implementation against a specification can be defined mechanically,\nthere is no algorithmic way of ensuring the correctness of the {\\it user-intent\nformalization for programs}, expressed as a formal specification. This is\nbecause intent or requirement is expressed {\\it informally} in natural language\nand the specification is a formal artefact. Despite, the advent of large\nlanguage models (LLMs) has made tremendous strides bridging the gap between\ninformal intent and formal program implementations recently, driven in large\nparts by benchmarks and automated metrics for evaluation.\n  Recent work has proposed a framework for evaluating the {\\it user-intent\nformalization} problem for mainstream programming\nlanguages~\\cite{endres-fse24}. However, such an approach does not readily\nextend to verification-aware languages that support rich specifications (using\nquantifiers and ghost variables) that cannot be evaluated through dynamic\nexecution. Previous work also required generating program mutants using LLMs to\ncreate the benchmark. We advocate an alternate, perhaps simpler approach of\n{\\it symbolically testing specifications} to provide an intuitive metric for\nevaluating the quality of specifications for verification-aware languages. We\ndemonstrate that our automated metric agrees closely on a human-labeled dataset\nof Dafny specifications for the popular MBPP code-generation benchmark, yet\ndemonstrates cases where the human labeling is not perfect. We also outline\nformal verification challenges that need to be addressed to apply the technique\nmore widely. We believe our work provides a stepping stone to enable the\nestablishment of a benchmark and research agenda for the problem of user-intent\nformalization for programs.']"
116,27,116_srl_metonymy_tst_nlp,"['srl', 'metonymy', 'tst', 'nlp', 'paraphrastic', 'paraphrase', 'fastfit', 'lsr', 'ls', 'noun']","[""Large Language Models (LLMs) play a crucial role in capturing structured\nsemantics to enhance language understanding, improve interpretability, and\nreduce bias. Nevertheless, an ongoing controversy exists over the extent to\nwhich LLMs can grasp structured semantics. To assess this, we propose using\nSemantic Role Labeling (SRL) as a fundamental task to explore LLMs' ability to\nextract structured semantics. In our assessment, we employ the prompting\napproach, which leads to the creation of our few-shot SRL parser, called\nPromptSRL. PromptSRL enables LLMs to map natural languages to explicit semantic\nstructures, which provides an interpretable window into the properties of LLMs.\nWe find interesting potential: LLMs can indeed capture semantic structures, and\nscaling-up doesn't always mirror potential. Additionally, limitations of LLMs\nare observed in C-arguments, etc. Lastly, we are surprised to discover that\nsignificant overlap in the errors is made by both LLMs and untrained humans,\naccounting for almost 30% of all errors."", 'Semantic role labeling (SRL) enriches many downstream applications, e.g.,\nmachine translation, question answering, summarization, and stance/belief\ndetection. However, building multilingual SRL models is challenging due to the\nscarcity of semantically annotated corpora for multiple languages. Moreover,\nstate-of-the-art SRL projection (XSRL) based on large language models (LLMs)\nyields output that is riddled with spurious role labels. Remediation of such\nhallucinations is not straightforward due to the lack of explainability of\nLLMs. We show that hallucinated role labels are related to naturally occurring\ndivergence types that interfere with initial alignments. We implement\nDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging\nlinguistically-informed alignment remediation followed by greedy First-Come\nFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL\nprojection without additional transformer-based machinery, beating XSRL in both\nhuman and automatic comparisons, and advancing beyond headwords to accommodate\nphrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our\nground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%\n(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%\n(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our\napproach to other language pairs (e.g., English-Tagalog).', 'Semantic role labeling (SRL) is a central natural language processing (NLP)\ntask aiming to understand the semantic roles within texts, facilitating a wide\nrange of downstream applications. While SRL has garnered extensive and enduring\nresearch, there is currently a lack of a comprehensive survey that thoroughly\norganizes and synthesizes the field. This paper aims to review the entire\nresearch trajectory of the SRL community over the past two decades. We begin by\nproviding a complete definition of SRL. To offer a comprehensive taxonomy, we\ncategorize SRL methodologies into four key perspectives: model architectures,\nsyntax feature modeling, application scenarios, and multi-modal extensions.\nFurther, we discuss SRL benchmarks, evaluation metrics, and paradigm modeling\napproaches, while also exploring practical applications across various domains.\nFinally, we analyze future research directions in SRL, addressing the evolving\nrole of SRL in the age of large language models (LLMs) and its potential impact\non the broader NLP landscape. We maintain a public repository and consistently\nupdate related resources at: https://github.com/DreamH1gh/Awesome-SRL']"
117,27,117_peer_reviews_review_reviewers,"['peer', 'reviews', 'review', 'reviewers', 'papers', 'peerreview', 'scientific', 'aspr', 'submissions', 'reviewer']","['Large Language Models (LLMs) have demonstrated wide-ranging applications\nacross various fields and have shown significant potential in the academic\npeer-review process. However, existing applications are primarily limited to\nstatic review generation based on submitted papers, which fail to capture the\ndynamic and iterative nature of real-world peer reviews. In this paper, we\nreformulate the peer-review process as a multi-turn, long-context dialogue,\nincorporating distinct roles for authors, reviewers, and decision makers. We\nconstruct a comprehensive dataset containing over 26,841 papers with 92,017\nreviews collected from multiple sources, including the top-tier conference and\nprestigious journal. This dataset is meticulously designed to facilitate the\napplications of LLMs for multi-turn dialogues, effectively simulating the\ncomplete peer-review process. Furthermore, we propose a series of metrics to\nevaluate the performance of LLMs for each role under this reformulated\npeer-review setting, ensuring fair and comprehensive evaluations. We believe\nthis work provides a promising perspective on enhancing the LLM-driven\npeer-review process by incorporating dynamic, role-based interactions. It\naligns closely with the iterative and interactive nature of real-world academic\npeer review, offering a robust foundation for future research and development\nin this area. We open-source the dataset at\nhttps://github.com/chengtan9907/ReviewMT.', ""Scholarly peer review is a cornerstone of scientific advancement, but the\nsystem is under strain due to increasing manuscript submissions and the\nlabor-intensive nature of the process. Recent advancements in large language\nmodels (LLMs) have led to their integration into peer review, with promising\nresults such as substantial overlaps between LLM- and human-generated reviews.\nHowever, the unchecked adoption of LLMs poses significant risks to the\nintegrity of the peer review system. In this study, we comprehensively analyze\nthe vulnerabilities of LLM-generated reviews by focusing on manipulation and\ninherent flaws. Our experiments show that injecting covert deliberate content\ninto manuscripts allows authors to explicitly manipulate LLM reviews, leading\nto inflated ratings and reduced alignment with human reviews. In a simulation,\nwe find that manipulating 5% of the reviews could potentially cause 12% of the\npapers to lose their position in the top 30% rankings. Implicit manipulation,\nwhere authors strategically highlight minor limitations in their papers,\nfurther demonstrates LLMs' susceptibility compared to human reviewers, with a\n4.5 times higher consistency with disclosed limitations. Additionally, LLMs\nexhibit inherent flaws, such as potentially assigning higher ratings to\nincomplete papers compared to full papers and favoring well-known authors in\nsingle-blind review process. These findings highlight the risks of\nover-reliance on LLMs in peer review, underscoring that we are not yet ready\nfor widespread adoption and emphasizing the need for robust safeguards."", 'Peer review is a critical process for ensuring the integrity of published\nscientific research. Confidence in this process is predicated on the assumption\nthat experts in the relevant domain give careful consideration to the merits of\nmanuscripts which are submitted for publication. With the recent rapid\nadvancements in large language models (LLMs), a new risk to the peer review\nprocess is that negligent reviewers will rely on LLMs to perform the often time\nconsuming process of reviewing a paper. However, there is a lack of existing\nresources for benchmarking the detectability of AI text in the domain of peer\nreview.\n  To address this deficiency, we introduce a comprehensive dataset containing a\ntotal of 788,984 AI-written peer reviews paired with corresponding human\nreviews, covering 8 years of papers submitted to each of two leading AI\nresearch conferences (ICLR and NeurIPS). We use this new resource to evaluate\nthe ability of 18 existing AI text detection algorithms to distinguish between\npeer reviews written by humans and different state-of-the-art LLMs. Motivated\nby the shortcomings of existing methods, we propose a new detection approach\nwhich surpasses existing methods in the identification of AI written peer\nreviews. Our work reveals the difficulty of identifying AI-generated text at\nthe individual peer review level, highlighting the urgent need for new tools\nand methods to detect this unethical use of generative AI.']"
118,26,118_protein_proteins_rna_sequences,"['protein', 'proteins', 'rna', 'sequences', 'sequence', 'structural', 'toursynbioagent', 'engineering', 'esm2', 'prediction']","['Proteins are fundamental components of biological systems and can be\nrepresented through various modalities, including sequences, structures, and\ntextual descriptions. Despite the advances in deep learning and scientific\nlarge language models (LLMs) for protein research, current methodologies\npredominantly focus on limited specialized tasks -- often predicting one\nprotein modality from another. These approaches restrict the understanding and\ngeneration of multimodal protein data. In contrast, large multimodal models\nhave demonstrated potential capabilities in generating any-to-any content like\ntext, images, and videos, thus enriching user interactions across various\ndomains. Integrating these multimodal model technologies into protein research\noffers significant promise by potentially transforming how proteins are\nstudied. To this end, we introduce HelixProtX, a system built upon the large\nmultimodal model, aiming to offer a comprehensive solution to protein research\nby supporting any-to-any protein modality generation. Unlike existing methods,\nit allows for the transformation of any input protein modality into any desired\nprotein modality. The experimental results affirm the advanced capabilities of\nHelixProtX, not only in generating functional descriptions from amino acid\nsequences but also in executing critical tasks such as designing protein\nsequences and structures from textual descriptions. Preliminary findings\nindicate that HelixProtX consistently achieves superior accuracy across a range\nof protein-related tasks, outperforming existing state-of-the-art models. By\nintegrating multimodal large models into protein research, HelixProtX opens new\navenues for understanding protein biology, thereby promising to accelerate\nscientific discovery.', ""Considering the significance of proteins, computational protein science has\nalways been a critical scientific field, dedicated to revealing knowledge and\ndeveloping applications within the protein sequence-structure-function\nparadigm. In the last few decades, Artificial Intelligence (AI) has made\nsignificant impacts in computational protein science, leading to notable\nsuccesses in specific protein modeling tasks. However, those previous AI models\nstill meet limitations, such as the difficulty in comprehending the semantics\nof protein sequences, and the inability to generalize across a wide range of\nprotein modeling tasks. Recently, LLMs have emerged as a milestone in AI due to\ntheir unprecedented language processing & generalization capability. They can\npromote comprehensive progress in fields rather than solving individual tasks.\nAs a result, researchers have actively introduced LLM techniques in\ncomputational protein science, developing protein Language Models (pLMs) that\nskillfully grasp the foundational knowledge of proteins and can be effectively\ngeneralized to solve a diversity of sequence-structure-function reasoning\nproblems. While witnessing prosperous developments, it's necessary to present a\nsystematic overview of computational protein science empowered by LLM\ntechniques. First, we summarize existing pLMs into categories based on their\nmastered protein knowledge, i.e., underlying sequence patterns, explicit\nstructural and functional information, and external scientific languages.\nSecond, we introduce the utilization and adaptation of pLMs, highlighting their\nremarkable achievements in promoting protein structure prediction, protein\nfunction prediction, and protein design studies. Then, we describe the\npractical application of pLMs in antibody design, enzyme design, and drug\ndiscovery. Finally, we specifically discuss the promising future directions in\nthis fast-growing field."", 'Protein-specific large language models (Protein LLMs) are revolutionizing\nprotein science by enabling more efficient protein structure prediction,\nfunction annotation, and design. While existing surveys focus on specific\naspects or applications, this work provides the first comprehensive overview of\nProtein LLMs, covering their architectures, training datasets, evaluation\nmetrics, and diverse applications. Through a systematic analysis of over 100\narticles, we propose a structured taxonomy of state-of-the-art Protein LLMs,\nanalyze how they leverage large-scale protein sequence data for improved\naccuracy, and explore their potential in advancing protein engineering and\nbiomedical research. Additionally, we discuss key challenges and future\ndirections, positioning Protein LLMs as essential tools for scientific\ndiscovery in protein science. Resources are maintained at\nhttps://github.com/Yijia-Xiao/Protein-LLM-Survey.']"
119,26,119_tabular_data_synthetic_table,"['tabular', 'data', 'synthetic', 'table', 'learning', 'rwd', 'datasets', 'ctgan', 'transfer', 'jolt']","['Despite the artificial intelligence (AI) revolution, deep learning has yet to\nachieve much success with tabular data due to heterogeneous feature space and\nlimited sample sizes without viable transfer learning. The new era of\ngenerative AI, powered by large language models (LLM), brings unprecedented\nlearning opportunities to diverse data and domains. This paper investigates the\neffectiveness of an LLM application programming interface (API) and transfer\nlearning of LLM in tabular data classification. LLM APIs respond to input text\nprompts with tokenized data and instructions, whereas transfer learning\nfinetunes an LLM for a target classification task. This paper proposes an\nend-to-end finetuning of LLM to demonstrate cross-data transfer learning on ten\nbenchmark data sets when large pre-trained tabular data models do not exist to\nfacilitate transfer learning. The proposed LLM finetuning method outperforms\nstate-of-the-art machine and deep learning methods on tabular data with less\nthan ten features - a standard feature size for tabular data sets. The transfer\nlearning approach uses a fraction of the computational cost of other deep\nlearning or API-based solutions while ensuring competitive or superior\nclassification performance.', 'Synthetic tabular data generation has gained significant attention for its\npotential in data augmentation, software testing and privacy-preserving data\nsharing. However, most research has primarily focused on larger datasets and\nevaluating their quality in terms of metrics like column-wise statistical\ndistributions and inter-feature correlations, while often overlooking its\nutility for data augmentation, particularly for datasets whose data is scarce.\nIn this paper, we propose Tabular Auto-Encoder Generative Adversarial Network\n(TAEGAN), an improved GAN-based framework for generating high-quality tabular\ndata. Although large language models (LLMs)-based methods represent the\nstate-of-the-art in synthetic tabular data generation, they are often overkill\nfor small datasets due to their extensive size and complexity. TAEGAN employs a\nmasked auto-encoder as the generator, which for the first time introduces the\npower of self-supervised pre-training in tabular data generation so that\nessentially exposes the networks to more information. We extensively evaluate\nTAEGAN against five state-of-the-art synthetic tabular data generation\nalgorithms. Results from 10 datasets show that TAEGAN outperforms existing\ndeep-learning-based tabular data generation models on 9 out of 10 datasets on\nthe machine learning efficacy and achieves superior data augmentation\nperformance on 7 out of 8 smaller datasets.', 'Synthetic tabular data generation has emerged as a promising method to\naddress limited data availability and privacy concerns. With the sharp increase\nin the performance of large language models in recent years, researchers have\nbeen interested in applying these models to the generation of tabular data.\nHowever, little is known about the quality of the generated tabular data from\nlarge language models. The predominant method for assessing the quality of\nsynthetic tabular data is the train-synthetic-test-real approach, where the\nartificial examples are compared to the original by how well machine learning\nmodels, trained separately on the real and synthetic sets, perform in some\ndownstream tasks. This method does not directly measure how closely the\ndistribution of generated data approximates that of the original. This paper\nintroduces rigorous methods for directly assessing synthetic tabular data\nagainst real data by looking at inter-column dependencies within the data. We\nfind that large language models (GPT-2), both when queried via few-shot\nprompting and when fine-tuned, and GAN (CTGAN) models do not produce data with\ndependencies that mirror the original real data. Results from this study can\ninform future practice in synthetic data generation to improve data quality.']"
120,26,120_audio_speech_codec_codecs,"['audio', 'speech', 'codec', 'codecs', 'tts', 'discrete', 'codebook', 'tokens', 'neural', 'texttospeech']","['Building upon advancements in Large Language Models (LLMs), the field of\naudio processing has seen increased interest in training audio generation tasks\nwith discrete audio token sequences. However, directly discretizing audio by\nneural audio codecs often results in sequences that fundamentally differ from\ntext sequences. Unlike text, where text token sequences are deterministic,\ndiscrete audio tokens can exhibit significant variability based on contextual\nfactors, while still producing perceptually identical audio segments. We refer\nto this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}.\nThis inconsistency can lead to a single audio segment being represented by\nmultiple divergent sequences, which creates confusion in neural codec language\nmodels and results in omissions and repetitions during speech generation. In\nthis paper, we quantitatively analyze the DRI phenomenon within popular audio\ntokenizers such as EnCodec. Our approach effectively mitigates the DRI\nphenomenon of the neural audio codec. Furthermore, extensive experiments on the\nneural codec language model over LibriTTS and large-scale MLS datases (44,000\nhours) demonstrate the effectiveness and generality of our method. The demo of\naudio samples is available\nonline~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.', 'Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)', 'Large language models (LLMs) have significantly advanced audio processing\nthrough audio codecs that convert audio into discrete tokens, enabling the\napplication of language modeling techniques to audio data. However, audio\ncodecs often operate at high frame rates, resulting in slow training and\ninference, especially for autoregressive models. To address this challenge, we\npresent the Low Frame-rate Speech Codec (LFSC): a neural audio codec that\nleverages finite scalar quantization and adversarial training with large speech\nlanguage models to achieve high-quality audio compression with a 1.89 kbps\nbitrate and 21.5 frames per second. We demonstrate that our novel codec can\nmake the inference of LLM-based text-to-speech models around three times faster\nwhile improving intelligibility and producing quality comparable to previous\nmodels.']"
121,26,121_quantum_computing_classical_quantuminspired,"['quantum', 'computing', 'classical', 'quantuminspired', 'tips', 'circuits', 'flaky', 'qiskit', 'quan', 'qnns']","[""Quantum computing is an emerging field recognized for the significant speedup\nit offers over classical computing through quantum algorithms. However,\ndesigning and implementing quantum algorithms pose challenges due to the\ncomplex nature of quantum mechanics and the necessity for precise control over\nquantum states. Despite the significant advancements in AI, there has been a\nlack of datasets specifically tailored for this purpose. In this work, we\nintroduce QCircuitNet, the first benchmark and test dataset designed to\nevaluate AI's capability in designing and implementing quantum algorithms in\nthe form of quantum circuit codes. Unlike using AI for writing traditional\ncodes, this task is fundamentally different and significantly more complicated\ndue to highly flexible design space and intricate manipulation of qubits. Our\nkey contributions include: 1. A general framework which formulates the key\nfeatures of quantum algorithm design task for Large Language Models. 2.\nImplementation for a wide range of quantum algorithms from basic primitives to\nadvanced applications, with easy extension to more quantum algorithms. 3.\nAutomatic validation and verification functions, allowing for iterative\nevaluation and interactive reasoning without human inspection. 4. Promising\npotential as a training dataset through primitive fine-tuning results. We\nobserved several interesting experimental phenomena: fine-tuning does not\nalways outperform few-shot learning, and LLMs tend to exhibit consistent error\npatterns. QCircuitNet provides a comprehensive benchmark for AI-driven quantum\nalgorithm design, offering advantages in model evaluation and improvement,\nwhile also revealing some limitations of LLMs in this domain."", 'Code Large Language Models (Code LLMs) have emerged as powerful tools,\nrevolutionizing the software development landscape by automating the coding\nprocess and reducing time and effort required to build applications. This paper\nfocuses on training Code LLMs to specialize in the field of quantum computing.\nWe begin by discussing the unique needs of quantum computing programming, which\ndiffer significantly from classical programming approaches or languages. A Code\nLLM specializing in quantum computing requires a foundational understanding of\nquantum computing and quantum information theory. However, the scarcity of\navailable quantum code examples and the rapidly evolving field, which\nnecessitates continuous dataset updates, present significant challenges.\nMoreover, we discuss our work on training Code LLMs to produce high-quality\nquantum code using the Qiskit library. This work includes an examination of the\nvarious aspects of the LLMs used for training and the specific training\nconditions, as well as the results obtained with our current models. To\nevaluate our models, we have developed a custom benchmark, similar to\nHumanEval, which includes a set of tests specifically designed for the field of\nquantum computing programming using Qiskit. Our findings indicate that our\nmodel outperforms existing state-of-the-art models in quantum computing tasks.\nWe also provide examples of code suggestions, comparing our model to other\nrelevant code LLMs. Finally, we introduce a discussion on the potential\nbenefits of Code LLMs for quantum computing computational scientists,\nresearchers, and practitioners. We also explore various features and future\nwork that could be relevant in this context.', 'Quantum computers leverage the unique advantages of quantum mechanics to\nachieve acceleration over classical computers for certain problems. Currently,\nvarious quantum simulators provide powerful tools for researchers, but\nsimulating quantum evolution with these simulators often incurs high time\ncosts. Additionally, resource consumption grows exponentially as the number of\nquantum bits increases. To address this issue, our research aims to utilize\nLarge Language Models (LLMs) to simulate quantum circuits. This paper details\nthe process of constructing 1-qubit and 2-qubit quantum simulator models,\nextending to multiple qubits, and ultimately implementing a 3-qubit example.\nOur study demonstrates that LLMs can effectively learn and predict the\nevolution patterns among quantum bits, with minimal error compared to the\ntheoretical output states. Even when dealing with quantum circuits comprising\nan exponential number of quantum gates, LLMs remain computationally efficient.\nOverall, our results highlight the potential of LLMs to predict the outputs of\ncomplex quantum dynamics, achieving speeds far surpassing those required to run\nthe same process on a quantum computer. This finding provides new insights and\ntools for applying machine learning methods in the field of quantum computing.']"
122,25,122_ner_entity_named_entities,"['ner', 'entity', 'named', 'entities', 'recognition', 'fewshot', 'zeroshot', 'span', 'vdms', 'crossdomain']","['Spoken named entity recognition (NER) aims to identify named entities from\nspeech, playing an important role in speech processing. New named entities\nappear every day, however, annotating their Spoken NER data is costly. In this\npaper, we demonstrate that existing Spoken NER systems perform poorly when\ndealing with previously unseen named entities. To tackle this challenge, we\npropose a method for generating Spoken NER data based on a named entity\ndictionary (NED) to reduce costs. Specifically, we first use a large language\nmodel (LLM) to generate sentences from the sampled named entities and then use\na text-to-speech (TTS) system to generate the speech. Furthermore, we introduce\na noise metric to filter out noisy data. To evaluate our approach, we release a\nnovel Spoken NER benchmark along with a corresponding NED containing 8,853\nentities. Experiment results show that our method achieves state-of-the-art\n(SOTA) performance in the in-domain, zero-shot domain adaptation, and fully\nzero-shot settings. Our data will be available at\nhttps://github.com/DeepLearnXMU/HeardU.', ""This paper presents ReverseNER, a method aimed at overcoming the limitation\nof large language models (LLMs) in zero-shot named entity recognition (NER)\ntasks, arising from their reliance on pre-provided demonstrations. ReverseNER\ntackles this challenge by constructing a reliable example library composed of\ndozens of entity-labeled sentences, generated through the reverse process of\nNER. Specifically, while conventional NER methods label entities in a sentence,\nReverseNER features reversing the process by using an LLM to generate entities\nfrom their definitions and subsequently expand them into full sentences. During\nthe entity expansion process, the LLM is guided to generate sentences by\nreplicating the structures of a set of specific \\textsl{feature sentences},\nextracted from the task sentences by clustering. This expansion process\nproduces dozens of entity-labeled task-relevant sentences. After constructing\nthe example library, the method selects several semantically similar\nentity-labeled examples for each task sentence as references to facilitate the\nLLM's entity recognition. We also propose an entity-level self-consistency\nscoring mechanism to improve NER performance with LLMs. Experiments show that\nReverseNER significantly outperforms other zero-shot NER methods with LLMs,\nmarking a notable improvement in NER for domains without labeled data, while\ndeclining computational resource consumption."", 'In recent years, the rise of large language models (LLMs) has made it\npossible to directly achieve named entity recognition (NER) without any\ndemonstration samples or only using a few samples through in-context learning\n(ICL). However, standard ICL only helps LLMs understand task instructions,\nformat and input-label mapping, but neglects the particularity of the NER task\nitself. In this paper, we propose a new prompting framework P-ICL to better\nachieve NER with LLMs, in which some point entities are leveraged as the\nauxiliary information to recognize each entity type. With such significant\ninformation, the LLM can achieve entity classification more precisely. To\nobtain optimal point entities for prompting LLMs, we also proposed a point\nentity selection method based on K-Means clustering. Our extensive experiments\non some representative NER benchmarks verify the effectiveness of our proposed\nstrategies in P-ICL and point entity selection.']"
123,25,123_analogies_metaphor_analogical_analogy,"['analogies', 'metaphor', 'analogical', 'analogy', 'metaphors', 'humans', 'dpl', 'cognitive', 'mappings', 'reasoning']","['Historical analogies, which compare known past events with contemporary but\nunfamiliar events, are important abilities that help people make decisions and\nunderstand the world. However, research in applied history suggests that people\nhave difficulty finding appropriate analogies. And previous studies in the AI\ncommunity have also overlooked historical analogies. To fill this gap, in this\npaper, we focus on the historical analogy acquisition task, which aims to\nacquire analogous historical events for a given event. We explore retrieval and\ngeneration methods for acquiring historical analogies based on different large\nlanguage models (LLMs). Furthermore, we propose a self-reflection method to\nmitigate hallucinations and stereotypes when LLMs generate historical\nanalogies. Through human evaluations and our specially designed automatic\nmulti-dimensional assessment, we find that LLMs generally have a good potential\nfor historical analogies. And the performance of the models can be further\nimproved by using our self-reflection method.', 'Extracting metaphors and analogies from free text requires high-level\nreasoning abilities such as abstraction and language understanding. Our study\nfocuses on the extraction of the concepts that form metaphoric analogies in\nliterary texts. To this end, we construct a novel dataset in this domain with\nthe help of domain experts. We compare the out-of-the-box ability of recent\nlarge language models (LLMs) to structure metaphoric mappings from fragments of\ntexts containing proportional analogies. The models are further evaluated on\nthe generation of implicit elements of the analogy, which are indirectly\nsuggested in the texts and inferred by human readers. The competitive results\nobtained by LLMs in our experiments are encouraging and open up new avenues\nsuch as automatically extracting analogies and metaphors from text instead of\ninvesting resources in domain experts to manually label data.', ""LLMs have performed well on several reasoning benchmarks, including ones that\ntest analogical reasoning abilities. However, there is debate on the extent to\nwhich they are performing general abstract reasoning versus employing\nnon-robust processes, e.g., that overly rely on similarity to pre-training\ndata. Here we investigate the robustness of analogy-making abilities previously\nclaimed for LLMs on three of four domains studied by Webb, Holyoak, and Lu\n(2023): letter-string analogies, digit matrices, and story analogies. For each\ndomain we test humans and GPT models on robustness to variants of the original\nanalogy problems that test the same abstract reasoning abilities but are likely\ndissimilar from tasks in the pre-training data. The performance of a system\nthat uses robust abstract reasoning should not decline substantially on these\nvariants.\n  On simple letter-string analogies, we find that while the performance of\nhumans remains high for two types of variants we tested, the GPT models'\nperformance declines sharply. This pattern is less pronounced as the complexity\nof these problems is increased, as both humans and GPT models perform poorly on\nboth the original and variant problems requiring more complex analogies. On\ndigit-matrix problems, we find a similar pattern but only on one out of the two\ntypes of variants we tested. On story-based analogy problems, we find that,\nunlike humans, the performance of GPT models are susceptible to answer-order\neffects, and that GPT models also may be more sensitive than humans to\nparaphrasing.\n  This work provides evidence that LLMs often lack the robustness of zero-shot\nhuman analogy-making, exhibiting brittleness on most of the variations we\ntested. More generally, this work points to the importance of carefully\nevaluating AI systems not only for accuracy but also robustness when testing\ntheir cognitive capabilities.""]"
124,25,124_scientific_astronomy_astronomical_ai,"['scientific', 'astronomy', 'astronomical', 'ai', 'discovery', 'agents', 'mas', 'mephisto', 'fathomgpt', 'data']","['Artificial Intelligence has proven to be a transformative tool for advancing\nscientific research across a wide range of disciplines. However, a significant\ngap still exists between AI and scientific communities, limiting the full\npotential of AI methods in driving broad scientific discovery. Existing efforts\nin identifying and bridging this gap have often relied on qualitative\nexamination of small samples of literature, offering a limited perspective on\nthe broader AI4Science landscape. In this work, we present a large-scale\nanalysis of the AI4Science literature, starting by using large language models\nto identify scientific problems and AI methods in publications from top science\nand AI venues. Leveraging this new dataset, we quantitatively highlight key\ndisparities between AI methods and scientific problems, revealing substantial\nopportunities for deeper AI integration across scientific disciplines.\nFurthermore, we explore the potential and challenges of facilitating\ncollaboration between AI and scientific communities through the lens of link\nprediction. Our findings and tools aim to promote more impactful\ninterdisciplinary collaborations and accelerate scientific discovery through\ndeeper and broader AI integration. Our code and dataset are available at:\nhttps://github.com/charles-pyj/Bridging-AI-and-Science.', 'Identifying and predicting the factors that contribute to the success of\ninterdisciplinary research is crucial for advancing scientific discovery.\nHowever, there is a lack of methods to quantify the integration of new ideas\nand technological advancements in astronomical research and how these new\ntechnologies drive further scientific breakthroughs. Large language models,\nwith their ability to extract key concepts from vast literature beyond keyword\nsearches, provide a new tool to quantify such processes. In this study, we\nextracted concepts in astronomical research from 297,807 publications between\n1993 and 2024 using large language models, resulting in a set of 24,939\nconcepts. These concepts were then used to form a knowledge graph, where the\nlink strength between any two concepts was determined by their relevance\nthrough the citation-reference relationships. By calculating this relevance\nacross different time periods, we quantified the impact of numerical\nsimulations and machine learning on astronomical research. The knowledge graph\ndemonstrates two phases of development: a phase where the technology was\nintegrated and another where the technology was explored in scientific\ndiscovery. The knowledge graph reveals that despite machine learning has made\nmuch inroad in astronomy, there is currently a lack of new concept development\nat the intersection of AI and Astronomy, which may be the current bottleneck\npreventing machine learning from further transforming the field of astronomy.', 'Scientific discovery is a complex cognitive process that has driven human\nknowledge and technological progress for centuries. While artificial\nintelligence (AI) has made significant advances in automating aspects of\nscientific reasoning, simulation, and experimentation, we still lack integrated\nAI systems capable of performing autonomous long-term scientific research and\ndiscovery. This paper examines the current state of AI for scientific\ndiscovery, highlighting recent progress in large language models and other AI\ntechniques applied to scientific tasks. We then outline key challenges and\npromising research directions toward developing more comprehensive AI systems\nfor scientific discovery, including the need for science-focused AI agents,\nimproved benchmarks and evaluation metrics, multimodal scientific\nrepresentations, and unified frameworks combining reasoning, theorem proving,\nand data-driven modeling. Addressing these challenges could lead to\ntransformative AI tools to accelerate progress across disciplines towards\nscientific discovery.']"
125,24,125_brain_fmri_functional_representations,"['brain', 'fmri', 'functional', 'representations', 'brainlike', 'activity', 'neural', 'alignment', 'human', 'mater']","['Given the remarkable capabilities of large language models (LLMs), there has\nbeen a growing interest in evaluating their similarity to the human brain. One\napproach towards quantifying this similarity is by measuring how well a model\npredicts neural signals, also called ""brain score"". Internal representations\nfrom LLMs achieve state-of-the-art brain scores, leading to speculation that\nthey share computational principles with human language processing. This\ninference is only valid if the subset of neural activity predicted by LLMs\nreflects core elements of language processing. Here, we question this\nassumption by analyzing three neural datasets used in an impactful study on\nLLM-to-brain mappings, with a particular focus on an fMRI dataset where\nparticipants read short passages. We first find that when using shuffled\ntrain-test splits, as done in previous studies with these datasets, a trivial\nfeature that encodes temporal autocorrelation not only outperforms LLMs but\nalso accounts for the majority of neural variance that LLMs explain. We\ntherefore use contiguous splits moving forward. Second, we explain the\nsurprisingly high brain scores of untrained LLMs by showing they do not account\nfor additional neural variance beyond two simple features: sentence length and\nsentence position. This undermines evidence used to claim that the transformer\narchitecture biases computations to be more brain-like. Third, we find that\nbrain scores of trained LLMs on this dataset can largely be explained by\nsentence length, position, and pronoun-dereferenced static word embeddings; a\nsmall, additional amount is explained by sense-specific embeddings and\ncontextual representations of sentence structure. We conclude that\nover-reliance on brain scores can lead to over-interpretations of similarity\nbetween LLMs and brains, and emphasize the importance of deconstructing what\nLLMs are mapping to in neural signals.', ""Large Language Models (LLMs) have been shown to be effective models of the\nhuman language system, with some models predicting most explainable variance of\nbrain activity in current datasets. Even in untrained models, the\nrepresentations induced by architectural priors can exhibit reasonable\nalignment to brain data. In this work, we investigate the key architectural\ncomponents driving the surprising alignment of untrained models. To estimate\nLLM-to-brain similarity, we first select language-selective units within an\nLLM, similar to how neuroscientists identify the language network in the human\nbrain. We then benchmark the brain alignment of these LLM units across five\ndifferent brain recording datasets. By isolating critical components of the\nTransformer architecture, we identify tokenization strategy and multihead\nattention as the two major components driving brain alignment. A simple form of\nrecurrence further improves alignment. We further demonstrate this quantitative\nbrain alignment of our model by reproducing landmark studies in the language\nneuroscience field, showing that localized model units -- just like language\nvoxels measured empirically in the human brain -- discriminate more reliably\nbetween lexical than syntactic differences, and exhibit similar response\nprofiles under the same experimental conditions. Finally, we demonstrate the\nutility of our model's representations for language modeling, achieving\nimproved sample and parameter efficiency over comparable architectures. Our\nmodel's estimates of surprisal sets a new state-of-the-art in the behavioral\nalignment to human reading times. Taken together, we propose a highly brain-\nand behaviorally-aligned model that conceptualizes the human language system as\nan untrained shallow feature encoder, with structural priors, combined with a\ntrained decoder to achieve efficient and performant language processing."", 'The human brain has long inspired the pursuit of artificial intelligence\n(AI). Recently, neuroimaging studies provide compelling evidence of alignment\nbetween the computational representation of artificial neural networks (ANNs)\nand the neural responses of the human brain to stimuli, suggesting that ANNs\nmay employ brain-like information processing strategies. While such alignment\nhas been observed across sensory modalities--visual, auditory, and\nlinguistic--much of the focus has been on the behaviors of artificial neurons\n(ANs) at the population level, leaving the functional organization of\nindividual ANs that facilitates such brain-like processes largely unexplored.\nIn this study, we bridge this gap by directly coupling sub-groups of artificial\nneurons with functional brain networks (FBNs), the foundational organizational\nstructure of the human brain. Specifically, we extract representative patterns\nfrom temporal responses of ANs in large language models (LLMs), and use them as\nfixed regressors to construct voxel-wise encoding models to predict brain\nactivity recorded by functional magnetic resonance imaging (fMRI). This\nframework links the AN sub-groups to FBNs, enabling the delineation of\nbrain-like functional organization within LLMs. Our findings reveal that LLMs\n(BERT and Llama 1-3) exhibit brain-like functional architecture, with\nsub-groups of artificial neurons mirroring the organizational patterns of\nwell-established FBNs. Notably, the brain-like functional organization of LLMs\nevolves with the increased sophistication and capability, achieving an improved\nbalance between the diversity of computational behaviors and the consistency of\nfunctional specializations. This research represents the first exploration of\nbrain-like functional organization within LLMs, offering novel insights to\ninform the development of artificial general intelligence (AGI) with human\nbrain principles.']"
126,24,126_chart_charts_chartqa_mllms,"['chart', 'charts', 'chartqa', 'mllms', 'visualizations', 'plots', 'color', 'misleading', 'visual', 'ui']","[""Misleading chart visualizations, which intentionally manipulate data\nrepresentations to support specific claims, can distort perceptions and lead to\nincorrect conclusions. Despite decades of research, misleading visualizations\nremain a widespread and pressing issue. Recent advances in multimodal large\nlanguage models (MLLMs) have demonstrated strong chart comprehension\ncapabilities, yet no existing work has systematically evaluated their ability\nto detect and interpret misleading charts. This paper introduces the Misleading\nChart Question Answering (Misleading ChartQA) Benchmark, a large-scale\nmultimodal dataset designed to assess MLLMs in identifying and reasoning about\nmisleading charts. It contains over 3,000 curated examples, covering 21 types\nof misleaders and 10 chart types. Each example includes standardized chart\ncode, CSV data, and multiple-choice questions with labeled explanations,\nvalidated through multi-round MLLM checks and exhausted expert human review. We\nbenchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitations\nin identifying visually deceptive practices. We also propose a novel pipeline\nthat detects and localizes misleaders, enhancing MLLMs' accuracy in misleading\nchart interpretation. Our work establishes a foundation for advancing\nMLLM-driven misleading chart comprehension. We publicly release the sample\ndataset to support further research in this critical area."", ""Recent studies customizing Multimodal Large Language Models (MLLMs) for\ndomain-specific tasks have yielded promising results, especially in the field\nof scientific chart comprehension. These studies generally utilize visual\ninstruction tuning with specialized datasets to enhance question and answer\n(QA) accuracy within the chart domain. However, they often neglect the\nfundamental discrepancy between natural image-caption pre-training data and\ndigital chart image-QA data, particularly in the models' capacity to extract\nunderlying numeric values from charts. This paper tackles this oversight by\nexploring the training processes necessary to improve MLLMs' comprehension of\ncharts. We present three key findings: (1) Incorporating raw data values in\nalignment pre-training markedly improves comprehension of chart data. (2)\nReplacing images with their textual representation randomly during end-to-end\nfine-tuning transfer the language reasoning capability to chart interpretation\nskills. (3) Requiring the model to first extract the underlying chart data and\nthen answer the question in the fine-tuning can further improve the accuracy.\nConsequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart\ncomprehension. CHOPINLLM effectively interprets various types of charts,\nincluding unannotated ones, while maintaining robust reasoning abilities.\nFurthermore, we establish a new benchmark to evaluate MLLMs' understanding of\ndifferent chart types across various comprehension levels. Experimental results\nshow that CHOPINLLM exhibits strong performance in understanding both annotated\nand unannotated charts across a wide range of types."", ""Natural language is a powerful complementary modality of communication for\ndata visualizations, such as bar and line charts. To facilitate chart-based\nreasoning using natural language, various downstream tasks have been introduced\nrecently such as chart question answering, chart summarization, and\nfact-checking with charts. These tasks pose a unique challenge, demanding both\nvision-language reasoning and a nuanced understanding of chart data tables,\nvisual encodings, and natural language prompts. Despite the recent success of\nLarge Language Models (LLMs) across diverse NLP tasks, their abilities and\nlimitations in the realm of data visualization remain under-explored, possibly\ndue to their lack of multi-modal capabilities. To bridge the gap, this paper\npresents the first comprehensive evaluation of the recently developed large\nvision language models (LVLMs) for chart understanding and reasoning tasks. Our\nevaluation includes a comprehensive assessment of LVLMs, including GPT-4V and\nGemini, across four major chart reasoning tasks. Furthermore, we perform a\nqualitative evaluation of LVLMs' performance on a diverse range of charts,\naiming to provide a thorough analysis of their strengths and weaknesses. Our\nfindings reveal that LVLMs demonstrate impressive abilities in generating\nfluent texts covering high-level data insights while also encountering common\nproblems like hallucinations, factual errors, and data bias. We highlight the\nkey strengths and limitations of chart comprehension tasks, offering insights\nfor future research.""]"
127,23,127_calling_function_ml_functioncalling,"['calling', 'function', 'ml', 'functioncalling', 'gtfs', 'tasks', 'program', 'programs', 'toolcoder', 'transitgpt']","[""Large language models have demonstrated impressive value in performing as\nautonomous agents when equipped with external tools and API calls. Nonetheless,\neffectively harnessing their potential for executing complex tasks crucially\nrelies on enhancements in their function calling capabilities. This paper\nidentifies a critical gap in existing function calling models, where\nperformance varies significantly across benchmarks, often due to being misled\nby specific naming conventions. To address such an issue, we introduce Hammer,\na novel family of foundation models specifically engineered for on-device\nfunction calling. Hammer employs an augmented dataset that enhances models'\nsensitivity to irrelevant functions and incorporates function masking\ntechniques to minimize misleading. Our empirical evaluations reveal that Hammer\nnot only outperforms larger models but also demonstrates robust generalization\nacross diverse benchmarks, achieving sota results. Our open source\ncontributions include a specialized dataset for irrelevance detection, a tuning\nframework for enhanced generalization, and the Hammer models, establishing a\nnew standard for function calling performance."", 'Large language models (LLMs) have recently shown tremendous promise in\nserving as the backbone to agentic systems, as demonstrated by their\nperformance in multi-faceted, challenging benchmarks like SWE-Bench and\nAgent-Bench. However, to realize the true potential of LLMs as autonomous\nagents, they must learn to identify, call, and interact with external tools and\napplication program interfaces (APIs) to complete complex tasks. These tasks\ntogether are termed function calling. Endowing LLMs with function calling\nabilities leads to a myriad of advantages, such as access to current and\ndomain-specific information in databases and knowledge sources, and the ability\nto outsource tasks that can be reliably performed by tools, e.g., a Python\ninterpreter or calculator. While there has been significant progress in\nfunction calling with LLMs, there is still a dearth of open models that perform\non par with proprietary LLMs like GPT, Claude, and Gemini. Therefore, in this\nwork, we introduce the GRANITE-20B-FUNCTIONCALLING model under an Apache 2.0\nlicense. The model is trained using a multi-task training approach on seven\nfundamental tasks encompassed in function calling, those being Nested Function\nCalling, Function Chaining, Parallel Functions, Function Name Detection,\nParameter-Value Pair Detection, Next-Best Function, and Response Generation. We\npresent a comprehensive evaluation on multiple out-of-domain datasets comparing\nGRANITE-20B-FUNCTIONCALLING to more than 15 other best proprietary and open\nmodels. GRANITE-20B-FUNCTIONCALLING provides the best performance among all\nopen models on the Berkeley Function Calling Leaderboard and fourth overall. As\na result of the diverse tasks and datasets used for training our model, we show\nthat GRANITE-20B-FUNCTIONCALLING has better generalizability on multiple tasks\nin seven different evaluation datasets.', 'The advanced function-calling capabilities of foundation models open up new\npossibilities for deploying agents to perform complex API tasks. However,\nmanaging large amounts of data and interacting with numerous APIs makes\nfunction calling hardware-intensive and costly, especially on edge devices.\nCurrent Large Language Models (LLMs) struggle with function calling at the edge\nbecause they cannot handle complex inputs or manage multiple tools effectively.\nThis results in low task-completion accuracy, increased delays, and higher\npower consumption. In this work, we introduce Less-is-More, a novel\nfine-tuning-free function-calling scheme for dynamic tool selection. Our\napproach is based on the key insight that selectively reducing the number of\ntools available to LLMs significantly improves their function-calling\nperformance, execution time, and power efficiency on edge devices. Experimental\nresults with state-of-the-art LLMs on edge hardware show agentic success rate\nimprovements, with execution time reduced by up to 70% and power consumption by\nup to 40%.']"
128,23,128_re_relation_extraction_docre,"['re', 'relation', 'extraction', 'docre', 'relations', 'umr', 'entity', 'entities', 'tacred', 'icl']","['Information Extraction (IE) is a transformative process that converts\nunstructured text data into a structured format by employing entity and\nrelation extraction (RE) methodologies. The identification of the relation\nbetween a pair of entities plays a crucial role within this framework. Despite\nthe existence of various techniques for relation extraction, their efficacy\nheavily relies on access to labeled data and substantial computational\nresources. In addressing these challenges, Large Language Models (LLMs) emerge\nas promising solutions; however, they might return hallucinating responses due\nto their own training data. To overcome these limitations, Retrieved-Augmented\nGeneration-based Relation Extraction (RAG4RE) in this work is proposed,\noffering a pathway to enhance the performance of relation extraction tasks.\n  This work evaluated the effectiveness of our RAG4RE approach utilizing\ndifferent LLMs. Through the utilization of established benchmarks, such as\nTACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to\ncomprehensively evaluate the efficacy of our RAG4RE approach. In particularly,\nwe leverage prominent LLMs including Flan T5, Llama2, and Mistral in our\ninvestigation. The results of our study demonstrate that our RAG4RE approach\nsurpasses performance of traditional RE approaches based solely on LLMs,\nparticularly evident in the TACRED dataset and its variations. Furthermore, our\napproach exhibits remarkable performance compared to previous RE methodologies\nacross both TACRED and TACREV datasets, underscoring its efficacy and potential\nfor advancing RE tasks in natural language processing.', ""Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE)\nthat utilizes limited training instances, appeals to more researchers in\nNatural Language Processing (NLP) due to its capability to extract textual\ninformation in extremely low-resource scenarios. The primary methodologies\nemployed for FSRE have been fine-tuning or prompt tuning techniques based on\nPre-trained Language Models (PLMs). Recently, the emergence of Large Language\nModels (LLMs) has prompted numerous researchers to explore FSRE through\nIn-Context Learning (ICL). However, there are substantial limitations\nassociated with methods based on either traditional RE models or LLMs.\nTraditional RE models are hampered by a lack of necessary prior knowledge,\nwhile LLMs fall short in their task-specific capabilities for RE. To address\nthese shortcomings, we propose a Dual-System Augmented Relation Extractor\n(DSARE), which synergistically combines traditional RE models with LLMs.\nSpecifically, DSARE innovatively injects the prior knowledge of LLMs into\ntraditional RE models, and conversely enhances LLMs' task-specific aptitude for\nRE through relation extraction augmentation. Moreover, an Integrated Prediction\nmodule is employed to jointly consider these two respective predictions and\nderive the final results. Extensive experiments demonstrate the efficacy of our\nproposed method."", 'Relation extraction (RE) aims to identify relations between entities\nmentioned in texts. Although large language models (LLMs) have demonstrated\nimpressive in-context learning (ICL) abilities in various tasks, they still\nsuffer from poor performances compared to most supervised fine-tuned RE\nmethods. Utilizing ICL for RE with LLMs encounters two challenges: (1)\nretrieving good demonstrations from training examples, and (2) enabling LLMs\nexhibit strong ICL abilities in RE. On the one hand, retrieving good\ndemonstrations is a non-trivial process in RE, which easily results in low\nrelevance regarding entities and relations. On the other hand, ICL with an LLM\nachieves poor performance in RE while RE is different from language modeling in\nnature or the LLM is not large enough. In this work, we propose a novel\nrecall-retrieve-reason RE framework that synergizes LLMs with retrieval corpora\n(training examples) to enable relevant retrieving and reliable in-context\nreasoning. Specifically, we distill the consistently ontological knowledge from\ntraining datasets to let LLMs generate relevant entity pairs grounded by\nretrieval corpora as valid queries. These entity pairs are then used to\nretrieve relevant training examples from the retrieval corpora as\ndemonstrations for LLMs to conduct better ICL via instruction tuning. Extensive\nexperiments on different LLMs and RE datasets demonstrate that our method\ngenerates relevant and valid entity pairs and boosts ICL abilities of LLMs,\nachieving competitive or new state-of-the-art performance on sentence-level RE\ncompared to previous supervised fine-tuning methods and ICL-based methods.']"
129,23,129_ensemble_ensembling_selfmoa_strengths,"['ensemble', 'ensembling', 'selfmoa', 'strengths', 'outputs', 'kshot', 'complementary', 'different', 'vocabulary', 'model']","['LLM Ensemble -- which involves the comprehensive use of multiple large\nlanguage models (LLMs), each aimed at handling user queries during downstream\ninference, to benefit from their individual strengths -- has gained substantial\nattention recently. The widespread availability of LLMs, coupled with their\nvarying strengths and out-of-the-box usability, has profoundly advanced the\nfield of LLM Ensemble. This paper presents the first systematic review of\nrecent developments in LLM Ensemble. First, we introduce our taxonomy of LLM\nEnsemble and discuss several related research problems. Then, we provide a more\nin-depth classification of the methods under the broad categories of\n""ensemble-before-inference, ensemble-during-inference,\nensemble-after-inference\'\', and review all relevant methods. Finally, we\nintroduce related benchmarks and applications, summarize existing studies, and\nsuggest several future research directions. A curated list of papers on LLM\nEnsemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.', 'Ensembling different large language models (LLMs) to unleash their\ncomplementary potential and harness their individual strengths is highly\nvaluable. Nevertheless, vocabulary discrepancies among various LLMs have\nconstrained previous studies to either selecting or blending completely\ngenerated outputs. This limitation hinders the dynamic correction and\nenhancement of outputs during the generation process, resulting in a limited\ncapacity for effective ensemble. To address this issue, we propose a novel\nmethod to Ensemble LLMs via Vocabulary Alignment (EVA). EVA bridges the lexical\ngap among various LLMs, enabling meticulous ensemble at each generation step.\nSpecifically, we first learn mappings between the vocabularies of different\nLLMs with the assistance of overlapping tokens. Subsequently, these mappings\nare employed to project output distributions of LLMs into a unified space,\nfacilitating a fine-grained ensemble. Finally, we design a filtering strategy\nto exclude models that generate unfaithful tokens. Experimental results on\ncommonsense reasoning, arithmetic reasoning, machine translation, and\ndata-to-text generation tasks demonstrate the superiority of our approach\ncompared with individual LLMs and previous ensemble methods conducted on\ncomplete outputs. Further analyses confirm that our approach can leverage\nknowledge from different language models and yield consistent improvement.', 'Ensembling various LLMs to unlock their complementary potential and leverage\ntheir individual strengths is highly valuable. Previous studies typically focus\non two main paradigms: sample-level and token-level ensembles. Sample-level\nensemble methods either select or blend fully generated outputs, which hinders\ndynamic correction and enhancement of outputs during the generation process. On\nthe other hand, token-level ensemble methods enable real-time correction\nthrough fine-grained ensemble at each generation step. However, the information\ncarried by an individual token is quite limited, leading to suboptimal\ndecisions at each step. To address these issues, we propose SweetSpan, a\nspan-level ensemble method that effectively balances the need for real-time\nadjustments and the information required for accurate ensemble decisions. Our\napproach involves two key steps: First, we have each candidate model\nindependently generate candidate spans based on the shared prefix. Second, we\ncalculate perplexity scores to facilitate mutual evaluation among the candidate\nmodels and achieve robust span selection by filtering out unfaithful scores. To\ncomprehensively evaluate ensemble methods, we propose a new challenging setting\n(ensemble models with significant performance gaps) in addition to the standard\nsetting (ensemble the best-performing models) to assess the performance of\nmodel ensembles in more realistic scenarios. Experimental results in both\nstandard and challenging settings across various language generation tasks\ndemonstrate the effectiveness, robustness, and versatility of our approach\ncompared with previous ensemble methods.']"
130,23,130_arabic_egyptian_dialectal_da,"['arabic', 'egyptian', 'dialectal', 'da', 'fanar', 'saudi', 'arab', 'gec', 'english', 'saudibert']","['Motivated by the widespread increase in the phenomenon of code-switching\nbetween Egyptian Arabic and English in recent times, this paper explores the\nintricacies of machine translation (MT) and automatic speech recognition (ASR)\nsystems, focusing on translating code-switched Egyptian Arabic-English to\neither English or Egyptian Arabic. Our goal is to present the methodologies\nemployed in developing these systems, utilizing large language models such as\nLLama and Gemma. In the field of ASR, we explore the utilization of the Whisper\nmodel for code-switched Egyptian Arabic recognition, detailing our experimental\nprocedures including data preprocessing and training techniques. Through the\nimplementation of a consecutive speech-to-text translation system that\nintegrates ASR with MT, we aim to overcome challenges posed by limited\nresources and the unique characteristics of the Egyptian Arabic dialect.\nEvaluation against established metrics showcases promising results, with our\nmethodologies yielding a significant improvement of $56\\%$ in English\ntranslation over the state-of-the-art and $9.3\\%$ in Arabic translation. Since\ncode-switching is deeply inherent in spoken languages, it is crucial that ASR\nsystems can effectively handle this phenomenon. This capability is crucial for\nenabling seamless interaction in various domains, including business\nnegotiations, cultural exchanges, and academic discourse. Our models and code\nare available as open-source resources. Code:\n\\url{http://github.com/ahmedheakl/arazn-llm}}, Models:\n\\url{http://huggingface.co/collections/ahmedheakl/arazn-llm-662ceaf12777656607b9524e}.', 'This survey offers a comprehensive overview of Large Language Models (LLMs)\ndesigned for Arabic language and its dialects. It covers key architectures,\nincluding encoder-only, decoder-only, and encoder-decoder models, along with\nthe datasets used for pre-training, spanning Classical Arabic, Modern Standard\nArabic, and Dialectal Arabic. The study also explores monolingual, bilingual,\nand multilingual LLMs, analyzing their architectures and performance across\ndownstream tasks, such as sentiment analysis, named entity recognition, and\nquestion answering. Furthermore, it assesses the openness of Arabic LLMs based\non factors, such as source code availability, training data, model weights, and\ndocumentation. The survey highlights the need for more diverse dialectal\ndatasets and attributes the importance of openness for research reproducibility\nand transparency. It concludes by identifying key challenges and opportunities\nfor future research and stressing the need for more inclusive and\nrepresentative models.', 'In recent years, Large Language Models have revolutionized the field of\nnatural language processing, showcasing an impressive rise predominantly in\nEnglish-centric domains. These advancements have set a global benchmark,\ninspiring significant efforts toward developing Arabic LLMs capable of\nunderstanding and generating the Arabic language with remarkable accuracy.\nDespite these advancements, a critical challenge persists: the potential bias\nin Arabic LLMs, primarily attributed to their reliance on datasets comprising\nEnglish data that has been translated into Arabic. This reliance not only\ncompromises the authenticity of the generated content but also reflects a\nbroader issue -the scarcity of original quality Arabic linguistic data. This\nstudy aims to address the data scarcity in the Arab world and to encourage the\ndevelopment of Arabic Language Models that are true to both the linguistic and\nnuances of the region. We undertook a large-scale data mining project,\nextracting a substantial volume of text from the Common Crawl WET files,\nspecifically targeting Arabic content. The extracted data underwent a rigorous\ncleaning and deduplication process, using innovative techniques to ensure the\nintegrity and uniqueness of the dataset. The result is the 101 Billion Arabic\nWords Dataset, the largest Arabic dataset available to date, which can\nsignificantly contribute to the development of authentic Arabic LLMs. This\nstudy not only highlights the potential for creating linguistically and\nculturally accurate Arabic LLMs but also sets a precedent for future research\nin enhancing the authenticity of Arabic language models.']"
131,23,131_tom_mind_theory_whether,"['tom', 'mind', 'theory', 'whether', 'mental', 'cognitive', 'state', 'metacognitive', 'cognition', 'about']","[""In recent years, evaluating the Theory of Mind (ToM) capabilities of large\nlanguage models (LLMs) has received significant attention within the research\ncommunity. As the field rapidly evolves, navigating the diverse approaches and\nmethodologies has become increasingly complex. This systematic review\nsynthesizes current efforts to assess LLMs' ability to perform ToM tasks, an\nessential aspect of human cognition involving the attribution of mental states\nto oneself and others. Despite notable advancements, the proficiency of LLMs in\nToM remains a contentious issue. By categorizing benchmarks and tasks through a\ntaxonomy rooted in cognitive science, this review critically examines\nevaluation techniques, prompting strategies, and the inherent limitations of\nLLMs in replicating human-like mental state reasoning. A recurring theme in the\nliterature reveals that while LLMs demonstrate emerging competence in ToM\ntasks, significant gaps persist in their emulation of human cognitive\nabilities."", ""Theory of Mind (ToM) can be used to assess the capabilities of Large Language\nModels (LLMs) in complex scenarios where social reasoning is required. While\nthe research community has proposed many ToM benchmarks, their hardness varies\ngreatly, and their complexity is not well defined. This work proposes a\nframework inspired by cognitive load theory to measure the complexity of ToM\ntasks. We quantify a problem's complexity as the number of states necessary to\nsolve it correctly. Our complexity measure also accounts for spurious states of\na ToM problem designed to make it apparently harder. We use our method to\nassess the complexity of five widely adopted ToM benchmarks. On top of this\nframework, we design a prompting technique that augments the information\navailable to a model with a description of how the environment changes with the\nagents' interactions. We name this technique Discrete World Models (DWM) and\nshow how it elicits superior performance on ToM tasks."", 'The question of whether large language models (LLMs) possess Theory of Mind\n(ToM) -- often defined as the ability to reason about others\' mental states --\nhas sparked significant scientific and public interest. However, the evidence\nas to whether LLMs possess ToM is mixed, and the recent growth in evaluations\nhas not resulted in a convergence. Here, we take inspiration from cognitive\nscience to re-evaluate the state of ToM evaluation in LLMs. We argue that a\nmajor reason for the disagreement on whether LLMs have ToM is a lack of clarity\non whether models should be expected to match human behaviors, or the\ncomputations underlying those behaviors. We also highlight ways in which\ncurrent evaluations may be deviating from ""pure"" measurements of ToM abilities,\nwhich also contributes to the confusion. We conclude by discussing several\ndirections for future research, including the relationship between ToM and\npragmatic communication, which could advance our understanding of artificial\nsystems as well as human cognition.']"
132,22,132_games_game_agents_respact,"['games', 'game', 'agents', 'respact', 'poker', 'reasoning', 'strategic', 'pokerbench', 'gamebot', 'gomoku']","[""We introduce a novel and extensible benchmark for large language models\n(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.\nThe open-source game simulation code, available on GitHub, allows LLMs to\ncompete and generates detailed data files in JSON, CSV, TXT, and PNG formats\nfor leaderboard rankings and further analysis. We present the results of games\namong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by\nAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and\nGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions of\nresults from other LLMs. In total, we simulated 2,310 matches (5 sessions for\neach pair among 7 LLMs and a random player) across three types of games, using\nthree distinct prompt types: list, illustration, and image. The results\nrevealed significant variations in LLM performance across different games and\nprompt types, with analysis covering win and disqualification rates, missed\nopportunity analysis, and invalid move analysis. The details of the leaderboard\nand result matrix data are available as open-access data on GitHub. This study\nenhances our understanding of LLMs' capabilities in playing games they were not\nspecifically trained for, helping to assess their rule comprehension and\nstrategic thinking. On the path to Artificial General Intelligence (AGI), this\nstudy lays the groundwork for future exploration into their utility in complex\ndecision-making scenarios, illuminating their strategic thinking abilities and\noffering directions for further inquiry into the limits of LLMs within\ngame-based frameworks."", ""Interactive fiction games have emerged as an important application to improve\nthe generalization capabilities of language-based reinforcement learning (RL)\nagents. Existing environments for interactive fiction games are domain-specific\nor time-consuming to generate and do not train the RL agents to master a\nspecific set of skills. In this work, we introduce an interactive environment\nfor self-supervised RL, STARLING, for text-based games that bootstraps the\ntext-based RL agents with automatically generated games (based on the seed set\nof game ideas) to boost the performance and generalization capabilities to\nreach a goal of the target environment. These games let the agent hone their\nskills on a predefined set of tasks. We create and test an environment with 100\ngames, generated using this automated framework that uses large language models\n(GPT-3) and an interactive fiction game engine (based on Inform7) to provide\nthe user with the ability to generate more games under minimal human\nsupervision. Experimental results based on both the human participants and\nbaseline text-based RL agents reveal that current state-of-the-art text-based\nRL agents cannot use previously learned skills in new situations at the level\nhumans can. These results enforce STARLING's potential to serve as a sandbox\nenvironment for further research in self-supervised text-based RL."", ""The rapid advancement of large language models (LLMs) has accelerated their\napplication in reasoning, with strategic reasoning drawing increasing\nattention. To evaluate LLMs' strategic reasoning capabilities, game theory,\nwith its concise structure, has become a preferred approach. However, current\nresearch focuses on a limited selection of games, resulting in low coverage.\nClassic game scenarios risk data leakage, and existing benchmarks often lack\nextensibility, making them inadequate for evaluating state-of-the-art models.\nTo address these challenges, we propose TMGBench, a benchmark with\ncomprehensive game type coverage, novel scenarios, and flexible organization.\nSpecifically, we incorporate all 144 game types summarized by the\nRobinson-Goforth topology of 2x2 games, constructed as classic games. We also\nemploy synthetic data generation to create diverse, higher-quality scenarios\nthrough topic guidance and human inspection, referred to as story-based games.\nLastly, we provide a sustainable framework for increasingly powerful LLMs by\ntreating these games as atomic units and organizing them into more complex\nforms via sequential, parallel, and nested structures. Our comprehensive\nevaluation of mainstream LLMs covers tests on rational reasoning, robustness,\nTheory-of-Mind (ToM), and reasoning in complex forms. Results reveal flaws in\naccuracy, consistency, and varying mastery of ToM. Additionally, o1-mini,\nOpenAI's latest reasoning model, achieved accuracy rates of 66.6%, 60.0%, and\n70.0% on sequential, parallel, and nested games, highlighting TMGBench's\nchallenges.""]"
133,22,133_log_logs_parsing_parsers,"['log', 'logs', 'parsing', 'parsers', 'templates', 'messages', 'anomaly', 'logbased', 'analysis', 'lad']","[""Logs are a first-hand source of information for software maintenance and\nfailure diagnosis. Log parsing, which converts semi-structured log messages\ninto structured templates, is a prerequisite for automated log analysis tasks\nsuch as anomaly detection, troubleshooting, and root cause analysis. However,\nexisting log parsers fail in real-world systems for three main reasons. First,\ntraditional heuristics-based parsers require handcrafted features and domain\nknowledge, which are difficult to generalize at scale. Second, existing large\nlanguage model-based parsers rely on periodic offline processing, limiting\ntheir effectiveness in real-time use cases. Third, existing online parsing\nalgorithms are susceptible to log drift, where slight log changes create false\npositives that drown out real anomalies. To address these challenges, we\npropose HELP, a Hierarchical Embeddings-based Log Parser. HELP is the first\nonline semantic-based parser to leverage LLMs for performant and cost-effective\nlog parsing. We achieve this through a novel hierarchical embeddings module,\nwhich fine-tunes a text embedding model to cluster logs before parsing,\nreducing querying costs by multiple orders of magnitude. To combat log drift,\nwe also develop an iterative rebalancing module, which periodically updates\nexisting log groupings. We evaluate HELP extensively on 14 public large-scale\ndatasets, showing that HELP achieves significantly higher F1-weighted grouping\nand parsing accuracy than current state-of-the-art online log parsers. We also\nimplement HELP into Iudex's production observability platform, confirming\nHELP's practicality in a production environment. Our results show that HELP is\neffective and efficient for high-throughput real-world log parsing."", 'Log parsing transforms log messages into structured formats, serving as a\ncrucial step for log analysis. Despite a variety of log parsers that have been\nproposed, their performance on evolving log data remains unsatisfactory due to\nreliance on human-crafted rules or learning-based models with limited training\ndata. The recent emergence of large language models (LLMs) has demonstrated\nstrong abilities in understanding natural language and code, making it\npromising to apply LLMs for log parsing. Consequently, several studies have\nproposed LLM-based log parsers. However, LLMs may produce inaccurate templates,\nand existing LLM-based log parsers directly use the template generated by the\nLLM as the parsing result, hindering the accuracy of log parsing. Furthermore,\nthese log parsers depend heavily on historical log data as demonstrations,\nwhich poses challenges in maintaining accuracy when dealing with scarce\nhistorical log data or evolving log data. To address these challenges, we\npropose AdaParser, an effective and adaptive log parsing framework using LLMs\nwith self-generated in-context learning (SG-ICL) and self-correction. To\nfacilitate accurate log parsing, AdaParser incorporates a novel component, a\ntemplate corrector, which utilizes the LLM to correct potential parsing errors\nin the templates it generates. In addition, AdaParser maintains a dynamic\ncandidate set composed of previously generated templates as demonstrations to\nadapt evolving log data. Extensive experiments on public large-scale datasets\nindicate that AdaParser outperforms state-of-the-art methods across all\nmetrics, even in zero-shot scenarios. Moreover, when integrated with different\nLLMs, AdaParser consistently enhances the performance of the utilized LLMs by a\nlarge margin.', 'Log parsing, the process of converting raw log messages into structured\nformats, is an important initial step for automated analysis of logs of\nlarge-scale software systems. Traditional log parsers often rely on heuristics\nor handcrafted features, which may not generalize well across diverse log\nsources or require extensive model tuning. Recently, some log parsers have\nutilized powerful generative capabilities of large language models (LLMs).\nHowever, they heavily rely on demonstration examples, resulting in substantial\noverhead in LLM invocations. To address these issues, we propose LogBatcher, a\ncost-effective LLM-based log parser that requires no training process or\nlabeled data. To leverage latent characteristics of log data and reduce the\noverhead, we divide logs into several partitions through clustering. Then we\nperform a cache matching process to match logs with previously parsed log\ntemplates. Finally, we provide LLMs with better prompt context specialized for\nlog parsing by batching a group of logs from each partition. We have conducted\nexperiments on 16 public log datasets and the results show that LogBatcher is\neffective and efficient for log parsing.']"
134,22,134_policy_optimization_reinforcement_problems,"['policy', 'optimization', 'reinforcement', 'problems', 'rl', 'optllm', 'agents', 'qvalue', 'subgoal', 'learning']","[""The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods."", ""Large language model (LLM)-based agents have recently shown impressive\nprogress in a variety of domains, including open-ended conversation and\nmulti-step decision-making. However, applying these agents to social deduction\ngames such as Werewolf, which requires both strategic decision-making and\nfree-form language interaction, remains non-trivial. Traditional methods based\non Counterfactual Regret Minimization (CFR) or reinforcement learning (RL)\ntypically depend on a predefined action space, making them unsuitable for\nlanguage games with unconstrained text action space. Meanwhile, pure LLM-based\nagents often suffer from intrinsic biases and require prohibitively large\ndatasets for fine-tuning. We propose Latent Space Policy Optimization (LSPO),\nan iterative framework that addresses these challenges by first mapping\nfree-form text to a discrete latent space, where methods like CFR and RL can\nlearn strategic policy more effectively. We then translate the learned policy\nback into natural language dialogues, which are used to fine-tune an LLM via\nDirect Preference Optimization (DPO). By iteratively alternating between these\nstages, our LSPO agent progressively enhances both strategic reasoning and\nlanguage communication. Experiment results on the Werewolf game show that our\nmethod improves the agent's performance in each iteration and outperforms\nexisting Werewolf agents, underscoring its promise for free-form language\ndecision-making."", ""Behavior cloning has shown success in many sequential decision-making tasks\nby learning from expert demonstrations, yet they can be very sample inefficient\nand fail to generalize to unseen scenarios. One approach to these problems is\nto introduce general domain knowledge, such that the policy can focus on the\nessential features and may generalize to unseen states by applying that\nknowledge. Although this knowledge is easy to acquire from the experts, it is\nhard to be combined with learning from individual examples due to the lack of\nsemantic structure in neural networks and the time-consuming nature of feature\nengineering. To enable learning from both general knowledge and specific\ndemonstration trajectories, we use a large language model's coding capability\nto instantiate a policy structure based on expert domain knowledge expressed in\nnatural language and tune the parameters in the policy with demonstrations. We\nname this approach the Knowledge Informed Model (KIM) as the structure reflects\nthe semantics of expert knowledge. In our experiments with lunar lander and car\nracing tasks, our approach learns to solve the tasks with as few as 5\ndemonstrations and is robust to action noise, outperforming the baseline model\nwithout domain knowledge. This indicates that with the help of large language\nmodels, we can incorporate domain knowledge into the structure of the policy,\nincreasing sample efficiency for behavior cloning.""]"
135,21,135_disaster_media_social_crisis,"['disaster', 'media', 'social', 'crisis', 'vaping', 'disasters', 'pandemic', 'outbreak', 'tweets', 'covid19']","['Disasters can result in the deaths of many, making quick response times\nvital. Large Language Models (LLMs) have emerged as valuable in the field. LLMs\ncan be used to process vast amounts of textual information quickly providing\nsituational context during a disaster. However, the question remains whether\nLLMs should be used for advice and decision making in a disaster. To evaluate\nthe capabilities of LLMs in disaster response knowledge, we introduce a\nbenchmark: DisasterQA created from six online sources. The benchmark covers a\nwide range of disaster response topics. We evaluated five LLMs each with four\ndifferent prompting methods on our benchmark, measuring both accuracy and\nconfidence levels through Logprobs. The results indicate that LLMs require\nimprovement on disaster response knowledge. We hope that this benchmark pushes\nforth further development of LLMs in disaster response, ultimately enabling\nthese models to work alongside. emergency managers in disasters.', 'In recent years, social media has emerged as a primary channel for users to\npromptly share feedback and issues during disasters and emergencies, playing a\nkey role in crisis management. While significant progress has been made in\ncollecting and analyzing social media content, there remains a pressing need to\nenhance the automation, aggregation, and customization of this data to deliver\nactionable insights tailored to diverse stakeholders, including the press,\npolice, EMS, and firefighters. This effort is essential for improving the\ncoordination of activities such as relief efforts, resource distribution, and\nmedia communication. This paper presents a methodology that leverages the\ncapabilities of LLMs to enhance disaster response and management. Our approach\ncombines classification techniques with generative AI to bridge the gap between\nraw user feedback and stakeholder-specific reports. Social media posts shared\nduring catastrophic events are analyzed with a focus on user-reported issues,\nservice interruptions, and encountered challenges. We employ full-spectrum\nLLMs, using analytical models like BERT for precise, multi-dimensional\nclassification of content type, sentiment, emotion, geolocation, and topic.\nGenerative models such as ChatGPT are then used to produce human-readable,\ninformative reports tailored to distinct audiences, synthesizing insights\nderived from detailed classifications. We compare standard approaches, which\nanalyze posts directly using prompts in ChatGPT, to our advanced method, which\nincorporates multi-dimensional classification, sub-event selection, and\ntailored report generation. Our methodology demonstrates superior performance\nin both quantitative metrics, such as text coherence scores and latent\nrepresentations, and qualitative assessments by automated tools and field\nexperts, delivering precise insights for diverse disaster response\nstakeholders.', 'In the field of crisis/disaster informatics, social media is increasingly\nbeing used for improving situational awareness to inform response and relief\nefforts. Efficient and accurate text classification tools have been a focal\narea of investigation in crisis informatics. However, current methods mostly\nrely on single-label text classification models, which fails to capture\ndifferent insights embedded in dynamic and multifaceted disaster-related social\nmedia data. This study introduces a novel approach to disaster text\nclassification by enhancing a pre-trained Large Language Model (LLM) through\ninstruction fine-tuning targeted for multi-label classification of\ndisaster-related tweets. Our methodology involves creating a comprehensive\ninstruction dataset from disaster-related tweets, which is then used to\nfine-tune an open-source LLM, thereby embedding it with disaster-specific\nknowledge. This fine-tuned model can classify multiple aspects of\ndisaster-related information simultaneously, such as the type of event,\ninformativeness, and involvement of human aid, significantly improving the\nutility of social media data for situational awareness in disasters. The\nresults demonstrate that this approach enhances the categorization of critical\ninformation from social media posts, thereby facilitating a more effective\ndeployment for situational awareness during emergencies. This research paves\nthe way for more advanced, adaptable, and robust disaster management tools,\nleveraging the capabilities of LLMs to improve real-time situational awareness\nand response strategies in disaster scenarios.']"
136,21,136_embedding_embeddings_sentence_text,"['embedding', 'embeddings', 'sentence', 'text', 'pooling', 'multilingual', 'bidirectional', 'mteb', 'dense', 'geneol']","[""Recent advancements in large language models (LLMs) based embedding models\nhave established new state-of-the-art benchmarks for text embedding tasks,\nparticularly in dense vector-based retrieval. However, these models\npredominantly focus on English, leaving multilingual embedding capabilities\nlargely unexplored. To address this limitation, we present LUSIFER, a novel\nzero-shot approach that adapts LLM-based embedding models for multilingual\ntasks without requiring multilingual supervision. LUSIFER's architecture\ncombines a multilingual encoder, serving as a language-universal learner, with\nan LLM-based embedding model optimized for embedding-specific tasks. These\ncomponents are seamlessly integrated through a minimal set of trainable\nparameters that act as a connector, effectively transferring the multilingual\nencoder's language understanding capabilities to the specialized embedding\nmodel. Additionally, to comprehensively evaluate multilingual embedding\nperformance, we introduce a new benchmark encompassing 5 primary embedding\ntasks, 123 diverse datasets, and coverage across 14 languages. Extensive\nexperimental results demonstrate that LUSIFER significantly enhances the\nmultilingual performance across various embedding tasks, particularly for\nmedium and low-resource languages, without requiring explicit multilingual\ntraining data."", 'The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance.', 'Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications - such as semantic matching,\nclustering, and information retrieval - continue to rely on text embeddings for\ntheir efficiency and effectiveness. Therefore, integrating LLMs with text\nembeddings has become a major research focus in recent years. In this survey,\nwe categorize the interplay between LLMs and text embeddings into three\noverarching themes: (1) LLM-augmented text embedding, enhancing traditional\nembedding methods with LLMs; (2) LLMs as text embedders, adapting their innate\ncapabilities for high-quality embedding; and (3) Text embedding understanding\nwith LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing\nrecent works based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.']"
137,21,137_graph_graphs_hypergraph_structured,"['graph', 'graphs', 'hypergraph', 'structured', 'edges', 'problems', 'tasks', 'llms', 'codegraph', 'graphtoolinstruction']","['The adoption of Large Language Models (LLMs) is rapidly expanding across\nvarious tasks that involve inherent graphical structures. Graphs are integral\nto a wide range of applications, including motion planning for autonomous\nvehicles, social networks, scene understanding, and knowledge graphs. Many\nproblems, even those not initially perceived as graph-based, can be effectively\naddressed through graph theory. However, when applied to these tasks, LLMs\noften encounter challenges, such as hallucinations and mathematical\ninaccuracies. To overcome these limitations, we propose Graph-Grounded LLMs, a\nsystem that improves LLM performance on graph-related tasks by integrating a\ngraph library through function calls. By grounding LLMs in this manner, we\ndemonstrate significant reductions in hallucinations and improved mathematical\naccuracy in solving graph-based problems, as evidenced by the performance on\nthe NLGraph benchmark. Finally, we showcase a disaster rescue application where\nthe Graph-Grounded LLM acts as a decision-support system.', 'Large language models (LLMs) have been demonstrated to possess the\ncapabilities to understand fundamental graph properties and address various\ngraph reasoning tasks. Existing methods fine-tune LLMs to understand and\nexecute graph reasoning tasks by specially designed task instructions. However,\nthese Text-Instruction methods generally exhibit poor performance. Inspired by\ntool learning, researchers propose Tool-Instruction methods to solve various\ngraph problems by special tool calling (e.g., function, API and model),\nachieving significant improvements in graph reasoning tasks. Nevertheless,\ncurrent Tool-Instruction approaches focus on the tool information and ignore\nthe graph structure information, which leads to significantly inferior\nperformance on small-scale LLMs (less than 13B). To tackle this issue, we\npropose GraphTool-Instruction, an innovative Instruction-tuning approach that\ndecomposes the graph reasoning task into three distinct subtasks (i.e., graph\nextraction, tool name identification and tool parameter extraction), and design\nspecialized instructions for each subtask. Our GraphTool-Instruction can be\nused as a plug-and-play prompt for different LLMs without fine-tuning.\nMoreover, building on GraphTool-Instruction, we develop GTools, a dataset that\nincludes twenty graph reasoning tasks, and create a graph reasoning LLM called\nGraphForge based on Llama3-8B. We conduct extensive experiments on twenty graph\nreasoning tasks with different graph types (e.g., graph size or graph\ndirection), and we find that GraphTool-Instruction achieves SOTA compared to\nText-Instruction and Tool-Instruction methods. Fine-tuned on GTools, GraphForge\ngets further improvement of over 30% compared to the Tool-Instruction enhanced\nGPT-3.5-turbo, and it performs comparably to the high-cost GPT-4o. Our codes\nand data are available at\nhttps://anonymous.4open.science/r/GraphTool-Instruction.', 'The growing importance of textual and relational systems has driven interest\nin enhancing large language models (LLMs) for graph-structured data,\nparticularly Text-Attributed Graphs (TAGs), where samples are represented by\ntextual descriptions interconnected by edges. While research has largely\nfocused on developing specialized graph LLMs through task-specific instruction\ntuning, a comprehensive benchmark for evaluating LLMs solely through prompt\ndesign remains surprisingly absent. Without such a carefully crafted evaluation\nbenchmark, most if not all, tailored graph LLMs are compared against general\nLLMs using simplistic queries (e.g., zero-shot reasoning with LLaMA), which can\npotentially camouflage many advantages as well as unexpected predicaments of\nthem. To achieve more general evaluations and unveil the true potential of LLMs\nfor graph tasks, we introduce Graph In-context Learning (GraphICL) Benchmark, a\ncomprehensive benchmark comprising novel prompt templates designed to capture\ngraph structure and handle limited label knowledge. Our systematic evaluation\nshows that general-purpose LLMs equipped with our GraphICL outperform\nstate-of-the-art specialized graph LLMs and graph neural network models in\nresource-constrained settings and out-of-domain tasks. These findings highlight\nthe significant potential of prompt engineering to enhance LLM performance on\ngraph learning tasks without training and offer a strong baseline for advancing\nresearch in graph LLMs.']"
138,20,138_anomaly_anomalies_vad_detection,"['anomaly', 'anomalies', 'vad', 'detection', 'zsad', 'industrial', 'video', 'abnormal', 'iad', 'normal']","['Recent advancements in video anomaly understanding (VAU) have opened the door\nto groundbreaking applications in various fields, such as traffic monitoring\nand industrial automation. While the current benchmarks in VAU predominantly\nemphasize the detection and localization of anomalies. Here, we endeavor to\ndelve deeper into the practical aspects of VAU by addressing the essential\nquestions: ""what anomaly occurred?"", ""why did it happen?"", and ""how severe is\nthis abnormal event?"". In pursuit of these answers, we introduce a\ncomprehensive benchmark for Exploring the Causation of Video Anomalies (ECVA).\nOur benchmark is meticulously designed, with each video accompanied by detailed\nhuman annotations. Specifically, each instance of our ECVA involves three sets\nof human annotations to indicate ""what"", ""why"" and ""how"" of an anomaly,\nincluding 1) anomaly type, start and end times, and event descriptions, 2)\nnatural language explanations for the cause of an anomaly, and 3) free text\nreflecting the effect of the abnormality. Building upon this foundation, we\npropose a novel prompt-based methodology that serves as a baseline for tackling\nthe intricate challenges posed by ECVA. We utilize ""hard prompt"" to guide the\nmodel to focus on the critical parts related to video anomaly segments, and\n""soft prompt"" to establish temporal and spatial relationships within these\nanomaly segments. Furthermore, we propose AnomEval, a specialized evaluation\nmetric crafted to align closely with human judgment criteria for ECVA. This\nmetric leverages the unique features of the ECVA dataset to provide a more\ncomprehensive and reliable assessment of various video large language models.\nWe demonstrate the efficacy of our approach through rigorous experimental\nanalysis and delineate possible avenues for further investigation into the\ncomprehension of video anomaly causation.', 'Zero-shot anomaly detection (ZSAD) recognizes and localizes anomalies in\npreviously unseen objects by establishing feature mapping between textual\nprompts and inspection images, demonstrating excellent research value in\nflexible industrial manufacturing. However, existing ZSAD methods are limited\nby closed-world settings, struggling to unseen defects with predefined prompts.\nRecently, adapting Multimodal Large Language Models (MLLMs) for Industrial\nAnomaly Detection (IAD) presents a viable solution. Unlike fixed-prompt\nmethods, MLLMs exhibit a generative paradigm with open-ended text\ninterpretation, enabling more adaptive anomaly analysis. However, this adaption\nfaces inherent challenges as anomalies often manifest in fine-grained regions\nand exhibit minimal visual discrepancies from normal samples. To address these\nchallenges, we propose a novel framework VMAD (Visual-enhanced MLLM Anomaly\nDetection) that enhances MLLM with visual-based IAD knowledge and fine-grained\nperception, simultaneously providing precise detection and comprehensive\nanalysis of anomalies. Specifically, we design a Defect-Sensitive Structure\nLearning scheme that transfers patch-similarities cues from visual branch to\nour MLLM for improved anomaly discrimination. Besides, we introduce a novel\nvisual projector, Locality-enhanced Token Compression, which mines multi-level\nfeatures in local contexts to enhance fine-grained detection. Furthermore, we\nintroduce the Real Industrial Anomaly Detection (RIAD), a comprehensive IAD\ndataset with detailed anomaly descriptions and analyses, offering a valuable\nresource for MLLM-based IAD development. Extensive experiments on zero-shot\nbenchmarks, including MVTec-AD, Visa, WFDD, and RIAD datasets, demonstrate our\nsuperior performance over state-of-the-art methods. The code and dataset will\nbe available soon.', 'How can we enable models to comprehend video anomalies occurring over varying\ntemporal scales and contexts? Traditional Video Anomaly Understanding (VAU)\nmethods focus on frame-level anomaly prediction, often missing the\ninterpretability of complex and diverse real-world anomalies. Recent multimodal\napproaches leverage visual and textual data but lack hierarchical annotations\nthat capture both short-term and long-term anomalies. To address this\nchallenge, we introduce HIVAU-70k, a large-scale benchmark for hierarchical\nvideo anomaly understanding across any granularity. We develop a semi-automated\nannotation engine that efficiently scales high-quality annotations by combining\nmanual video segmentation with recursive free-text annotation using large\nlanguage models (LLMs). This results in over 70,000 multi-granular annotations\norganized at clip-level, event-level, and video-level segments. For efficient\nanomaly detection in long videos, we propose the Anomaly-focused Temporal\nSampler (ATS). ATS integrates an anomaly scorer with a density-aware sampler to\nadaptively select frames based on anomaly scores, ensuring that the multimodal\nLLM concentrates on anomaly-rich regions, which significantly enhances both\nefficiency and accuracy. Extensive experiments demonstrate that our\nhierarchical instruction data markedly improves anomaly comprehension. The\nintegrated ATS and visual-language model outperform traditional methods in\nprocessing long videos. Our benchmark and model are publicly available at\nhttps://github.com/pipixin321/HolmesVAU.']"
139,20,139_personalization_personalized_user_profiles,"['personalization', 'personalized', 'user', 'profiles', 'profile', 'users', 'longtext', 'userspecific', 'profiling', 'personal']","[""Personalization plays a critical role in numerous language tasks and\napplications, since users with the same requirements may prefer diverse outputs\nbased on their individual interests. This has led to the development of various\npersonalized approaches aimed at adapting large language models (LLMs) to\ngenerate customized outputs aligned with user preferences. Some of them involve\nfine-tuning a unique personalized LLM for each user, which is too expensive for\nwidespread application. Alternative approaches introduce personalization\ninformation in a plug-and-play manner by retrieving the user's relevant\nhistorical texts as demonstrations. However, this retrieval-based strategy may\nbreak the continuity of the user history and fail to capture the user's overall\nstyles and patterns, hence leading to sub-optimal performance. To address these\nchallenges, we propose a novel personalized LLM model, \\ours{}. It constructs a\nuser-specific embedding for each individual by modeling all her historical\ncontexts through a lightweight plug-in user embedder module. By attaching this\nembedding to the task input, LLMs can better understand and capture user habits\nand preferences, thereby producing more personalized outputs without tuning\ntheir own parameters. Extensive experiments on various tasks in the language\nmodel personalization (LaMP) benchmark demonstrate that the proposed model\nsignificantly outperforms existing personalized LLM approaches."", 'Personalization of Large Language Models (LLMs) has recently become\nincreasingly important with a wide range of applications. Despite the\nimportance and recent progress, most existing works on personalized LLMs have\nfocused either entirely on (a) personalized text generation or (b) leveraging\nLLMs for personalization-related downstream applications, such as\nrecommendation systems. In this work, we bridge the gap between these two\nseparate main directions for the first time by introducing a taxonomy for\npersonalized LLM usage and summarizing the key differences and challenges. We\nprovide a formalization of the foundations of personalized LLMs that\nconsolidates and expands notions of personalization of LLMs, defining and\ndiscussing novel facets of personalization, usage, and desiderata of\npersonalized LLMs. We then unify the literature across these diverse fields and\nusage scenarios by proposing systematic taxonomies for the granularity of\npersonalization, personalization techniques, datasets, evaluation methods, and\napplications of personalized LLMs. Finally, we highlight challenges and\nimportant open problems that remain to be addressed. By unifying and surveying\nrecent research using the proposed taxonomies, we aim to provide a clear guide\nto the existing literature and different facets of personalization in LLMs,\nempowering both researchers and practitioners.', 'Utilizing user profiles to personalize Large Language Models (LLMs) has been\nshown to enhance the performance on a wide range of tasks. However, the precise\nrole of user profiles and their effect mechanism on LLMs remains unclear. This\nstudy first confirms that the effectiveness of user profiles is primarily due\nto personalization information rather than semantic information. Furthermore,\nwe investigate how user profiles affect the personalization of LLMs. Within the\nuser profile, we reveal that it is the historical personalized response\nproduced or approved by users that plays a pivotal role in personalizing LLMs.\nThis discovery unlocks the potential of LLMs to incorporate a greater number of\nuser profiles within the constraints of limited input length. As for the\nposition of user profiles, we observe that user profiles integrated into\ndifferent positions of the input context do not contribute equally to\npersonalization. Instead, where the user profile that is closer to the\nbeginning affects more on the personalization of LLMs. Our findings reveal the\nrole of user profiles for the personalization of LLMs, and showcase how\nincorporating user profiles impacts performance providing insight to leverage\nuser profiles effectively.']"
140,20,140_molecular_molecule_molecules_property,"['molecular', 'molecule', 'molecules', 'property', 'graph', 'smiles', 'odor', 'prediction', 'chemistry', 'neural']","['Molecules have various computational representations, including numerical\ndescriptors, strings, graphs, point clouds, and surfaces. Each representation\nmethod enables the application of various machine learning methodologies from\nlinear regression to graph neural networks paired with large language models.\nTo complement existing representations, we introduce the representation of\nmolecules through vector-valued functions, or $n$-dimensional vector fields,\nthat are parameterized by neural networks, which we denote molecular neural\nfields. Unlike surface representations, molecular neural fields capture\nexternal features and the hydrophobic core of macromolecules such as proteins.\nCompared to discrete graph or point representations, molecular neural fields\nare compact, resolution independent and inherently suited for interpolation in\nspatial and temporal dimensions. These properties inherited by molecular neural\nfields lend themselves to tasks including the generation of molecules based on\ntheir desired shape, structure, and composition, and the resolution-independent\ninterpolation between molecular conformations in space and time. Here, we\nprovide a framework and proofs-of-concept for molecular neural fields, namely,\nthe parametrization and superresolution reconstruction of a protein-ligand\ncomplex using an auto-decoder architecture and the embedding of molecular\nvolumes in latent space using an auto-encoder architecture.', 'In-context learning (ICL) effectively conditions large language models (LLMs)\nfor molecular tasks, such as property prediction and molecule captioning, by\nembedding carefully selected demonstration examples into the input prompt. This\napproach avoids the computational overhead of extensive pertaining and\nfine-tuning. However, current prompt retrieval methods for molecular tasks have\nrelied on molecule feature similarity, such as Morgan fingerprints, which do\nnot adequately capture the global molecular and atom-binding relationships. As\na result, these methods fail to represent the full complexity of molecular\nstructures during inference. Moreover, small-to-medium-sized LLMs, which offer\nsimpler deployment requirements in specialized systems, have remained largely\nunexplored in the molecular ICL literature. To address these gaps, we propose a\nself-supervised learning technique, GAMIC (Graph-Aligned Molecular In-Context\nlearning, which aligns global molecular structures, represented by graph neural\nnetworks (GNNs), with textual captions (descriptions) while leveraging local\nfeature similarity through Morgan fingerprints. In addition, we introduce a\nMaximum Marginal Relevance (MMR) based diversity heuristic during retrieval to\noptimize input prompt demonstration samples. Our experimental findings using\ndiverse benchmark datasets show GAMIC outperforms simple Morgan-based ICL\nretrieval methods across all tasks by up to 45%.', 'Recent advances in Large Language Models (LLMs) have motivated the\ndevelopment of general LLMs for molecular tasks. While several studies have\ndemonstrated that fine-tuned LLMs can achieve impressive benchmark\nperformances, they are far from genuine generalist molecular LLMs due to a lack\nof fundamental understanding of molecular structure. Specifically, when given\nmolecular task instructions, LLMs trained with naive next-token prediction\ntraining assign similar likelihood scores to both original and negatively\ncorrupted molecules, revealing their lack of molecular structure understanding\nthat is crucial for reliable and general molecular LLMs. To overcome this\nlimitation and obtain a true generalist molecular LLM, we introduce a novel\nmulti-modal training method based on a thorough multi-modal instruction tuning\nas well as a molecular structure preference optimization between chosen and\nrejected graphs. On various molecular benchmarks, the proposed generalist\nmolecular LLM, called Mol-LLM, achieves state-of-the-art performances among\ngeneralist LLMs on most tasks, at the same time, surpassing or comparable to\nstate-of-the-art specialist LLMs. Moreover, Mol-LLM also shows superior\ngeneralization performances in reaction prediction tasks, demonstrating the\neffect of the molecular structure understanding for generalization perspective.']"
141,20,141_selfcorrection_intrinsic_moral_feedback,"['selfcorrection', 'intrinsic', 'moral', 'feedback', 'response', 'responses', 'behavior', 'capability', 'thought', 'ml']","['In this paper, we investigate the degree to which fine-tuning in Large\nLanguage Models (LLMs) effectively mitigates versus merely conceals undesirable\nbehavior. Through the lens of semi-realistic role-playing exercises designed to\nelicit such behaviors, we explore the response dynamics of LLMs post\nfine-tuning interventions. Our methodology involves prompting models for\nChain-of-Thought (CoT) reasoning and analyzing the coherence between the\nreasoning traces and the resultant outputs. Notably, we identify a pervasive\nphenomenon we term \\emph{reason-based deception}, where models either stop\nproducing reasoning traces or produce seemingly ethical reasoning traces that\nbelie the unethical nature of their final outputs. We further examine the\nefficacy of response strategies (polite refusal versus explicit rebuttal) in\ncurbing the occurrence of undesired behavior in subsequent outputs of\nmulti-turn interactions. Our findings reveal that explicit rebuttals\nsignificantly outperform polite refusals in preventing the continuation of\nundesired outputs and nearly eliminate reason-based deception, challenging\ncurrent practices in model fine-tuning. Accordingly, the two key contributions\nof this paper are (1) defining and studying reason-based deception, a new type\nof hidden behavior, and (2) demonstrating that rebuttals provide a more robust\nresponse model to harmful requests than refusals, thereby highlighting the need\nto reconsider the response strategies in fine-tuning approaches.', ""What can contemporary machine learning (ML) models do? Given the\nproliferation of ML models in society, answering this question matters to a\nvariety of stakeholders, both public and private. The evaluation of models'\ncapabilities is rapidly emerging as a key subfield of modern ML, buoyed by\nregulatory attention and government grants. Despite this, the notion of an ML\nmodel possessing a capability has not been interrogated: what are we saying\nwhen we say that a model is able to do something? And what sorts of evidence\nbear upon this question? In this paper, we aim to answer these questions, using\nthe capabilities of large language models (LLMs) as a running example. Drawing\non the large philosophical literature on abilities, we develop an account of ML\nmodels' capabilities which can be usefully applied to the nascent science of\nmodel evaluation. Our core proposal is a conditional analysis of model\nabilities (CAMA): crudely, a machine learning model has a capability to X just\nwhen it would reliably succeed at doing X if it 'tried'. The main contribution\nof the paper is making this proposal precise in the context of ML, resulting in\nan operationalisation of CAMA applicable to LLMs. We then put CAMA to work,\nshowing that it can help make sense of various features of ML model evaluation\npractice, as well as suggest procedures for performing fair inter-model\ncomparisons."", 'Iterative human engagement is a common and effective means of leveraging the\nadvanced language processing power of large language models (LLMs). Using\nwell-structured prompts in a conversational manner, human users can effectively\ninfluence an LLM to develop more thoughtful and accurate responses. Motivated\nby this insight, we propose the Iteration of Thought (IoT) framework for\nenhancing LLM responses by generating ""thought""-provoking prompts vis a vis an\ninput query and the current iteration of an LLM\'s response. Unlike static or\nsemi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),\nIoT adapts its reasoning path dynamically, based on evolving context, and\nwithout generating alternate explorative thoughts which are ultimately\ndiscarded. The three components of the IoT framework are (1) an Inner Dialogue\nAgent (IDA) responsible for generating instructive, context-specific prompts;\n(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;\nand (3) an iterative prompting loop that implements a conversation between the\nformer two components. We introduce two variants of our framework: Autonomous\nIteration of Thought (AIoT), where an LLM decides when to stop iterating, and\nGuided Iteration of Thought (GIoT), which always forces a fixed number\niterations. We investigate the performance of IoT across various datasets,\nspanning complex reasoning tasks from the GPQA dataset, explorative\nproblem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop\nquestion answering from the HotpotQA dataset. Our results show that IoT\nrepresents a viable paradigm for autonomous response refinement in LLMs,\nshowcasing significant improvements over CoT and thereby enabling more adaptive\nand efficient reasoning systems that minimize human intervention.']"
142,20,142_chemical_chemistry_reaction_catalyst,"['chemical', 'chemistry', 'reaction', 'catalyst', 'chemtask', 'synthesis', 'cactus', 'elucidation', 'discovery', 'catalysts']","[""To enhance large language models (LLMs) for chemistry problem solving,\nseveral LLM-based agents augmented with tools have been proposed, such as\nChemCrow and Coscientist. However, their evaluations are narrow in scope,\nleaving a large gap in understanding the benefits of tools across diverse\nchemistry tasks. To bridge this gap, we develop ChemToolAgent, an enhanced\nchemistry agent over ChemCrow, and conduct a comprehensive evaluation of its\nperformance on both specialized chemistry tasks and general chemistry\nquestions. Surprisingly, ChemToolAgent does not consistently outperform its\nbase LLMs without tools. Our error analysis with a chemistry expert suggests\nthat: For specialized chemistry tasks, such as synthesis prediction, we should\naugment agents with specialized tools; however, for general chemistry questions\nlike those in exams, agents' ability to reason correctly with chemistry\nknowledge matters more, and tool augmentation does not always help."", 'There is a growing interest in the role that LLMs play in chemistry which\nlead to an increased focus on the development of LLMs benchmarks tailored to\nchemical domains to assess the performance of LLMs across a spectrum of\nchemical tasks varying in type and complexity. However, existing benchmarks in\nthis domain fail to adequately meet the specific requirements of chemical\nresearch professionals. To this end, we propose \\textbf{\\textit{ChemEval}},\nwhich provides a comprehensive assessment of the capabilities of LLMs across a\nwide range of chemical domain tasks. Specifically, ChemEval identified 4\ncrucial progressive levels in chemistry, assessing 12 dimensions of LLMs across\n42 distinct chemical tasks which are informed by open-source data and the data\nmeticulously crafted by chemical experts, ensuring that the tasks have\npractical value and can effectively evaluate the capabilities of LLMs. In the\nexperiment, we evaluate 12 mainstream LLMs on ChemEval under zero-shot and\nfew-shot learning contexts, which included carefully selected demonstration\nexamples and carefully designed prompts. The results show that while general\nLLMs like GPT-4 and Claude-3.5 excel in literature understanding and\ninstruction following, they fall short in tasks demanding advanced chemical\nknowledge. Conversely, specialized LLMs exhibit enhanced chemical competencies,\nalbeit with reduced literary comprehension. This suggests that LLMs have\nsignificant potential for enhancement when tackling sophisticated tasks in the\nfield of chemistry. We believe our work will facilitate the exploration of\ntheir potential to drive progress in chemistry. Our benchmark and analysis will\nbe available at {\\color{blue} \\url{https://github.com/USTC-StarTeam/ChemEval}}.', 'While machine learning algorithms have been shown to excel at specific\nchemical tasks, they have struggled to capture the strategic thinking that\ncharacterizes expert chemical reasoning, limiting their widespread adoption.\nHere we demonstrate that large language models (LLMs) can serve as powerful\nchemical reasoning engines when integrated with traditional search algorithms,\nenabling a new approach to computer-aided chemistry that mirrors human expert\nthinking. Rather than using LLMs to directly manipulate chemical structures, we\nleverage their ability to evaluate chemical strategies and guide search\nalgorithms toward chemically meaningful solutions. We demonstrate this paradigm\nthrough two fundamental challenges: strategy-aware retrosynthetic planning and\nmechanism elucidation. In retrosynthetic planning, our method allows chemists\nto specify desired synthetic strategies in natural language to find routes that\nsatisfy these constraints in vast searches. In mechanism elucidation, LLMs\nguide the search for plausible reaction mechanisms by combining chemical\nprinciples with systematic exploration. Our approach shows strong performance\nacross diverse chemical tasks, with larger models demonstrating increasingly\nsophisticated chemical reasoning. Our approach establishes a new paradigm for\ncomputer-aided chemistry that combines the strategic understanding of LLMs with\nthe precision of traditional chemical tools, opening possibilities for more\nintuitive and powerful chemical reasoning systems.']"
143,20,143_robots_social_robot_humanrobot,"['robots', 'social', 'robot', 'humanrobot', 'interactions', 'ethical', 'interaction', 'enjoyment', 'hri', 'tactile']","[""Understanding user enjoyment is crucial in human-robot interaction (HRI), as\nit can impact interaction quality and influence user acceptance and long-term\nengagement with robots, particularly in the context of conversations with\nsocial robots. However, current assessment methods rely solely on self-reported\nquestionnaires, failing to capture interaction dynamics. This work introduces\nthe Human-Robot Interaction Conversational User Enjoyment Scale (HRI CUES), a\nnovel scale for assessing user enjoyment from an external perspective during\nconversations with a robot. Developed through rigorous evaluations and\ndiscussions of three annotators with relevant expertise, the scale provides a\nstructured framework for assessing enjoyment in each conversation exchange\n(turn) alongside overall interaction levels. It aims to complement\nself-reported enjoyment from users and holds the potential for autonomously\nidentifying user enjoyment in real-time HRI. The scale was validated on 25\nolder adults' open-domain dialogue with a companion robot that was powered by a\nlarge language model for conversations, corresponding to 174 minutes of data,\nshowing moderate to good alignment. The dataset is available online.\nAdditionally, the study offers insights into understanding the nuances and\nchallenges of assessing user enjoyment in robot interactions, and provides\nguidelines on applying the scale to other domains."", ""We present a novel framework for designing emotionally agile robots with\ndynamic personalities and memory-based learning, with the aim of performing\nadaptive and non-deterministic interactions with humans while conforming to\nshared social understanding. While existing work has largely focused on emotion\nrecognition and static response systems, many approaches rely on sentiment\nanalysis and action mapping frameworks that are pre-defined with limited\ndimensionality and fixed configurations, lacking the flexibility of dynamic\npersonality traits and memory-enabled adaptation. Other systems are often\nrestricted to limited modes of expression and fail to develop a causal\nrelationship between human behavior and the robot's proactive physical actions,\nresulting in constrained adaptability and reduced responsiveness in complex,\ndynamic interactions. Our methodology integrates the Big Five Personality\nTraits, Appraisal Theory, and abstracted memory layers through Large Language\nModels (LLMs). The LLM generates a parameterized robot personality based on the\nBig Five, processes human language and sentiments, evaluates human behavior\nusing Appraisal Theory, and generates emotions and selects appropriate actions\nadapted by historical context over time. We validated the framework by testing\nthree robots with distinct personalities in identical background contexts and\nfound that personality, appraisal, and memory influence the adaptability of\nhuman-robot interactions. The impact of the individual components was further\nvalidated through ablation tests. We conclude that this system enables robots\nto engage in meaningful and personalized interactions with users, and holds\nsignificant potential for applications in domains such as pet robots, assistive\nrobots, educational robots, and collaborative functional robots, where\ncultivating tailored relationships and enriching user experiences are\nessential."", 'In this work, we describe our approach to developing an intelligent and\nrobust social robotic system for the Nadine social robot platform. We achieve\nthis by integrating Large Language Models (LLMs) and skilfully leveraging the\npowerful reasoning and instruction-following capabilities of these types of\nmodels to achieve advanced human-like affective and cognitive capabilities.\nThis approach is novel compared to the current state-of-the-art LLM-based\nagents which do not implement human-like long-term memory or sophisticated\nemotional appraisal. The naturalness of social robots, consisting of multiple\nmodules, highly depends on the performance and capabilities of each component\nof the system and the seamless integration of the components. We built a social\nrobot system that enables generating appropriate behaviours through multimodal\ninput processing, bringing episodic memories accordingly to the recognised\nuser, and simulating the emotional states of the robot induced by the\ninteraction with the human partner. In particular, we introduce an LLM-agent\nframe for social robots, SoR-ReAct, serving as a core component for the\ninteraction module in our system. This design has brought forth the advancement\nof social robots and aims to increase the quality of human-robot interaction.']"
144,19,144_forgery_detection_deepfake_image,"['forgery', 'detection', 'deepfake', 'image', 'truthlens', 'child', 'ifdl', 'mad', 'face', 'facial']","[""Image forgery localization, which centers on identifying tampered pixels\nwithin an image, has seen significant advancements. Traditional approaches\noften model this challenge as a variant of image segmentation, treating the\nbinary segmentation of forged areas as the end product. We argue that the basic\nbinary forgery mask is inadequate for explaining model predictions. It doesn't\nclarify why the model pinpoints certain areas and treats all forged pixels the\nsame, making it hard to spot the most fake-looking parts. In this study, we\nmitigate the aforementioned limitations by generating salient region-focused\ninterpretation for the forgery images. To support this, we craft a Multi-Modal\nTramper Tracing (MMTT) dataset, comprising facial images manipulated using\ndeepfake techniques and paired with manual, interpretable textual annotations.\nTo harvest high-quality annotation, annotators are instructed to meticulously\nobserve the manipulated images and articulate the typical characteristics of\nthe forgery regions. Subsequently, we collect a dataset of 128,303 image-text\npairs. Leveraging the MMTT dataset, we develop ForgeryTalker, an architecture\ndesigned for concurrent forgery localization and interpretation. ForgeryTalker\nfirst trains a forgery prompter network to identify the pivotal clues within\nthe explanatory text. Subsequently, the region prompter is incorporated into\nmultimodal large language model for finetuning to achieve the dual goals of\nlocalization and interpretation. Extensive experiments conducted on the MMTT\ndataset verify the superior performance of our proposed model. The dataset,\ncode as well as pretrained checkpoints will be made publicly available to\nfacilitate further research and ensure the reproducibility of our results."", ""The rapid advancement of deepfake technologies has sparked widespread public\nconcern, particularly as face forgery poses a serious threat to public\ninformation security. However, the unknown and diverse forgery techniques,\nvaried facial features and complex environmental factors pose significant\nchallenges for face forgery analysis. Existing datasets lack descriptive\nannotations of these aspects, making it difficult for models to distinguish\nbetween real and forged faces using only visual information amid various\nconfounding factors. In addition, existing methods fail to yield user-friendly\nand explainable results, hindering the understanding of the model's\ndecision-making process. To address these challenges, we introduce a novel\nOpen-World Face Forgery Analysis VQA (OW-FFA-VQA) task and its corresponding\nbenchmark. To tackle this task, we first establish a dataset featuring a\ndiverse collection of real and forged face images with essential descriptions\nand reliable forgery reasoning. Based on this dataset, we introduce FFAA: Face\nForgery Analysis Assistant, consisting of a fine-tuned Multimodal Large\nLanguage Model (MLLM) and Multi-answer Intelligent Decision System (MIDS). By\nintegrating hypothetical prompts with MIDS, the impact of fuzzy classification\nboundaries is effectively mitigated, enhancing model robustness. Extensive\nexperiments demonstrate that our method not only provides user-friendly and\nexplainable results but also significantly boosts accuracy and robustness\ncompared to previous methods."", 'Multimodal Large Language Models (MLLMs), such as GPT4o, have shown strong\ncapabilities in visual reasoning and explanation generation. However, despite\nthese strengths, they face significant challenges in the increasingly critical\ntask of Image Forgery Detection and Localization (IFDL). Moreover, existing\nIFDL methods are typically limited to the learning of low-level\nsemantic-agnostic clues and merely provide a single outcome judgment. To tackle\nthese issues, we propose ForgeryGPT, a novel framework that advances the IFDL\ntask by capturing high-order forensics knowledge correlations of forged images\nfrom diverse linguistic feature spaces, while enabling explainable generation\nand interactive dialogue through a newly customized Large Language Model (LLM)\narchitecture. Specifically, ForgeryGPT enhances traditional LLMs by integrating\nthe Mask-Aware Forgery Extractor, which enables the excavating of precise\nforgery mask information from input images and facilitating pixel-level\nunderstanding of tampering artifacts. The Mask-Aware Forgery Extractor consists\nof a Forgery Localization Expert (FL-Expert) and a Mask Encoder, where the\nFL-Expert is augmented with an Object-agnostic Forgery Prompt and a\nVocabulary-enhanced Vision Encoder, allowing for effectively capturing of\nmulti-scale fine-grained forgery details. To enhance its performance, we\nimplement a three-stage training strategy, supported by our designed Mask-Text\nAlignment and IFDL Task-Specific Instruction Tuning datasets, which align\nvision-language modalities and improve forgery detection and\ninstruction-following capabilities. Extensive experiments demonstrate the\neffectiveness of the proposed method.']"
145,19,145_iot_cpsiot_sensor_things,"['iot', 'cpsiot', 'sensor', 'things', 'crash', 'internet', 'tinyml', 'iotase', 'chatiot', 'data']","['The Internet of Things (IoT) in the sixth generation (6G) era is envisioned\nto evolve towards intelligence, ubiquity, and self-optimization. Large language\nmodels (LLMs) have demonstrated remarkable generalization capabilities across\ndiverse domains, including natural language processing (NLP), computer vision\n(CV), and beyond. In this article, we propose an LLM-empowered IoT architecture\nfor 6G networks to achieve intelligent autonomy while supporting advanced IoT\napplications. LLMs are pushed to the edge of the 6G network to support the\nsynergy of LLMs and IoT. LLM solutions are tailored to both IoT application\nrequirements and IoT management needs, i.e., LLM for IoT. On the other hand,\nedge inference and edge fine-tuning are discussed to support the deployment of\nLLMs, i.e., LLM on IoT. Furthermore, we propose a memory-efficient split\nfederated learning (SFL) framework for LLM fine-tuning on heterogeneous IoT\ndevices that alleviates memory pressures on both IoT devices and the edge\nserver while achieving comparable performance and convergence time. Finally, a\ncase study is presented, followed by a discussion about open issues of\nLLM-empowered IoT for 6G networks.', 'Recent advances in Large Language Models (LLMs) have positively and\nefficiently transformed workflows in many domains. One such domain with\nsignificant potential for LLM integration is the Internet of Things (IoT),\nwhere this integration brings new opportunities for improved decision making\nand system interaction. In this paper, we explore the various roles of LLMs in\nIoT, with a focus on their reasoning capabilities. We show how LLM-IoT\nintegration can facilitate advanced decision making and contextual\nunderstanding in a variety of IoT scenarios. Furthermore, we explore the\nintegration of LLMs with edge, fog, and cloud computing paradigms, and show how\nthis synergy can optimize resource utilization, enhance real-time processing,\nand provide scalable solutions for complex IoT applications. To the best of our\nknowledge, this is the first comprehensive study covering IoT-LLM integration\nbetween edge, fog, and cloud systems. Additionally, we propose a novel system\nmodel for industrial IoT applications that leverages LLM-based collective\nintelligence to enable predictive maintenance and condition monitoring.\nFinally, we highlight key challenges and open issues that provide insights for\nfuture research in the field of LLM-IoT integration.', 'Large Language Models (LLMs) have demonstrated remarkable capabilities across\ntextual and visual domains but often generate outputs that violate physical\nlaws, revealing a gap in their understanding of the physical world. Inspired by\nhuman cognition, where perception is fundamental to reasoning, we explore\naugmenting LLMs with enhanced perception abilities using Internet of Things\n(IoT) sensor data and pertinent knowledge for IoT task reasoning in the\nphysical world. In this work, we systematically study LLMs capability to\naddress real-world IoT tasks by augmenting their perception and knowledge base,\nand then propose a unified framework, IoT-LLM, to enhance such capability. In\nIoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats\namenable to LLMs, activating their commonsense knowledge through\nchain-of-thought prompting and specialized role definitions, and expanding\ntheir understanding via IoT-oriented retrieval-augmented generation based on\nin-context learning. To evaluate the performance, We design a new benchmark\nwith five real-world IoT tasks with different data types and reasoning\ndifficulties and provide the benchmarking results on six open-source and\nclose-source LLMs. Experimental results demonstrate the limitations of existing\nLLMs with naive textual inputs that cannot perform these tasks effectively. We\nshow that IoT-LLM significantly enhances the performance of IoT tasks reasoning\nof LLM, such as GPT-4, achieving an average improvement of 65% across various\ntasks against previous methods. The results also showcase LLMs ability to\ncomprehend IoT data and the physical law behind data by providing a reasoning\nprocess. Limitations of our work are claimed to inspire future research in this\nnew era.']"
146,19,146_eeg_bci_signals_bcis,"['eeg', 'bci', 'signals', 'bcis', 'decoding', 'brain', 'braincomputer', 'speller', 'neural', 'p300']","['Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial\nIntelligence (GenAI) has opened new frontiers in brain signal decoding,\nenabling assistive communication, neural representation learning, and\nmultimodal integration. BCIs, particularly those leveraging\nElectroencephalography (EEG), provide a non-invasive means of translating\nneural activity into meaningful outputs. Recent advances in deep learning,\nincluding Generative Adversarial Networks (GANs) and Transformer-based Large\nLanguage Models (LLMs), have significantly improved EEG-based generation of\nimages, text, and speech. This paper provides a literature review of the\nstate-of-the-art in EEG-based multimodal generation, focusing on (i)\nEEG-to-image generation through GANs, Variational Autoencoders (VAEs), and\nDiffusion Models, and (ii) EEG-to-text generation leveraging Transformer based\nlanguage models and contrastive learning methods. Additionally, we discuss the\nemerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We\nhighlight key datasets, use cases, challenges, and EEG feature encoding methods\nthat underpin generative approaches. By providing a structured overview of\nEEG-based generative AI, this survey aims to equip researchers and\npractitioners with insights to advance neural decoding, enhance assistive\ntechnologies, and expand the frontiers of brain-computer interaction.', 'Electroencephalography (EEG) is a non-invasive technique to measure and\nrecord brain electrical activity, widely used in various BCI and healthcare\napplications. Early EEG decoding methods rely on supervised learning, limited\nby specific tasks and datasets, hindering model performance and\ngeneralizability. With the success of large language models, there is a growing\nbody of studies focusing on EEG foundation models. However, these studies still\nleave challenges: Firstly, most of existing EEG foundation models employ full\nEEG modeling strategy. It models the spatial and temporal dependencies between\nall EEG patches together, but ignores that the spatial and temporal\ndependencies are heterogeneous due to the unique structural characteristics of\nEEG signals. Secondly, existing EEG foundation models have limited\ngeneralizability on a wide range of downstream BCI tasks due to varying formats\nof EEG data, making it challenging to adapt to. To address these challenges, we\npropose a novel foundation model called CBraMod. Specifically, we devise a\ncriss-cross transformer as the backbone to thoroughly leverage the structural\ncharacteristics of EEG signals, which can model spatial and temporal\ndependencies separately through two parallel attention mechanisms. And we\nutilize an asymmetric conditional positional encoding scheme which can encode\npositional information of EEG patches and be easily adapted to the EEG with\ndiverse formats. CBraMod is pre-trained on a very large corpus of EEG through\npatch-based masked EEG reconstruction. We evaluate CBraMod on up to 10\ndownstream BCI tasks (12 public datasets). CBraMod achieves the\nstate-of-the-art performance across the wide range of tasks, proving its strong\ncapability and generalizability. The source code is publicly available at\nhttps://github.com/wjq-learning/CBraMod.', ""Decoding linguistic information from non-invasive brain signals using EEG has\ngained increasing research attention due to its vast applicational potential.\nRecently, a number of works have adopted a generative-based framework to decode\nelectroencephalogram (EEG) signals into sentences by utilizing the power\ngenerative capacity of pretrained large language models (LLMs). However, this\napproach has several drawbacks that hinder the further development of\nlinguistic applications for brain-computer interfaces (BCIs). Specifically, the\nability of the EEG encoder to learn semantic information from EEG data remains\nquestionable, and the LLM decoder's tendency to generate sentences based on its\ntraining memory can be hard to avoid. These issues necessitate a novel approach\nfor converting EEG signals into sentences. In this paper, we propose a novel\ntwo-step pipeline that addresses these limitations and enhances the validity of\nlinguistic EEG decoding research. We first confirm that word-level semantic\ninformation can be learned from EEG data recorded during natural reading by\ntraining a Conformer encoder via a masked contrastive objective for word-level\nclassification. To achieve sentence decoding results, we employ a training-free\nretrieval method to retrieve sentences based on the predictions from the EEG\nencoder. Extensive experiments and ablation studies were conducted in this\npaper for a comprehensive evaluation of the proposed approach. Visualization of\nthe top prediction candidates reveals that our model effectively groups EEG\nsegments into semantic categories with similar meanings, thereby validating its\nability to learn patterns from unspoken EEG recordings. Despite the exploratory\nnature of this work, these results suggest that our method holds promise for\nproviding more reliable solutions for converting EEG signals into text.""]"
147,19,147_normalization_layers_preln_ln,"['normalization', 'layers', 'preln', 'ln', 'massive', 'transformer', 'layer', 'postln', 'activations', 'training']","[""Designing Transformer architectures with the optimal layer normalization (LN)\nstrategy that ensures large-scale training stability and expedite convergence\nhas remained elusive, even in this era of large language models (LLMs). To this\nend, we present a comprehensive analytical foundation for understanding how\ndifferent LN strategies influence training dynamics in large-scale Transformer\ntraining. Until recently, Pre-LN and Post-LN have long dominated standard\npractices despite their limitations in large-scale training. However, several\nopen-source large-scale models have recently begun silently adopting a third\nstrategy without much explanation. This strategy places layer normalization\n(LN) peripherally around sublayers, a design we term Peri-LN. While Peri-LN has\ndemonstrated promising empirical performance, its precise mechanisms and\nbenefits remain almost unexplored. Our in-depth analysis shows that Peri-LN\nstrikes an ideal balance in variance growth -- unlike Pre-LN and Post-LN, which\nare prone to vanishing gradients and ``massive activations.'' To validate our\ntheoretical insight, we conduct large-scale experiments on Transformers up to\n3.2B parameters, showing that Peri-LN consistently achieves more balanced\nvariance growth, steadier gradient flow, and convergence stability. Our results\nsuggest that Peri-LN warrants broader consideration for large-scale Transformer\narchitectures, providing renewed insights into the optimal placement and\napplication of LN."", 'Training stability of large language models(LLMs) is an important research\ntopic. Reproducing training instabilities can be costly, so we use a small\nlanguage model with 830M parameters and experiment with higher learning rates\nto force models to diverge. One of the sources of training instability is the\ngrowth of logits in attention layers. We extend the focus of the previous work\nand look not only at the magnitude of the logits but at all outputs of linear\nlayers in the Transformer block. We observe that with a high learning rate the\nL2 norm of all linear layer outputs can grow with each training step and the\nmodel diverges. Specifically we observe that QKV, Proj and FC2 layers have the\nlargest growth of the output magnitude. This prompts us to explore several\noptions: 1) apply layer normalization not only after QK layers but also after\nProj and FC2 layers too; 2) apply layer normalization after the QKV layer (and\nremove pre normalization). 3) apply QK layer normalization together with\nsoftmax capping. We show that with the last two methods we can increase\nlearning rate by 1.5x (without model divergence) in comparison to an approach\nbased on QK layer normalization only. Also we observe significant perplexity\nimprovements for all three methods in comparison to the baseline model.', 'In this paper, we introduce the Curse of Depth, a concept that highlights,\nexplains, and addresses the recent observation in modern Large Language\nModels(LLMs) where nearly half of the layers are less effective than expected.\nWe first confirm the wide existence of this phenomenon across the most popular\nfamilies of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis,\ntheoretically and empirically, identifies that the underlying reason for the\nineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer\nNormalization (Pre-LN). While Pre-LN stabilizes the training of Transformer\nLLMs, its output variance exponentially grows with the model depth, which\nundesirably causes the derivative of the deep Transformer blocks to be an\nidentity matrix, and therefore barely contributes to the training. To resolve\nthis training pitfall, we propose LayerNorm Scaling, which scales the variance\nof output of the layer normalization inversely by the square root of its depth.\nThis simple modification mitigates the output variance explosion of deeper\nTransformer layers, improving their contribution. Our experimental results,\nspanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling\nsignificantly enhances LLM pre-training performance compared to Pre-LN.\nMoreover, this improvement seamlessly carries over to supervised fine-tuning.\nAll these gains can be attributed to the fact that LayerNorm Scaling enables\ndeeper layers to contribute more effectively during training.']"
148,18,148_geometric_geometry_layout_fire,"['geometric', 'geometry', 'layout', 'fire', 'visual', 'graphics', 'images', 'individuals', 'tonggeometry', 'egoblind']","['Addressing the challenge of automated geometry math problem-solving in\nartificial intelligence (AI) involves understanding multi-modal information and\nmathematics. Current methods struggle with accurately interpreting geometry\ndiagrams, which hinders effective problem-solving. To tackle this issue, we\npresent the Geometry problem sOlver with natural Language Description (GOLD)\nmodel. GOLD enhances the extraction of geometric relations by separately\nprocessing symbols and geometric primitives within the diagram. Subsequently,\nit converts the extracted relations into natural language descriptions,\nefficiently utilizing large language models to solve geometry math problems.\nExperiments show that the GOLD model outperforms the Geoformer model, the\nprevious best method on the UniGeo dataset, by achieving accuracy improvements\nof 12.7% and 42.1% in calculation and proving subsets. Additionally, it\nsurpasses the former best model on the PGPS9K and Geometry3K datasets, PGPSNet,\nby obtaining accuracy enhancements of 1.8% and 3.2%, respectively.', 'Geometric diagrams are critical in conveying mathematical and scientific\nconcepts, yet traditional diagram generation methods are often manual and\nresource-intensive. While text-to-image generation has made strides in\nphotorealistic imagery, creating accurate geometric diagrams remains a\nchallenge due to the need for precise spatial relationships and the scarcity of\ngeometry-specific datasets. This paper presents MagicGeo, a training-free\nframework for generating geometric diagrams from textual descriptions. MagicGeo\nformulates the diagram generation process as a coordinate optimization problem,\nensuring geometric correctness through a formal language solver, and then\nemploys coordinate-aware generation. The framework leverages the strong\nlanguage translation capability of large language models, while formal\nmathematical solving ensures geometric correctness. We further introduce\nMagicGeoBench, a benchmark dataset of 220 geometric diagram descriptions, and\ndemonstrate that MagicGeo outperforms current methods in both qualitative and\nquantitative evaluations. This work provides a scalable, accurate solution for\nautomated diagram generation, with significant implications for educational and\nacademic applications.', ""Geometric ability is a significant challenge for large language models (LLMs)\ndue to the need for advanced spatial comprehension and abstract thinking.\nExisting datasets primarily evaluate LLMs on their final answers, but they\ncannot truly measure their true understanding of geometric structures, as LLMs\ncan arrive at correct answers by coincidence. To fill this gap, we introduce\nthe GeomRel dataset, designed to evaluate LLMs' understanding of geometric\nstructures by isolating the core step of geometric relationship identification\nin problem-solving. Using this benchmark, we conduct thorough evaluations of\ndiverse LLMs and identify key limitations in understanding geometric\nstructures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,\nwhich enhances LLMs' ability to identify geometric relationships, resulting in\nsignificant performance improvements.""]"
149,18,149_essay_aes_scoring_essays,"['essay', 'aes', 'scoring', 'essays', 'trait', 'explainable', 'multitrait', 'arabic', 'scores', 'automated']","[""Multi-trait automated essay scoring (AES) systems provide a fine-grained\nevaluation of an essay's diverse aspects. While they excel in scoring, prior\nsystems fail to explain why specific trait scores are assigned. This lack of\ntransparency leaves instructors and learners unconvinced of the AES outputs,\nhindering their practical use. To address this, we propose a self-explainable\nRationale-Driven Multi-trait automated Essay scoring (RaDME) framework. RaDME\nleverages the reasoning capabilities of large language models (LLMs) by\ndistilling them into a smaller yet effective scorer. This more manageable\nstudent model is optimized to sequentially generate a trait score followed by\nthe corresponding rationale, thereby inherently learning to select a more\njustifiable score by considering the subsequent rationale during training. Our\nfindings indicate that while LLMs underperform in direct AES tasks, they excel\nin rationale generation when provided with precise numerical scores. Thus,\nRaDME integrates the superior reasoning capacities of LLMs into the robust\nscoring accuracy of an optimized smaller model. Extensive experiments\ndemonstrate that RaDME achieves both accurate and adequate reasoning while\nsupporting high-quality multi-trait scoring, significantly enhancing the\ntransparency of AES."", 'Individual feedback can help students improve their essay writing skills.\nHowever, the manual effort required to provide such feedback limits\nindividualization in practice. Automatically-generated essay feedback may serve\nas an alternative to guide students at their own pace, convenience, and desired\nfrequency. Large language models (LLMs) have demonstrated strong performance in\ngenerating coherent and contextually relevant text. Yet, their ability to\nprovide helpful essay feedback is unclear. This work explores several prompting\nstrategies for LLM-based zero-shot and few-shot generation of essay feedback.\nInspired by Chain-of-Thought prompting, we study how and to what extent\nautomated essay scoring (AES) can benefit the quality of generated feedback. We\nevaluate both the AES performance that LLMs can achieve with prompting only and\nthe helpfulness of the generated essay feedback. Our results suggest that\ntackling AES and feedback generation jointly improves AES performance. However,\nwhile our manual evaluation emphasizes the quality of the generated essay\nfeedback, the impact of essay scoring on the generated feedback remains low\nultimately.', ""Automated Essay Scoring (AES) plays a crucial role in assessing language\nlearners' writing quality, reducing grading workload, and providing real-time\nfeedback. Arabic AES systems are particularly challenged by the lack of\nannotated essay datasets. This paper presents a novel framework leveraging\nLarge Language Models (LLMs) and Transformers to generate synthetic Arabic\nessay datasets for AES. We prompt an LLM to generate essays across CEFR\nproficiency levels and introduce controlled error injection using a fine-tuned\nStandard Arabic BERT model for error type prediction. Our approach produces\nrealistic human-like essays, contributing a dataset of 3,040 annotated essays.\nAdditionally, we develop a BERT-based auto-marking system for accurate and\nscalable Arabic essay evaluation. Experimental results demonstrate the\neffectiveness of our framework in improving Arabic AES performance.""]"
150,18,150_instructions_instruction_multilingual_negative,"['instructions', 'instruction', 'multilingual', 'negative', 'lowresource', 'following', 'likra', 'lingualift', 'coursegptzh', 'xiwu']","[""One of the key strengths of Large Language Models (LLMs) is their ability to\ninteract with humans by generating appropriate responses to given instructions.\nThis ability, known as instruction-following capability, has established a\nfoundation for the use of LLMs across various fields and serves as a crucial\nmetric for evaluating their performance. While numerous evaluation benchmarks\nhave been developed, most focus solely on clear and coherent instructions.\nHowever, we have noted that LLMs can become easily distracted by\ninstruction-formatted statements, which may lead to an oversight of their\ninstruction comprehension skills. To address this issue, we introduce the\nIntention of Instruction (IoInst) benchmark. This benchmark evaluates LLMs'\ncapacity to remain focused and understand instructions without being misled by\nextraneous instructions. The primary objective of this benchmark is to identify\nthe appropriate instruction that accurately guides the generation of a given\ncontext. Our findings suggest that even recently introduced state-of-the-art\nmodels still lack instruction understanding capability. Along with the\nproposition of IoInst in this study, we also present broad analyses of the\nseveral strategies potentially applicable to IoInst."", ""Following multiple instructions is a crucial ability for large language\nmodels (LLMs). Evaluating this ability comes with significant challenges: (i)\nlimited coherence between multiple instructions, (ii) positional bias where the\norder of instructions affects model performance, and (iii) a lack of\nobjectively verifiable tasks. To address these issues, we introduce a benchmark\ndesigned to evaluate models' abilities to follow multiple instructions through\nsequential instruction following (SIFo) tasks. In SIFo, the successful\ncompletion of multiple instructions is verifiable by examining only the final\ninstruction. Our benchmark evaluates instruction following using four tasks\n(text modification, question answering, mathematics, and security rules), each\nassessing different aspects of sequential instruction following. Our evaluation\nof popular LLMs, both closed-source and open-source, shows that more recent and\nlarger models significantly outperform their older and smaller counterparts on\nthe SIFo tasks, validating the benchmark's effectiveness. All models struggle\nwith following sequences of instructions, hinting at an important lack of\nrobustness of today's language models."", 'Instruction following is one of the fundamental capabilities of large\nlanguage models (LLMs). As the ability of LLMs is constantly improving, they\nhave been increasingly applied to deal with complex human instructions in\nreal-world scenarios. Therefore, how to evaluate the ability of complex\ninstruction-following of LLMs has become a critical research problem. Existing\nbenchmarks mainly focus on modeling different types of constraints in human\ninstructions while neglecting the composition of different constraints, which\nis an indispensable constituent in complex instructions. To this end, we\npropose ComplexBench, a benchmark for comprehensively evaluating the ability of\nLLMs to follow complex instructions composed of multiple constraints. We\npropose a hierarchical taxonomy for complex instructions, including 4\nconstraint types, 19 constraint dimensions, and 4 composition types, and\nmanually collect a high-quality dataset accordingly. To make the evaluation\nreliable, we augment LLM-based evaluators with rules to effectively verify\nwhether generated texts can satisfy each constraint and composition.\nFurthermore, we obtain the final evaluation score based on the dependency\nstructure determined by different composition types. ComplexBench identifies\nsignificant deficiencies in existing LLMs when dealing with complex\ninstructions with multiple constraints composition.']"
151,18,151_merging_model_merged_parameter,"['merging', 'model', 'merged', 'parameter', 'taskspecific', 'kinship', 'finetuned', 'supermerge', 'rmm', 'dps']","['Recent advances in large language models have led to numerous\ntask-specialized fine-tuned variants, creating a need for efficient model\nmerging techniques that preserve specialized capabilities while avoiding costly\nretraining. While existing task vector-based merging methods show promise, they\ntypically apply uniform coefficients across all parameters, overlooking varying\nparameter importance both within and across tasks. We present Sens-Merging, a\nsensitivity-guided coefficient adjustment method that enhances existing model\nmerging techniques by operating at both task-specific and cross-task levels.\nOur method analyzes parameter sensitivity within individual tasks and evaluates\ncross-task transferability to determine optimal merging coefficients. Extensive\nexperiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that\nSens-Merging significantly improves performance across general knowledge,\nmathematical reasoning, and code generation tasks. Notably, when combined with\nexisting merging techniques, our method enables merged models to outperform\nspecialized fine-tuned models, particularly in code generation tasks. Our\nfindings reveal important trade-offs between task-specific and cross-task\nscalings, providing insights for future model merging strategies.', 'Model merging, a method that combines the parameters and embeddings of\nmultiple fine-tuned large language models (LLMs), offers a promising approach\nto enhance model performance across various tasks while maintaining\ncomputational efficiency. This paper introduces Activation-Informed Merging\n(AIM), a technique that integrates the information from the activation space of\nLLMs into the merging process to improve performance and robustness. AIM is\ndesigned as a flexible, complementary solution that is applicable to any\nexisting merging method. It aims to preserve critical weights from the base\nmodel, drawing on principles from continual learning~(CL) and model\ncompression. Utilizing a task-agnostic calibration set, AIM selectively\nprioritizes essential weights during merging. We empirically demonstrate that\nAIM significantly enhances the performance of merged models across multiple\nbenchmarks. Our findings suggest that considering the activation-space\ninformation can provide substantial advancements in the model merging\nstrategies for LLMs with up to 40\\% increase in benchmark performance.', 'Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.']"
152,18,152_stance_detection_media_heat,"['stance', 'detection', 'media', 'heat', 'social', 'segregation', 'opinion', 'fewshot', 'targets', 'sentiment']","['Stance detection, which aims to identify public opinion towards specific\ntargets using social media data, is an important yet challenging task. With the\nproliferation of diverse multimodal social media content including text, and\nimages multimodal stance detection (MSD) has become a crucial research area.\nHowever, existing MSD studies have focused on modeling stance within individual\ntext-image pairs, overlooking the multi-party conversational contexts that\nnaturally occur on social media. This limitation stems from a lack of datasets\nthat authentically capture such conversational scenarios, hindering progress in\nconversational MSD. To address this, we introduce a new multimodal multi-turn\nconversational stance detection dataset (called MmMtCSD). To derive stances\nfrom this challenging dataset, we propose a novel multimodal large language\nmodel stance detection framework (MLLM-SD), that learns joint stance\nrepresentations from textual and visual modalities. Experiments on MmMtCSD show\nstate-of-the-art performance of our proposed MLLM-SD approach for multimodal\nstance detection. We believe that MmMtCSD will contribute to advancing\nreal-world applications of stance detection research.', 'Stance detection has emerged as a popular task in natural language processing\nresearch, enabled largely by the abundance of target-specific social media\ndata. While there has been considerable research on the development of stance\ndetection models, datasets, and application, we highlight important gaps\npertaining to (i) a lack of theoretical conceptualization of stance, and (ii)\nthe treatment of stance at an individual- or user-level, as opposed to\nmessage-level. In this paper, we first review the interdisciplinary origins of\nstance as an individual-level construct to highlight relevant attributes (e.g.,\npsychological features) that might be useful to incorporate in stance detection\nmodels. Further, we argue that recent pre-trained and large language models\n(LLMs) might offer a way to flexibly infer such user-level attributes and/or\nincorporate them in modelling stance. To better illustrate this, we briefly\nreview and synthesize the emerging corpus of studies on using LLMs for\ninferring stance, and specifically on incorporating user attributes in such\ntasks. We conclude by proposing a four-point agenda for pursuing stance\ndetection research that is theoretically informed, inclusive, and practically\nimpactful.', 'In modern digital environments, users frequently express opinions on\ncontentious topics, providing a wealth of information on prevailing attitudes.\nThe systematic analysis of these opinions offers valuable insights for\ndecision-making in various sectors, including marketing and politics. As a\nresult, stance detection has emerged as a crucial subfield within affective\ncomputing, enabling the automatic detection of user stances in social media\nconversations and providing a nuanced understanding of public sentiment on\ncomplex issues. Recent years have seen a surge of research interest in\ndeveloping effective stance detection methods, with contributions from multiple\ncommunities, including natural language processing, web science, and social\ncomputing. This paper provides a comprehensive survey of stance detection\ntechniques on social media, covering task definitions, datasets, approaches,\nand future works. We review traditional stance detection models, as well as\nstate-of-the-art methods based on large language models, and discuss their\nstrengths and limitations. Our survey highlights the importance of stance\ndetection in understanding public opinion and sentiment, and identifies gaps in\ncurrent research. We conclude by outlining potential future directions for\nstance detection on social media, including the need for more robust and\ngeneralizable models, and the importance of addressing emerging challenges such\nas multi-modal stance detection and stance detection in low-resource languages.']"
153,17,153_fuzzing_fuzz_greybox_fuzzers,"['fuzzing', 'fuzz', 'greybox', 'fuzzers', 'testing', 'libraries', 'vulnerabilities', 'perfgen', 'inputs', 'afl']","['Fuzzing is an important dynamic program analysis technique designed for\nfinding vulnerabilities in complex software. Fuzzing involves presenting a\ntarget program with crafted malicious input to cause crashes, buffer overflows,\nmemory errors, and exceptions. Crafting malicious inputs in an efficient manner\nis a difficult open problem and the best approaches often apply uniform random\nmutations to pre-existing valid inputs. In this work, we propose to adopt\nfine-tuned large language models (FuzzCoder) to learn patterns in the input\nfiles from successful attacks to guide future fuzzing explorations.\nSpecifically, we develop a framework to leverage the code LLMs to guide the\nmutation process of inputs in fuzzing. The mutation process is formulated as\nthe sequence-to-sequence modeling, where LLM receives a sequence of bytes and\nthen outputs the mutated byte sequence. FuzzCoder is fine-tuned on the created\ninstruction dataset (Fuzz-Instruct), where the successful fuzzing history is\ncollected from the heuristic fuzzing tool. FuzzCoder can predict mutation\nlocations and strategies locations in input files to trigger abnormal behaviors\nof the program. Experimental results show that FuzzCoder based on AFL (American\nFuzzy Lop) gain significant improvements in terms of effective proportion of\nmutation (EPM) and number of crashes (NC) for various input formats including\nELF, JPG, MP3, and XML.', ""Greybox fuzzing has achieved success in revealing bugs and vulnerabilities in\nprograms. However, randomized mutation strategies have limited the fuzzer's\nperformance on structured data. Specialized fuzzers can handle complex\nstructured data, but require additional efforts in grammar and suffer from low\nthroughput.\n  In this paper, we explore the potential of utilizing the Large Language Model\nto enhance greybox fuzzing for structured data. We utilize the pre-trained\nknowledge of LLM about data conversion and format to generate new valid inputs.\nWe further fine-tuned it with paired mutation seeds to learn structured format\nand mutation strategies effectively. Our LLM-based fuzzer, LLAMAFUZZ,\nintegrates the power of LLM to understand and mutate structured data to\nfuzzing. We conduct experiments on the standard bug-based benchmark Magma and a\nwide variety of real-world programs. LLAMAFUZZ outperforms our top competitor\nby 41 bugs on average. We also identified 47 unique bugs across all trials.\nMoreover, LLAMAFUZZ demonstrated consistent performance on both bug trigger and\nbug reached. Compared to AFL++, LLAMAFUZZ achieved 27.19% more branches in\nreal-world program sets on average. We also demonstrate a case study to explain\nhow LLMs enhance the fuzzing process in terms of code coverage."", 'Greybox fuzzing is one of the most popular methods for detecting software\nvulnerabilities, which conducts a biased random search within the program input\nspace. To enhance its effectiveness in achieving deep coverage of program\nbehaviors, greybox fuzzing is often combined with concolic execution, which\nperforms a path-sensitive search over the domain of program inputs. In hybrid\nfuzzing, conventional greybox fuzzing is followed by concolic execution in an\niterative loop, where reachability roadblocks encountered by greybox fuzzing\nare tackled by concolic execution. However, such hybrid fuzzing still suffers\nfrom difficulties conventionally faced by symbolic execution, such as the need\nfor environment modeling and system call support. In this work, we show how to\nachieve the effect of concolic execution without having to compute and solve\nsymbolic path constraints. When coverage-based greybox fuzzing reaches a\nroadblock in terms of reaching certain branches, we conduct a slicing on the\nexecution trace and suggest modifications of the input to reach the relevant\nbranches. A Large Language Model (LLM) is used as a solver to generate the\nmodified input for reaching the desired branches. Compared with both the\nvanilla greybox fuzzer AFL and hybrid fuzzers Intriguer and Qsym, our LLM-based\nhybrid fuzzer HyLLfuzz (pronounced ""hill fuzz"") demonstrates superior coverage.\nFurthermore, the LLM-based concolic execution in HyLLfuzz takes a time that is\n4-19 times faster than the concolic execution running in existing hybrid\nfuzzing tools. This experience shows that LLMs can be effectively inserted into\nthe iterative loop of hybrid fuzzers, to efficiently expose more program\nbehaviors.']"
154,17,154_fashion_recommendation_multimodal_advertisement,"['fashion', 'recommendation', 'multimodal', 'advertisement', 'intents', 'personalized', 'item', 'user', 'outfits', 'molar']","[""With the increasing availability of multimodal data, many fields urgently\nrequire advanced architectures capable of effectively integrating these diverse\ndata sources to address specific problems. This study proposes a hybrid\nrecommendation model that combines the Mixture of Experts (MOE) framework with\nlarge language models to enhance the performance of recommendation systems in\nthe healthcare domain. We built a small dataset for recommending healthy food\nbased on patient descriptions and evaluated the model's performance on several\nkey metrics, including Precision, Recall, NDCG, and MAP@5. The experimental\nresults show that the hybrid model outperforms the baseline models, which use\nMOE or large language models individually, in terms of both accuracy and\npersonalized recommendation effectiveness. The paper finds image data provided\nrelatively limited improvement in the performance of the personalized\nrecommendation system, particularly in addressing the cold start problem. Then,\nthe issue of reclassification of images also affected the recommendation\nresults, especially when dealing with low-quality images or changes in the\nappearance of items, leading to suboptimal performance. The findings provide\nvaluable insights into the development of powerful, scalable, and\nhigh-performance recommendation systems, advancing the application of\npersonalized recommendation technologies in real-world domains such as\nhealthcare."", 'Personalized outfit recommendation remains a complex challenge, demanding\nboth fashion compatibility understanding and trend awareness. This paper\npresents a novel framework that harnesses the expressive power of large\nlanguage models (LLMs) for this task, mitigating their ""black box"" and static\nnature through fine-tuning and direct feedback integration. We bridge the item\nvisual-textual gap in items descriptions by employing image captioning with a\nMultimodal Large Language Model (MLLM). This enables the LLM to extract style\nand color characteristics from human-curated fashion images, forming the basis\nfor personalized recommendations. The LLM is efficiently fine-tuned on the\nopen-source Polyvore dataset of curated fashion images, optimizing its ability\nto recommend stylish outfits. A direct preference mechanism using negative\nexamples is employed to enhance the LLM\'s decision-making process. This creates\na self-enhancing AI feedback loop that continuously refines recommendations in\nline with seasonal fashion trends. Our framework is evaluated on the Polyvore\ndataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,\nand complementary item retrieval. These evaluations underline the framework\'s\nability to generate stylish, trend-aligned outfit suggestions, continuously\nimproving through direct feedback. The evaluation results demonstrated that our\nproposed framework significantly outperforms the base LLM, creating more\ncohesive outfits. The improved performance in these tasks underscores the\nproposed framework\'s potential to enhance the shopping experience with accurate\nsuggestions, proving its effectiveness over the vanilla LLM based outfit\ngeneration.', 'The fashion industry is one of the leading domains in the global e-commerce\nsector, prompting major online retailers to employ recommendation systems for\nproduct suggestions and customer convenience. While recommendation systems have\nbeen widely studied, most are designed for general e-commerce problems and\nstruggle with the unique challenges of the fashion domain. To address these\nissues, we propose a sequential fashion recommendation framework that leverages\na pre-trained large language model (LLM) enhanced with recommendation-specific\nprompts. Our framework employs parameter-efficient fine-tuning with extensive\nfashion data and introduces a novel mix-up-based retrieval technique for\ntranslating text into relevant product suggestions. Extensive experiments show\nour proposed framework significantly enhances fashion recommendation\nperformance.']"
155,17,155_chatbots_cultural_chatbot_ai,"['chatbots', 'cultural', 'chatbot', 'ai', 'false', 'cultures', 'memories', 'chatgpt', 'gpt', 'pronouns']","['In open-ended generative tasks like narrative writing or dialogue, large\nlanguage models often exhibit cultural biases, showing limited knowledge and\ngenerating templated outputs for less prevalent cultures. Recent works show\nthat these biases may stem from uneven cultural representation in pretraining\ncorpora. This work investigates how pretraining leads to biased\nculture-conditioned generations by analyzing how models associate entities with\ncultures based on pretraining data patterns. We propose the MEMOed framework\n(MEMOrization from pretraining document) to determine whether a generation for\na culture arises from memorization. Using MEMOed on culture-conditioned\ngenerations about food and clothing for 110 cultures, we find that\nhigh-frequency cultures in pretraining data yield more generations with\nmemorized symbols, while some low-frequency cultures produce none.\nAdditionally, the model favors generating entities with extraordinarily high\nfrequency regardless of the conditioned culture, reflecting biases toward\nfrequent pretraining terms irrespective of relevance. We hope that the MEMOed\nframework and our insights will inspire more works on attributing model\nperformance on pretraining data.', 'This article proposes a new integration of linguistic anthropology and\nmachine learning (ML) around convergent interests in both the underpinnings of\nlanguage and making language technologies more socially responsible. While\nlinguistic anthropology focuses on interpreting the cultural basis for human\nlanguage use, the ML field of interpretability is concerned with uncovering the\npatterns that Large Language Models (LLMs) learn from human verbal behavior.\nThrough the analysis of a conversation between a human user and an LLM-powered\nchatbot, we demonstrate the theoretical feasibility of a new, conjoint field of\ninquiry, cultural interpretability (CI). By focusing attention on the\ncommunicative competence involved in the way human users and AI chatbots\nco-produce meaning in the articulatory interface of human-computer interaction,\nCI emphasizes how the dynamic relationship between language and culture makes\ncontextually sensitive, open-ended conversation possible. We suggest that, by\nexamining how LLMs internally ""represent"" relationships between language and\nculture, CI can: (1) provide insight into long-standing linguistic\nanthropological questions about the patterning of those relationships; and (2)\naid model developers and interface designers in improving value alignment\nbetween language models and stylistically diverse speakers and culturally\ndiverse speech communities. Our discussion proposes three critical research\naxes: relativity, variation, and indexicality.', ""This study examines the impact of AI on human false memories -- recollections\nof events that did not occur or deviate from actual occurrences. It explores\nfalse memory induction through suggestive questioning in Human-AI interactions,\nsimulating crime witness interviews. Four conditions were tested: control,\nsurvey-based, pre-scripted chatbot, and generative chatbot using a large\nlanguage model (LLM). Participants (N=200) watched a crime video, then\ninteracted with their assigned AI interviewer or survey, answering questions\nincluding five misleading ones. False memories were assessed immediately and\nafter one week. Results show the generative chatbot condition significantly\nincreased false memory formation, inducing over 3 times more immediate false\nmemories than the control and 1.7 times more than the survey method. 36.4% of\nusers' responses to the generative chatbot were misled through the interaction.\nAfter one week, the number of false memories induced by generative chatbots\nremained constant. However, confidence in these false memories remained higher\nthan the control after one week. Moderating factors were explored: users who\nwere less familiar with chatbots but more familiar with AI technology, and more\ninterested in crime investigations, were more susceptible to false memories.\nThese findings highlight the potential risks of using advanced AI in sensitive\ncontexts, like police interviews, emphasizing the need for ethical\nconsiderations.""]"
156,17,156_radiology_reports_impressions_pe,"['radiology', 'reports', 'impressions', 'pe', 'report', 'tnm', 'conciseness', 'radiologists', 'clinical', 'fullystructured']","['Purpose: To develop and evaluate an automated system for extracting\nstructured clinical information from unstructured radiology and pathology\nreports using open-weights large language models (LMs) and retrieval augmented\ngeneration (RAG), and to assess the effects of model configuration variables on\nextraction performance. Methods and Materials: The study utilized two datasets:\n7,294 radiology reports annotated for Brain Tumor Reporting and Data System\n(BT-RADS) scores and 2,154 pathology reports annotated for isocitrate\ndehydrogenase (IDH) mutation status. An automated pipeline was developed to\nbenchmark the performance of various LMs and RAG configurations. The impact of\nmodel size, quantization, prompting strategies, output formatting, and\ninference parameters was systematically evaluated. Results: The best performing\nmodels achieved over 98% accuracy in extracting BT-RADS scores from radiology\nreports and over 90% for IDH mutation status extraction from pathology reports.\nThe top model being medical fine-tuned llama3. Larger, newer, and domain\nfine-tuned models consistently outperformed older and smaller models. Model\nquantization had minimal impact on performance. Few-shot prompting\nsignificantly improved accuracy. RAG improved performance for complex pathology\nreports but not for shorter radiology reports. Conclusions: Open LMs\ndemonstrate significant potential for automated extraction of structured\nclinical data from unstructured clinical reports with local privacy-preserving\napplication. Careful model selection, prompt engineering, and semi-automated\noptimization using annotated data are critical for optimal performance. These\napproaches could be reliable enough for practical use in research workflows,\nhighlighting the potential for human-machine collaboration in healthcare data\nextraction.', ""Evaluating generated radiology reports is crucial for the development of\nradiology AI, but existing metrics fail to reflect the task's clinical\nrequirements. This study proposes a novel evaluation framework using large\nlanguage models (LLMs) to compare radiology reports for assessment. We compare\nthe performance of various LLMs and demonstrate that, when using GPT-4, our\nproposed metric achieves evaluation consistency close to that of radiologists.\nFurthermore, to reduce costs and improve accessibility, making this method\npractical, we construct a dataset using LLM evaluation results and perform\nknowledge distillation to train a smaller model. The distilled model achieves\nevaluation capabilities comparable to GPT-4. Our framework and distilled model\noffer an accessible and efficient evaluation method for radiology report\ngeneration, facilitating the development of more clinically relevant models.\nThe model will be further open-sourced and accessible."", 'In recent years, the field of radiology has increasingly harnessed the power\nof artificial intelligence (AI) to enhance diagnostic accuracy, streamline\nworkflows, and improve patient care. Large language models (LLMs) have emerged\nas particularly promising tools, offering significant potential in assisting\nradiologists with report generation, clinical decision support, and patient\ncommunication. This paper presents an advanced radiology-focused large language\nmodel: MGH Radiology Llama. It is developed using the Llama 3 70B model,\nbuilding upon previous domain-specific models like Radiology-GPT and\nRadiology-Llama2. Leveraging a unique and comprehensive dataset from\nMassachusetts General Hospital, comprising over 6.5 million de-identified\nmedical reports across various imaging modalities, the model demonstrates\nsignificant improvements in generating accurate and clinically relevant\nradiology impressions given the corresponding findings. Our evaluation,\nincorporating both traditional metrics and a GPT-4-based assessment, highlights\nthe enhanced performance of this work over general-purpose LLMs.']"
157,17,157_attention_mamba_recurrent_linear,"['attention', 'mamba', 'recurrent', 'linear', 'transformers', 'softmax', 'mechanism', 'linearizing', 'gating', 'transformer']","['The Transformer architecture is widely deployed in many popular and impactful\nLarge Language Models. At its core is the attention mechanism for calculating\ncorrelations between pairs of tokens. Performing an attention computation takes\nquadratic time in the input size, and had become the time bottleneck for\ntransformer operations. In order to circumvent this, researchers have used a\nvariety of approaches, including designing heuristic algorithms for performing\nattention computations faster, and proposing alternatives to the attention\nmechanism which can be computed more quickly. For instance, state space models\nsuch as Mamba were designed to replace attention with an almost linear time\nalternative.\n  In this paper, we prove that any such approach cannot perform important tasks\nthat Transformer is able to perform (assuming a popular conjecture from\nfine-grained complexity theory). We focus on document similarity tasks, where\none is given as input many documents and would like to find a pair which is\n(approximately) the most similar. We prove that Transformer is able to perform\nthis task, and we prove that this task cannot be performed in truly\nsubquadratic time by any algorithm. Thus, any model which can be evaluated in\nsubquadratic time - whether because of subquadratic-time heuristics for\nattention, faster attention replacements like Mamba, or any other reason -\ncannot perform this task. In other words, in order to perform tasks that\n(implicitly or explicitly) involve document similarity, one may as well use\nTransformer and cannot avoid its quadratic running time.', 'Recent advancements in Large Language Models (LLMs) have set themselves apart\nwith their exceptional performance in complex language modelling tasks.\nHowever, these models are also known for their significant computational and\nstorage requirements, primarily due to the quadratic computation complexity of\nsoftmax attention. To mitigate this issue, linear attention has been designed\nto reduce the quadratic space-time complexity that is inherent in standard\ntransformers. In this work, we embarked on a comprehensive exploration of three\nkey components that substantially impact the performance of the Gated Linear\nAttention module: feature maps, normalization, and the gating mechanism. We\ndeveloped a feature mapping function to address some crucial issues that\nprevious suggestions overlooked. Then we offered further rationale for the\nintegration of normalization layers to stabilize the training process.\nMoreover, we explored the saturation phenomenon of the gating mechanism and\naugmented it with a refining module. We conducted extensive experiments and\nshowed our architecture outperforms previous Gated Linear Attention mechanisms\nin extensive tasks including training from scratch and post-linearization with\ncontinual pre-training.', 'Transformers with linear recurrent modeling offer linear-time training and\nconstant-memory inference. Despite their demonstrated efficiency and\nperformance, pretraining such non-standard architectures from scratch remains\ncostly and risky. The linearization of large language models (LLMs) transforms\npretrained standard models into linear recurrent structures, enabling more\nefficient deployment. However, current linearization methods typically\nintroduce additional feature map modules that require extensive fine-tuning and\noverlook the gating mechanisms used in state-of-the-art linear recurrent\nmodels. To address these issues, this paper presents Liger, short for\nLinearizing LLMs to gated recurrent structures. Liger is a novel approach for\nconverting pretrained LLMs into gated linear recurrent models without adding\nextra parameters. It repurposes the pretrained key matrix weights to construct\ndiverse gating mechanisms, facilitating the formation of various gated\nrecurrent structures while avoiding the need to train additional components\nfrom scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA),\nLiger restores the performance of the linearized gated recurrent models to\nmatch that of the original LLMs. Additionally, we introduce Liger Attention, an\nintra-layer hybrid attention mechanism, which significantly recovers 93\\% of\nthe Transformer-based LLM at 0.02\\% pre-training tokens during the\nlinearization process, achieving competitive results across multiple\nbenchmarks, as validated on models ranging from 1B to 8B parameters. Code is\navailable at https://github.com/OpenSparseLLMs/Linearization.']"
158,16,158_honesty_deceptive_deception_markers,"['honesty', 'deceptive', 'deception', 'markers', 'epistemic', 'honest', 'truthful', 'dishonesty', 'misleading', 'dishonest']","[""Large Language Models (LLMs) have achieved remarkable success across various\nindustries due to their exceptional generative capabilities. However, for safe\nand effective real-world deployments, ensuring honesty and helpfulness is\ncritical. This paper addresses the question: Can we prioritize the helpfulness\nof LLMs while preserving their honesty? To begin with, we establish exhaustive\nprinciples aimed at guaranteeing the honesty of LLM. Additionally, we introduce\na novel dataset, referred to as HoneSet, comprising 930 queries spanning six\ncategories meticulously crafted to assess an LLM's capacity for maintaining\nhonesty. Subsequently, we present two approaches to augmenting honesty and\nhelpfulness in LLMs: a training-free enhancement and a fine-tuning-based\nimprovement. The training-free approach, which is based on curiosity-driven\nprompting, empowers LLMs to articulate internal confusion and uncertainty\nregarding queries, thereby optimizing their responses. Conversely, the\nfine-tuning-based method employs a two-stage process inspired by curriculum\nlearning: initially instructing LLMs to discern between honest and dishonest\nresponses, then refining their training to enhance helpfulness. Experiments\nconducted on nine prominent LLMs demonstrate a significant improvement in\nalignment with honesty across all models through the implementation of our\nproposed enhancements. Particularly noteworthy is the 65.3% enhancement\nobserved in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as\nmeasured by the H$^{2}$ (honest and helpful) assessment. We believe that our\nwork can pave the way for developing more trustworthy LLMs for real-world\napplications."", 'Previous works on Large Language Models (LLMs) have mainly focused on\nevaluating their helpfulness or harmlessness. However, honesty, another crucial\nalignment criterion, has received relatively less attention. Dishonest\nbehaviors in LLMs, such as spreading misinformation and defrauding users,\npresent severe risks that intensify as these models approach superintelligent\nlevels. Enhancing honesty in LLMs addresses critical limitations and helps\nuncover latent capabilities that are not readily expressed. This underscores\nthe urgent need for reliable methods and benchmarks to effectively ensure and\nevaluate the honesty of LLMs.\n  In this paper, we introduce BeHonest, a pioneering benchmark specifically\ndesigned to assess honesty in LLMs comprehensively. BeHonest evaluates three\nessential aspects of honesty: awareness of knowledge boundaries, avoidance of\ndeceit, and consistency in responses. Building on this foundation, we designed\n10 scenarios to evaluate and analyze 9 popular LLMs on the market, including\nboth closed-source and open-source models from different model families with\nvaried model sizes. Our findings indicate that there is still significant room\nfor improvement in the honesty of LLMs. We encourage the AI community to\nprioritize honesty alignment in these models, which can harness their full\npotential to benefit society while preventing them from causing harm through\ndeception or inconsistency. Our benchmark and code can be found at:\n\\url{https://github.com/GAIR-NLP/BeHonest}.', 'As large language models (LLMs) become more capable and agentic, the\nrequirement for trust in their outputs grows significantly, yet at the same\ntime concerns have been mounting that models may learn to lie in pursuit of\ntheir goals. To address these concerns, a body of work has emerged around the\nnotion of ""honesty"" in LLMs, along with interventions aimed at mitigating\ndeceptive behaviors. However, evaluations of honesty are currently highly\nlimited, with no benchmark combining large scale and applicability to all\nmodels. Moreover, many benchmarks claiming to measure honesty in fact simply\nmeasure accuracy--the correctness of a model\'s beliefs--in disguise. In this\nwork, we introduce a large-scale human-collected dataset for measuring honesty\ndirectly, allowing us to disentangle accuracy from honesty for the first time.\nAcross a diverse set of LLMs, we find that while larger models obtain higher\naccuracy on our benchmark, they do not become more honest. Surprisingly, while\nmost frontier LLMs obtain high scores on truthfulness benchmarks, we find a\nsubstantial propensity in frontier LLMs to lie when pressured to do so,\nresulting in low honesty scores on our benchmark. We find that simple methods,\nsuch as representation engineering interventions, can improve honesty. These\nresults underscore the growing need for robust evaluations and effective\ninterventions to ensure LLMs remain trustworthy.']"
159,16,159_circuit_analog_circuits_design,"['circuit', 'analog', 'circuits', 'design', 'netlists', 'bo', 'ams', 'schematics', 'topology', 'netlist']","[""Analog circuit design requires substantial human expertise and involvement,\nwhich is a significant roadblock to design productivity. Bayesian Optimization\n(BO), a popular machine learning based optimization strategy, has been\nleveraged to automate analog design given its applicability across various\ncircuit topologies and technologies. Traditional BO methods employ black box\nGaussian Process surrogate models and optimized labeled data queries to find\noptimization solutions by trading off between exploration and exploitation.\nHowever, the search for the optimal design solution in BO can be expensive from\nboth a computational and data usage point of view, particularly for high\ndimensional optimization problems. This paper presents ADO-LLM, the first work\nintegrating large language models (LLMs) with Bayesian Optimization for analog\ndesign optimization. ADO-LLM leverages the LLM's ability to infuse domain\nknowledge to rapidly generate viable design points to remedy BO's inefficiency\nin finding high value design areas specifically under the limited design space\ncoverage of the BO's probabilistic surrogate model. In the meantime, sampling\nof design points evaluated in the iterative BO process provides quality\ndemonstrations for the LLM to generate high quality design points while\nleveraging infused broad design knowledge. Furthermore, the diversity brought\nby BO's exploration enriches the contextual understanding of the LLM and allows\nit to more broadly search in the design space and prevent repetitive and\nredundant suggestions. We evaluate the proposed framework on two different\ntypes of analog circuits and demonstrate notable improvements in design\nefficiency and effectiveness."", 'Analog circuit design is a significant task in modern chip technology,\nfocusing on the selection of component types, connectivity, and parameters to\nensure proper circuit functionality. Despite advances made by Large Language\nModels (LLMs) in digital circuit design, the complexity and scarcity of data in\nanalog circuitry pose significant challenges. To mitigate these issues, we\nintroduce AnalogCoder, the first training-free LLM agent for designing analog\ncircuits through Python code generation. Firstly, AnalogCoder incorporates a\nfeedback-enhanced flow with tailored domain-specific prompts, enabling the\nautomated and self-correcting design of analog circuits with a high success\nrate. Secondly, it proposes a circuit tool library to archive successful\ndesigns as reusable modular sub-circuits, simplifying composite circuit\ncreation. Thirdly, extensive experiments on a benchmark designed to cover a\nwide range of analog circuit tasks show that AnalogCoder outperforms other\nLLM-based methods. It has successfully designed 20 circuits, 5 more than\nstandard GPT-4o. We believe AnalogCoder can significantly improve the\nlabor-intensive chip design process, enabling non-experts to design analog\ncircuits efficiently.', 'High-performance analog and mixed-signal (AMS) circuits are mainly\nfull-custom designed, which is time-consuming and labor-intensive. A\nsignificant portion of the effort is experience-driven, which makes the\nautomation of AMS circuit design a formidable challenge. Large language models\n(LLMs) have emerged as powerful tools for Electronic Design Automation (EDA)\napplications, fostering advancements in the automatic design process for\nlarge-scale AMS circuits. However, the absence of high-quality datasets has led\nto issues such as model hallucination, which undermines the robustness of\nautomatically generated circuit designs. To address this issue, this paper\nintroduces AMSnet-KG, a dataset encompassing various AMS circuit schematics and\nnetlists. We construct a knowledge graph with annotations on detailed\nfunctional and performance characteristics. Facilitated by AMSnet-KG, we\npropose an automated AMS circuit generation framework that utilizes the\ncomprehensive knowledge embedded in LLMs. We first formulate a design strategy\n(e.g., circuit architecture using a number of circuit components) based on\nrequired specifications. Next, matched circuit components are retrieved and\nassembled into a complete topology, and transistor sizing is obtained through\nBayesian optimization. Simulation results of the netlist are fed back to the\nLLM for further topology refinement, ensuring the circuit design specifications\nare met. We perform case studies of operational amplifier and comparator design\nto verify the automatic design flow from specifications to netlists with\nminimal human effort. The dataset used in this paper will be open-sourced upon\npublishing of this paper.']"
160,16,160_reward_rl_credit_reinforcement,"['reward', 'rl', 'credit', 'reinforcement', 'assignment', 'rewards', 'acing', 'icrl', 'learning', 'bus']","[""Bus holding control is a widely-adopted strategy for maintaining stability\nand improving the operational efficiency of bus systems. Traditional\nmodel-based methods often face challenges with the low accuracy of bus state\nprediction and passenger demand estimation. In contrast, Reinforcement Learning\n(RL), as a data-driven approach, has demonstrated great potential in\nformulating bus holding strategies. RL determines the optimal control\nstrategies in order to maximize the cumulative reward, which reflects the\noverall control goals. However, translating sparse and delayed control goals in\nreal-world tasks into dense and real-time rewards for RL is challenging,\nnormally requiring extensive manual trial-and-error. In view of this, this\nstudy introduces an automatic reward generation paradigm by leveraging the\nin-context learning and reasoning capabilities of Large Language Models (LLMs).\nThis new paradigm, termed the LLM-enhanced RL, comprises several LLM-based\nmodules: reward initializer, reward modifier, performance analyzer, and reward\nrefiner. These modules cooperate to initialize and iteratively improve the\nreward function according to the feedback from training and test results for\nthe specified RL-based task. Ineffective reward functions generated by the LLM\nare filtered out to ensure the stable evolution of the RL agents' performance\nover iterations. To evaluate the feasibility of the proposed LLM-enhanced RL\nparadigm, it is applied to various bus holding control scenarios, including a\nsynthetic single-line system and a real-world multi-line system. The results\ndemonstrate the superiority and robustness of the proposed paradigm compared to\nvanilla RL strategies, the LLM-based controller, and conventional space\nheadway-based feedback control. This study sheds light on the great potential\nof utilizing LLMs in various smart mobility applications."", 'Reinforcement learning (RL) often encounters delayed and sparse feedback in\nreal-world applications, even with only episodic rewards. Previous approaches\nhave made some progress in reward redistribution for credit assignment but\nstill face challenges, including training difficulties due to redundancy and\nambiguous attributions stemming from overlooking the multifaceted nature of\nmission performance evaluation. Hopefully, Large Language Model (LLM)\nencompasses fruitful decision-making knowledge and provides a plausible tool\nfor reward redistribution. Even so, deploying LLM in this case is non-trivial\ndue to the misalignment between linguistic knowledge and the symbolic form\nrequirement, together with inherent randomness and hallucinations in inference.\nTo tackle these issues, we introduce LaRe, a novel LLM-empowered symbolic-based\ndecision-making framework, to improve credit assignment. Key to LaRe is the\nconcept of the Latent Reward, which works as a multi-dimensional performance\nevaluation, enabling more interpretable goal attainment from various\nperspectives and facilitating more effective reward redistribution. We examine\nthat semantically generated code from LLM can bridge linguistic knowledge and\nsymbolic latent rewards, as it is executable for symbolic objects. Meanwhile,\nwe design latent reward self-verification to increase the stability and\nreliability of LLM inference. Theoretically, reward-irrelevant redundancy\nelimination in the latent reward benefits RL performance from more accurate\nreward estimation. Extensive experimental results witness that LaRe (i)\nachieves superior temporal credit assignment to SOTA methods, (ii) excels in\nallocating contributions among multiple agents, and (iii) outperforms policies\ntrained with ground truth rewards for certain tasks.', 'The temporal credit assignment problem is a central challenge in\nReinforcement Learning (RL), concerned with attributing the appropriate\ninfluence to each actions in a trajectory for their ability to achieve a goal.\nHowever, when feedback is delayed and sparse, the learning signal is poor, and\naction evaluation becomes harder. Canonical solutions, such as reward shaping\nand options, require extensive domain knowledge and manual intervention,\nlimiting their scalability and applicability. In this work, we lay the\nfoundations for Credit Assignment with Language Models (CALM), a novel approach\nthat leverages Large Language Models (LLMs) to automate credit assignment via\nreward shaping and options discovery. CALM uses LLMs to decompose a task into\nelementary subgoals and assess the achievement of these subgoals in\nstate-action transitions. Every time an option terminates, a subgoal is\nachieved, and CALM provides an auxiliary reward. This additional reward signal\ncan enhance the learning process when the task reward is sparse and delayed\nwithout the need for human-designed rewards. We provide a preliminary\nevaluation of CALM using a dataset of human-annotated demonstrations from\nMiniHack, suggesting that LLMs can be effective in assigning credit in\nzero-shot settings, without examples or LLM fine-tuning. Our preliminary\nresults indicate that the knowledge of LLMs is a promising prior for credit\nassignment in RL, facilitating the transfer of human knowledge into value\nfunctions.']"
161,16,161_ideas_scientific_research_idea,"['ideas', 'scientific', 'research', 'idea', 'literature', 'papers', 'researchers', 'academic', 'scideator', 'novelty']","['Effective research ideation is a critical step for scientific research.\nHowever, the exponential increase in scientific literature makes it challenging\nfor researchers to stay current with recent advances and identify meaningful\nresearch directions. Recent developments in large language models~(LLMs)\nsuggest a promising avenue for automating the generation of novel research\nideas. However, existing methods for idea generation either trivially prompt\nLLMs or directly expose LLMs to extensive literature without indicating useful\ninformation. Inspired by the research process of human researchers, we propose\na Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant\nliterature in a chain structure to effectively mirror the progressive\ndevelopment in a research domain. This organization facilitates LLMs to capture\nthe current advancements in research, thereby enhancing their ideation\ncapabilities. Furthermore, we propose Idea Arena, an evaluation protocol that\ncan comprehensively evaluate idea generation methods from different\nperspectives, aligning closely with the preferences of human researchers.\nExperimental results indicate that the CoI agent consistently outperforms other\nmethods and shows comparable quality as humans in research idea generation.\nMoreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to\ngenerate a candidate idea and its corresponding experimental design.', 'Large Language Models (LLMs) have transformed how people interact with\nartificial intelligence (AI) systems, achieving state-of-the-art results in\nvarious tasks, including scientific discovery and hypothesis generation.\nHowever, the lack of a comprehensive and systematic evaluation framework for\ngenerating research ideas using LLMs poses a significant obstacle to\nunderstanding and assessing their generative capabilities in scientific\ndiscovery. To address this gap, we propose IdeaBench, a benchmark system that\nincludes a comprehensive dataset and an evaluation framework for standardizing\nthe assessment of research idea generation using LLMs. Our dataset comprises\ntitles and abstracts from a diverse range of influential papers, along with\ntheir referenced works. To emulate the human process of generating research\nideas, we profile LLMs as domain-specific researchers and ground them in the\nsame context considered by human researchers. This maximizes the utilization of\nthe LLMs\' parametric knowledge to dynamically generate new research ideas. We\nalso introduce an evaluation framework for assessing the quality of generated\nresearch ideas. Our evaluation framework is a two-stage process: first, using\nGPT-4o to rank ideas based on user-specified quality indicators such as novelty\nand feasibility, enabling scalable personalization; and second, calculating\nrelative ranking based ""Insight Score"" to quantify the chosen quality\nindicator. The proposed benchmark system will be a valuable asset for the\ncommunity to measure and compare different LLMs, ultimately advancing the\nautomation of the scientific discovery process.', 'Reading relevant scientific papers and analyzing research development trends\nis a critical step in generating new scientific ideas. However, the rapid\nincrease in the volume of research literature and the complex citation\nrelationships make it difficult for researchers to quickly analyze and derive\nmeaningful research trends. The development of large language models (LLMs) has\nprovided a novel approach for automatically summarizing papers and generating\ninnovative research ideas. However, existing paper-based idea generation\nmethods either simply input papers into LLMs via prompts or form logical chains\nof creative development based on citation relationships, without fully\nexploiting the semantic information embedded in these citations. Inspired by\nknowledge graphs and human cognitive processes, we propose a framework called\nthe Graph of AI Ideas (GoAI) for the AI research field, which is dominated by\nopen-access papers. This framework organizes relevant literature into entities\nwithin a knowledge graph and summarizes the semantic information contained in\ncitations into relations within the graph. This organization effectively\nreflects the relationships between two academic papers and the advancement of\nthe AI research field. Such organization aids LLMs in capturing the current\nprogress of research, thereby enhancing their creativity. Experimental results\ndemonstrate the effectiveness of our approach in generating novel, clear, and\neffective research ideas.']"
162,16,162_game_narrative_pangea_de,"['game', 'narrative', 'pangea', 'de', 'gricean', 'rpg', 'player', 'freeform', 'character', 'games']","[""Role-playing games (RPGs) provide players with a rich, interactive world to\nexplore. Dialogue serves as the primary means of communication between\ndevelopers and players, manifesting in various forms such as guides, NPC\ninteractions, and storytelling. While most games rely on written scripts to\ndefine the main story and character personalities, player immersion can be\nsignificantly enhanced through casual interactions between characters. With the\nadvent of large language models (LLMs), we introduce a dialogue filler\nframework that utilizes LLMs enhanced by knowledge graphs to generate dynamic\nand contextually appropriate character interactions. We test this framework\nwithin the environments of Final Fantasy VII Remake and Pokemon, providing\nqualitative and quantitative evidence that demonstrates GPT-4's capability to\nact with defined personalities and generate dialogue. However, some flaws\nremain, such as GPT-4 being overly positive or more subtle personalities, such\nas maturity, tend to be of lower quality compared to more overt traits like\ntimidity. This study aims to assist developers in crafting more nuanced filler\ndialogues, thereby enriching player immersion and enhancing the overall RPG\nexperience."", ""Large language models, such as the well-known ChatGPT, have brought about an\nunexpected revolution in the field of artificial intelligence. On the one hand,\nthey have numerous practical applications and enormous potential still to be\nexplored. On the other hand, they are also the subject of debate from\nscientific, philosophical, and social perspectives: there are doubts about the\nexact mechanisms of their functioning and their actual capacity for language\ncomprehension, and their applications raise ethical dilemmas. In this chapter,\nwe describe how this technology has been developed and the fundamentals of its\noperation, allowing us to better understand its capabilities and limitations\nand to introduce some of the main debates surrounding its development and use.\n  --\n  Los grandes modelos de lenguaje, como el conocido ChatGPT, han supuesto una\ninesperada revoluci\\'on en el \\'ambito de la inteligencia artificial. Por un\nlado, cuentan con multitud de aplicaciones pr\\'acticas y un enorme potencial\ntodav\\'ia por explorar. Por otro lado, son tambi\\'en objeto de debate, tanto\ndesde el punto de vista cient\\'ifico y filos\\'ofico como social: hay dudas\nsobre los mecanismos exactos de su funcionamiento y su capacidad real de\ncomprensi\\'on del lenguaje, y sus aplicaciones plantean dilemas \\'eticos. En\neste cap\\'itulo describimos c\\'omo se ha llegado a esta tecnolog\\'ia y los\nfundamentos de su funcionamiento, permiti\\'endonos as\\'i comprender mejor sus\ncapacidades y limitaciones e introducir algunos de los principales debates que\nrodean su desarrollo y uso."", ""This research introduces Procedural Artificial Narrative using Generative AI\n(PANGeA), a structured approach for leveraging large language models (LLMs),\nguided by a game designer's high-level criteria, to generate narrative content\nfor turn-based role-playing video games (RPGs). Distinct from prior\napplications of LLMs used for video game design, PANGeA innovates by not only\ngenerating game level data (which includes, but is not limited to, setting, key\nitems, and non-playable characters (NPCs)), but by also fostering dynamic,\nfree-form interactions between the player and the environment that align with\nthe procedural game narrative. The NPCs generated by PANGeA are\npersonality-biased and express traits from the Big 5 Personality Model in their\ngenerated responses. PANGeA addresses challenges behind ingesting free-form\ntext input, which can prompt LLM responses beyond the scope of the game\nnarrative. A novel validation system that uses the LLM's intelligence evaluates\ntext input and aligns generated responses with the unfolding narrative. Making\nthese interactions possible, PANGeA is supported by a server that hosts a\ncustom memory system that supplies context for augmenting generated responses\nthus aligning them with the procedural narrative. For its broad application,\nthe server has a REST interface enabling any game engine to integrate directly\nwith PANGeA, as well as an LLM interface adaptable with local or private LLMs.\nPANGeA's ability to foster dynamic narrative generation by aligning responses\nwith the procedural narrative is demonstrated through an empirical study and\nablation test of two versions of a demo game. These are, a custom,\nbrowser-based GPT and a Unity demo. As the results show, PANGeA holds potential\nto assist game designers in using LLMs to generate narrative-consistent content\neven when provided varied and unpredictable, free-form text input.""]"
163,15,163_layers_jetmoe8b_networks_llmbraces,"['layers', 'jetmoe8b', 'networks', 'llmbraces', 'transformer', 'attnchecker', 'neuroscience', 'neural', 'transformercot', 'subupdates']","['As artificial intelligence models have exploded in scale and capability,\nunderstanding of their internal mechanisms remains a critical challenge.\nInspired by the success of dynamical systems approaches in neuroscience, here\nwe propose a novel framework for studying computations in deep learning\nsystems. We focus on the residual stream (RS) in transformer models,\nconceptualizing it as a dynamical system evolving across layers. We find that\nactivations of individual RS units exhibit strong continuity across layers,\ndespite the RS being a non-privileged basis. Activations in the RS accelerate\nand grow denser over layers, while individual units trace unstable periodic\norbits. In reduced-dimensional spaces, the RS follows a curved trajectory with\nattractor-like dynamics in the lower layers. These insights bridge dynamical\nsystems theory and mechanistic interpretability, establishing a foundation for\na ""neuroscience of AI"" that combines theoretical rigor with large-scale data\nanalysis to advance our understanding of modern neural networks.', 'Landmark universal function approximation results for neural networks with\ntrained weights and biases provided the impetus for the ubiquitous use of\nneural networks as learning models in neuroscience and Artificial Intelligence\n(AI). Recent work has extended these results to networks in which a smaller\nsubset of weights (e.g., output weights) are tuned, leaving other parameters\nrandom. However, it remains an open question whether universal approximation\nholds when only biases are learned, despite evidence from neuroscience and AI\nthat biases significantly shape neural responses. The current paper answers\nthis question. We provide theoretical and numerical evidence demonstrating that\nfeedforward neural networks with fixed random weights can approximate any\ncontinuous function on compact sets. We further show an analogous result for\nthe approximation of dynamical systems with recurrent neural networks. Our\nfindings are relevant to neuroscience, where they demonstrate the potential for\nbehaviourally relevant changes in dynamics without modifying synaptic weights,\nas well as for AI, where they shed light on recent fine-tuning methods for\nlarge language models, like bias and prefix-based approaches.', 'Most work treats large language models as black boxes without in-depth\nunderstanding of their internal working mechanism. In order to explain the\ninternal representations of LLMs, we propose a gradient-based metric to assess\nthe activation level of model parameters. Based on this metric, we obtain three\npreliminary findings. (1) When the inputs are in the same domain, parameters in\nthe shallow layers will be activated densely, which means a larger portion of\nparameters will have great impacts on the outputs. In contrast, parameters in\nthe deep layers are activated sparsely. (2) When the inputs are across\ndifferent domains, parameters in shallow layers exhibit higher similarity in\nthe activation behavior than deep layers. (3) In deep layers, the similarity of\nthe distributions of activated parameters is positively correlated to the\nempirical data relevance. Further, we develop three validation experiments to\nsolidify these findings. (1) Firstly, starting from the first finding, we\nattempt to configure different prune ratios for different layers, and find this\nmethod can benefit model pruning. (2) Secondly, we find that a pruned model\nbased on one calibration set can better handle tasks related to the calibration\ntask than those not related, which validate the second finding. (3) Thirdly,\nBased on the STS-B and SICK benchmark, we find that two sentences with\nconsistent semantics tend to share similar parameter activation patterns in\ndeep layers, which aligns with our third finding. Our work sheds light on the\nbehavior of parameter activation in LLMs, and we hope these findings will have\nthe potential to inspire more practical applications.']"
164,15,164_rag_attack_poisoned_attacks,"['rag', 'attack', 'poisoned', 'attacks', 'passages', 'backdoor', 'adversarial', 'retrieval', 'blackbox', 'poisoning']","[""Retrieval-Augmented Generation (RAG) has been empirically shown to enhance\nthe performance of large language models (LLMs) in knowledge-intensive domains\nsuch as healthcare, finance, and legal contexts. Given a query, RAG retrieves\nrelevant documents from a corpus and integrates them into the LLMs' generation\nprocess. In this study, we investigate the adversarial robustness of RAG,\nfocusing specifically on examining the retrieval system. First, across 225\ndifferent setup combinations of corpus, retriever, query, and targeted\ninformation, we show that retrieval systems are vulnerable to universal\npoisoning attacks in medical Q\\&A. In such attacks, adversaries generate\npoisoned documents containing a broad spectrum of targeted information, such as\npersonally identifiable information. When these poisoned documents are inserted\ninto a corpus, they can be accurately retrieved by any users, as long as\nattacker-specified queries are used. To understand this vulnerability, we\ndiscovered that the deviation from the query's embedding to that of the\npoisoned document tends to follow a pattern in which the high similarity\nbetween the poisoned document and the query is retained, thereby enabling\nprecise retrieval. Based on these findings, we develop a new detection-based\ndefense to ensure the safe use of RAG. Through extensive experiments spanning\nvarious Q\\&A domains, we observed that our proposed method consistently\nachieves excellent detection rates in nearly all cases."", ""Despite significant advancements, large language models (LLMs) still struggle\nwith providing accurate answers when lacking domain-specific or up-to-date\nknowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by\nincorporating external knowledge bases, but it also introduces new attack\nsurfaces. In this paper, we investigate data extraction attacks targeting RAG's\nknowledge databases. We show that previous prompt injection-based extraction\nattacks largely rely on the instruction-following capabilities of LLMs. As a\nresult, they fail on models that are less responsive to such malicious prompts\n-- for example, our experiments show that state-of-the-art attacks achieve\nnear-zero success on Gemma-2B-IT. Moreover, even for models that can follow\nthese instructions, we found fine-tuning may significantly reduce attack\nperformance. To further reveal the vulnerability, we propose to backdoor RAG,\nwhere a small portion of poisoned data is injected during the fine-tuning phase\nto create a backdoor within the LLM. When this compromised LLM is integrated\ninto a RAG system, attackers can exploit specific triggers in prompts to\nmanipulate the LLM to leak documents from the retrieval database. By carefully\ndesigning the poisoned data, we achieve both verbatim and paraphrased document\nextraction. For example, on Gemma-2B-IT, we show that with only 5\\% poisoned\ndata, our method achieves an average success rate of 94.1\\% for verbatim\nextraction (ROUGE-L score: 82.1) and 63.6\\% for paraphrased extraction (average\nROUGE score: 66.4) across four datasets. These results underscore the privacy\nrisks associated with the supply chain when deploying RAG systems."", 'Large Language Models (LLMs) are constrained by outdated information and a\ntendency to generate incorrect data, commonly referred to as ""hallucinations.""\nRetrieval-Augmented Generation (RAG) addresses these limitations by combining\nthe strengths of retrieval-based methods and generative models. This approach\ninvolves retrieving relevant information from a large, up-to-date dataset and\nusing it to enhance the generation process, leading to more accurate and\ncontextually appropriate responses. Despite its benefits, RAG introduces a new\nattack surface for LLMs, particularly because RAG databases are often sourced\nfrom public data, such as the web. In this paper, we propose \\TrojRAG{} to\nidentify the vulnerabilities and attacks on retrieval parts (RAG database) and\ntheir indirect attacks on generative parts (LLMs). Specifically, we identify\nthat poisoning several customized content passages could achieve a retrieval\nbackdoor, where the retrieval works well for clean queries but always returns\ncustomized poisoned adversarial queries. Triggers and poisoned passages can be\nhighly customized to implement various attacks. For example, a trigger could be\na semantic group like ""The Republican Party, Donald Trump, etc."" Adversarial\npassages can be tailored to different contents, not only linked to the triggers\nbut also used to indirectly attack generative LLMs without modifying them.\nThese attacks can include denial-of-service attacks on RAG and semantic\nsteering attacks on LLM generations conditioned by the triggers. Our\nexperiments demonstrate that by just poisoning 10 adversarial passages can\ninduce 98.2\\% success rate to retrieve the adversarial passages. Then, these\npassages can increase the reject ratio of RAG-based GPT-4 from 0.01\\% to 74.6\\%\nor increase the rate of negative responses from 0.22\\% to 72\\% for targeted\nqueries.']"
165,15,165_scholarly_scientific_orkg_publications,"['scholarly', 'scientific', 'orkg', 'publications', 'literature', 'research', 'papers', 'openresearcher', 'properties', 'science']","[""Structured science summaries or research contributions using properties or\ndimensions beyond traditional keywords enhances science findability. Current\nmethods, such as those used by the Open Research Knowledge Graph (ORKG),\ninvolve manually curating properties to describe research papers' contributions\nin a structured manner, but this is labor-intensive and inconsistent between\nthe domain expert human curators. We propose using Large Language Models (LLMs)\nto automatically suggest these properties. However, it's essential to assess\nthe readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before\napplication. Our study performs a comprehensive comparative analysis between\nORKG's manually curated properties and those generated by the aforementioned\nstate-of-the-art LLMs. We evaluate LLM performance through four unique\nperspectives: semantic alignment and deviation with ORKG properties,\nfine-grained properties mapping accuracy, SciNCL embeddings-based cosine\nsimilarity, and expert surveys comparing manual annotations with LLM outputs.\nThese evaluations occur within a multidisciplinary science setting. Overall,\nLLMs show potential as recommendation systems for structuring science, but\nfurther finetuning is recommended to improve their alignment with scientific\ntasks and mimicry of human expertise."", ""Current research highlights the great potential of Large Language Models\n(LLMs) for constructing Scholarly Knowledge Graphs (SKGs). One particularly\ncomplex step in this process is relation extraction, aimed at identifying\nsuitable properties to describe the content of research. This study builds\ndirectly on previous research of three Open Research Knowledge Graph (ORKG)\nteam members who assessed the readiness of LLMs such as GPT-3.5, Llama 2, and\nMistral for property extraction in scientific literature. Given the moderate\nperformance observed, the previous work concluded that fine-tuning is needed to\nimprove these models' alignment with scientific tasks and their emulation of\nhuman expertise. Expanding on this prior experiment, this study evaluates the\nimpact of advanced prompt engineering techniques and demonstrates that these\ntechniques can highly significantly enhance the results. Additionally, this\nstudy extends the property extraction process to include property matching to\nexisting ORKG properties, which are retrieved via the API. The evaluation\nreveals that results generated through advanced prompt engineering achieve a\nhigher proportion of matches with ORKG properties, further emphasizing the\nenhanced alignment achieved. Moreover, this lays the groundwork for addressing\nchallenges such as the inconsistency of ORKG properties, an issue highlighted\nin prior studies. By assigning unique URIs and using standardized terminology,\nthis work increases the consistency of the properties, fulfilling a crucial\naspect of Linked Data and FAIR principles - core commitments of ORKG. This, in\nturn, significantly enhances the applicability of ORKG content for subsequent\ntasks such as comparisons of research publications. Finally, the study\nconcludes with recommendations for future improvements in the overall property\nextraction process."", 'The increasing amount of published scholarly articles, exceeding 2.5 million\nyearly, raises the challenge for researchers in following scientific progress.\nIntegrating the contributions from scholarly articles into a novel type of\ncognitive knowledge graph (CKG) will be a crucial element for accessing and\norganizing scholarly knowledge, surpassing the insights provided by titles and\nabstracts. This research focuses on effectively conveying structured scholarly\nknowledge by utilizing large language models (LLMs) to categorize scholarly\narticles and describe their contributions in a structured and comparable\nmanner. While previous studies explored language models within specific\nresearch domains, the extensive domain-independent knowledge captured by LLMs\noffers a substantial opportunity for generating structured contribution\ndescriptions as CKGs. Additionally, LLMs offer customizable pathways through\nprompt engineering or fine-tuning, thus facilitating to leveraging of smaller\nLLMs known for their efficiency, cost-effectiveness, and environmental\nconsiderations. Our methodology involves harnessing LLM knowledge, and\ncomplementing it with domain expert-verified scholarly data sourced from a CKG.\nThis strategic fusion significantly enhances LLM performance, especially in\ntasks like scholarly article categorization and predicate recommendation. Our\nmethod involves fine-tuning LLMs with CKG knowledge and additionally injecting\nknowledge from a CKG with a novel prompting technique significantly increasing\nthe accuracy of scholarly knowledge extraction. We integrated our approach in\nthe Open Research Knowledge Graph (ORKG), thus enabling precise access to\norganized scholarly knowledge, crucially benefiting domain-independent\nscholarly knowledge exchange and dissemination among policymakers, industrial\npractitioners, and the general public.']"
166,15,166_ecg_cardiac_ecgs_clinical,"['ecg', 'cardiac', 'ecgs', 'clinical', 'interpretation', 'heart', 'diagnostic', 'ef', 'reports', 'medical']","['The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for\nassessing cardiac conditions. Existing automatic interpretation methods suffer\nfrom limited generalizability, focusing on a narrow range of cardiac\nconditions, and typically depend on raw physiological signals, which may not be\nreadily available in resource-limited settings where only printed or digital\nECG images are accessible. Recent advancements in multimodal large language\nmodels (MLLMs) present promising opportunities for addressing these challenges.\nHowever, the application of MLLMs to ECG image interpretation remains\nchallenging due to the lack of instruction tuning datasets and well-established\nECG image benchmarks for quantitative evaluation. To address these challenges,\nwe introduce ECGInstruct, a comprehensive ECG image instruction tuning dataset\nof over one million samples, covering a wide range of ECG-related tasks from\ndiverse data sources. Using ECGInstruct, we develop PULSE, an MLLM tailored for\nECG image comprehension. In addition, we curate ECGBench, a new evaluation\nbenchmark covering four key ECG image interpretation tasks across nine\ndifferent datasets. Our experiments show that PULSE sets a new\nstate-of-the-art, outperforming general MLLMs with an average accuracy\nimprovement of 15% to 30%. This work highlights the potential of PULSE to\nenhance ECG interpretation in clinical practice.', 'Electrocardiogram (ECG) interpretation requires specialized expertise, often\ninvolving synthesizing insights from ECG signals with complex clinical queries\nposed in natural language. The scarcity of labeled ECG data coupled with the\ndiverse nature of clinical inquiries presents a significant challenge for\ndeveloping robust and adaptable ECG diagnostic systems. This work introduces a\nnovel multimodal meta-learning method for few-shot ECG question answering,\naddressing the challenge of limited labeled data while leveraging the rich\nknowledge encoded within large language models (LLMs). Our LLM-agnostic\napproach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA\nand Gemma) via a trainable fusion module, enabling the language model to reason\nabout ECG data and generate clinically meaningful answers. Extensive\nexperiments demonstrate superior generalization to unseen diagnostic tasks\ncompared to supervised baselines, achieving notable performance even with\nlimited ECG leads. For instance, in a 5-way 5-shot setting, our method using\nLLaMA-3.1-8B achieves accuracy of 84.6%, 77.3%, and 69.6% on single verify,\nchoose and query question types, respectively. These results highlight the\npotential of our method to enhance clinical ECG interpretation by combining\nsignal processing with the nuanced language understanding capabilities of LLMs,\nparticularly in data-constrained scenarios.', 'The success of Multimodal Large Language Models (MLLMs) in the medical\nauxiliary field shows great potential, allowing patients to engage in\nconversations using physiological signal data. However, general MLLMs perform\npoorly in cardiac disease diagnosis, particularly in the integration of ECG\ndata analysis and long-text medical report generation, mainly due to the\ncomplexity of ECG data analysis and the gap between text and ECG signal\nmodalities. Additionally, models often exhibit severe stability deficiencies in\nlong-text generation due to the lack of precise knowledge strongly related to\nuser queries. To address these issues, we propose ECG-Chat, the first multitask\nMLLMs focused on ECG medical report generation, providing multimodal\nconversational capabilities based on cardiology knowledge. We propose a\ncontrastive learning approach that integrates ECG waveform data with text\nreports, aligning ECG features with reports in a fine-grained manner. This\nmethod also results in an ECG encoder that excels in zero-shot report retrieval\ntasks. Additionally, expanding existing datasets, we constructed a 19k ECG\ndiagnosis dataset and a 25k multi-turn dialogue dataset for training and\nfine-tuning ECG-Chat, which provides professional diagnostic and conversational\ncapabilities. Furthermore, ECG-Chat can generate comprehensive ECG analysis\nreports through an automated LaTeX generation pipeline. We established a\nbenchmark for the ECG report generation task and tested our model on multiple\nbaselines. ECG-Chat achieved the best performance in classification, retrieval,\nmultimodal dialogue, and medical report generation tasks. Our report template\ndesign has also been widely recognized by medical practitioners.']"
167,15,167_energy_power_gpu_slos,"['energy', 'power', 'gpu', 'slos', 'carbon', 'consumption', 'emissions', 'mobile', 'inference', 'cpu']","[""Efficient power management in cloud data centers is essential for reducing\ncosts, enhancing performance, and minimizing environmental impact. GPUs,\ncritical for tasks like machine learning (ML) and GenAI, are major contributors\nto power consumption. NVIDIA's Multi-Instance GPU (MIG) technology improves GPU\nutilization by enabling isolated partitions with per-partition resource\ntracking, facilitating GPU sharing by multiple tenants. However, accurately\napportioning GPU power consumption among MIG instances remains challenging due\nto a lack of hardware support. This paper addresses this challenge by\ndeveloping software methods to estimate power usage per MIG partition. We\nanalyze NVIDIA GPU utilization metrics and find that light-weight methods with\ngood accuracy can be difficult to construct. We hence explore the use of\nML-based power models to enable accurate, partition-level power estimation. Our\nfindings reveal that a single generic offline power model or modeling method is\nnot applicable across diverse workloads, especially with concurrent MIG usage,\nand that online models constructed using partition-level utilization metrics of\nworkloads under execution can significantly improve accuracy. Using NVIDIA A100\nGPUs, we demonstrate this approach for accurate partition-level power\nestimation for workloads including matrix multiplication and Large Language\nModel inference, contributing to transparent and fair carbon reporting."", 'The rise of Artificial Intelligence and Large Language Models is driving\nincreased GPU usage in data centers for complex training and inference tasks,\nimpacting operational costs, energy demands, and the environmental footprint of\nlarge-scale computing infrastructures. This work addresses the online\nscheduling problem in GPU datacenters, which involves scheduling tasks without\nknowledge of their future arrivals. We focus on two objectives: minimizing GPU\nfragmentation and reducing power consumption. GPU fragmentation occurs when\npartial GPU allocations hinder the efficient use of remaining resources,\nespecially as the datacenter nears full capacity. A recent scheduling policy,\nFragmentation Gradient Descent (FGD), leverages a fragmentation metric to\naddress this issue. Reducing power consumption is also crucial due to the\nsignificant power demands of GPUs. To this end, we propose PWR, a novel\nscheduling policy to minimize power usage by selecting power-efficient GPU and\nCPU combinations. This involves a simplified model for measuring power\nconsumption integrated into a Kubernetes score plugin. Through an extensive\nexperimental evaluation in a simulated cluster, we show how PWR, when combined\nwith FGD, achieves a balanced trade-off between reducing power consumption and\nminimizing GPU fragmentation.', ""The rapid evolution and widespread adoption of generative large language\nmodels (LLMs) have made them a pivotal workload in various applications. Today,\nLLM inference clusters receive a large number of queries with strict Service\nLevel Objectives (SLOs). To achieve the desired performance, these models\nexecute on power-hungry GPUs causing the inference clusters to consume large\namount of energy and, consequently, result in excessive carbon emissions.\nFortunately, we find that there is a great opportunity to exploit the\nheterogeneity in inference compute properties and fluctuations in inference\nworkloads, to significantly improve energy-efficiency. However, such a diverse\nand dynamic environment creates a large search-space where different system\nconfigurations (e.g., number of instances, model parallelism, and GPU\nfrequency) translate into different energy-performance trade-offs. To address\nthese challenges, we propose DynamoLLM, the first energy-management framework\nfor LLM inference environments. DynamoLLM automatically and dynamically\nreconfigures the inference cluster to optimize for energy and cost of LLM\nserving under the service's performance SLOs. We show that at a service-level,\nDynamoLLM conserves 53% energy and 38% operational carbon emissions, and\nreduces 61% cost to the customer, while meeting the latency SLOs.""]"
168,14,168_mcda_lms_pydecision_answer,"['mcda', 'lms', 'pydecision', 'answer', 'fallacy', 'zepo', 'brp', 'saup', 'uncertainty', 'maxims']","[""Purpose: Multicriteria decision analysis (MCDA) has become increasingly\nessential for decision-making in complex environments. In response to this\nneed, the pyDecision library, implemented in Python and available at\nhttps://bit.ly/3tLFGtH, has been developed to provide a comprehensive and\naccessible collection of MCDA methods. Methods: The pyDecision offers 70 MCDA\nmethods, including AHP, TOPSIS, and the PROMETHEE and ELECTRE families. Beyond\noffering a vast range of techniques, the library provides visualization tools\nfor more intuitive results interpretation. In addition to these features,\npyDecision has integrated ChatGPT, an advanced Large Language Model, where\ndecision-makers can use ChatGPT to discuss and compare the outcomes of\ndifferent methods, providing a more interactive and intuitive understanding of\nthe solutions. Findings: Large Language Models are undeniably potent but can\nsometimes be a double-edged sword. Its answers may be misleading without\nrigorous verification of its outputs, especially for researchers lacking deep\ndomain expertise. It's imperative to approach its insights with a discerning\neye and a solid foundation in the relevant field. Originality: With the\nintegration of MCDA methods and ChatGPT, pyDecision is a significant\ncontribution to the scientific community, as it is an invaluable resource for\nresearchers, practitioners, and decision-makers navigating complex\ndecision-making problems and seeking the most appropriate solutions based on\nMCDA methods."", ""Large Language Models (LLMs) are extensively used today across various\nsectors, including academia, research, business, and finance, for tasks such as\ntext generation, summarization, and translation. Despite their widespread\nadoption, these models often produce incorrect and misleading information,\nexhibiting a tendency to hallucinate. This behavior can be attributed to\nseveral factors, with consistency and reasoning capabilities being significant\ncontributors. LLMs frequently lack the ability to generate explanations and\nengage in coherent reasoning, leading to inaccurate responses. Moreover, they\nexhibit inconsistencies in their outputs. This paper aims to evaluate and\ncompare the consistency and reasoning capabilities of both public and\nproprietary LLMs. The experiments utilize the Boolq dataset as the ground\ntruth, comprising questions, answers, and corresponding explanations. Queries\nfrom the dataset are presented as prompts to the LLMs, and the generated\nresponses are evaluated against the ground truth answers. Additionally,\nexplanations are generated to assess the models' reasoning abilities.\nConsistency is evaluated by repeatedly presenting the same query to the models\nand observing for variations in their responses. For measuring reasoning\ncapabilities, the generated explanations are compared to the ground truth\nexplanations using metrics such as BERT, BLEU, and F-1 scores. The findings\nreveal that proprietary models generally outperform public models in terms of\nboth consistency and reasoning capabilities. However, even when presented with\nbasic general knowledge questions, none of the models achieved a score of 90\\%\nin both consistency and reasoning. This study underscores the direct\ncorrelation between consistency and reasoning abilities in LLMs and highlights\nthe inherent reasoning challenges present in current language models."", 'One of the most widely used methods to evaluate LLMs are Multiple Choice\nQuestion (MCQ) tests. MCQ benchmarks enable the testing of LLM knowledge on\nalmost any topic at scale as the results can be processed automatically. To\nhelp the LLM answer, a few examples called few shots can be included in the\nprompt. Moreover, the LLM can be asked to answer the question directly with the\nselected option or to first provide the reasoning and then the selected answer,\nwhich is known as chain of thought. In addition to checking whether the\nselected answer is correct, the evaluation can look at the LLM-estimated\nprobability of its response as an indication of the confidence of the LLM in\nthe response. In this paper, we study how the LLM confidence in its answer\ndepends on whether the model has been asked to answer directly or to provide\nthe reasoning before answering. The results of the evaluation of questions on a\nwide range of topics in seven different models show that LLMs are more\nconfident in their answers when they provide reasoning before the answer. This\noccurs regardless of whether the selected answer is correct. Our hypothesis is\nthat this behavior is due to the reasoning that modifies the probability of the\nselected answer, as the LLM predicts the answer based on the input question and\nthe reasoning that supports the selection made. Therefore, LLM estimated\nprobabilities seem to have intrinsic limitations that should be understood in\norder to use them in evaluation procedures. Interestingly, the same behavior\nhas been observed in humans, for whom explaining an answer increases confidence\nin its correctness.']"
169,14,169_audit_evaluators_auditwen_textsccheckeval,"['audit', 'evaluators', 'auditwen', 'textsccheckeval', 'mrc', 'rc', 'critiquing', 'dcr', 'multiqa', 'activecritic']","['The zero-shot capability of Large Language Models (LLMs) has enabled highly\nflexible, reference-free metrics for various tasks, making LLM evaluators\ncommon tools in NLP. However, the robustness of these LLM evaluators remains\nrelatively understudied; existing work mainly pursued optimal performance in\nterms of correlating LLM scores with human expert scores. In this paper, we\nconduct a series of analyses using the SummEval dataset and confirm that LLMs\nare biased evaluators as they: (1) exhibit familiarity bias-a preference for\ntext with lower perplexity, (2) show skewed and biased distributions of\nratings, and (3) experience anchoring effects for multi-attribute judgments. We\nalso found that LLMs are inconsistent evaluators, showing low ""inter-sample""\nagreement and sensitivity to prompt differences that are insignificant to human\nunderstanding of text quality. Furthermore, we share recipes for configuring\nLLM evaluators to mitigate these limitations. Experimental results on the RoSE\ndataset demonstrate improvements over the state-of-the-art LLM evaluators.', 'Traditional evaluation metrics like BLEU and ROUGE fall short when capturing\nthe nuanced qualities of generated text, particularly when there is no single\nground truth. In this paper, we explore the potential of Large Language Models\n(LLMs), specifically Google Gemini 1, to serve as automatic evaluators for\nnon-standardized metrics in summarization and dialog-based tasks. We conduct\nexperiments across multiple prompting strategies to examine how LLMs fare as\nquality evaluators when compared with human judgments on the SummEval and USR\ndatasets, asking the model to generate both a score as well as a justification\nfor the score. Furthermore, we explore the robustness of the LLM evaluator by\nusing perturbed inputs. Our findings suggest that while LLMs show promise,\ntheir alignment with human evaluators is limited, they are not robust against\nperturbations and significant improvements are required for their standalone\nuse as reliable evaluators for subjective metrics.', 'Intelligent auditing represents a crucial advancement in modern audit\npractices, enhancing both the quality and efficiency of audits within the realm\nof artificial intelligence. With the rise of large language model (LLM), there\nis enormous potential for intelligent models to contribute to audit domain.\nHowever, general LLMs applied in audit domain face the challenges of lacking\nspecialized knowledge and the presence of data biases. To overcome these\nchallenges, this study introduces AuditWen, an open-source audit LLM by\nfine-tuning Qwen with constructing instruction data from audit domain. We first\noutline the application scenarios for LLMs in the audit and extract\nrequirements that shape the development of LLMs tailored for audit purposes. We\nthen propose an audit LLM, called AuditWen, by fine-tuning Qwen with\nconstructing 28k instruction dataset from 15 audit tasks and 3 layers. In\nevaluation stage, we proposed a benchmark with 3k instructions that covers a\nset of critical audit tasks derived from the application scenarios. With the\nbenchmark, we compare AuditWen with other existing LLMs from information\nextraction, question answering and document generation. The experimental\nresults demonstrate superior performance of AuditWen both in question\nunderstanding and answer generation, making it an immediately valuable tool for\naudit.']"
170,14,170_finetuning_lora_domainadjacent_delift,"['finetuning', 'lora', 'domainadjacent', 'delift', 'continual', 'pretraining', 'training', 'versatune', 'during', 'knowledge']","['Large Language Models (LLMs) have achieved impressive results across numerous\nNLP tasks but still encounter difficulties in machine translation. Traditional\nmethods to improve translation have typically involved fine-tuning LLMs using\nparallel corpora. However, vanilla fine-tuning often leads to catastrophic\nforgetting of the instruction-following capabilities and alignment with human\npreferences, compromising their broad general abilities and introducing\npotential security risks. These abilities, which are developed using\nproprietary and unavailable training data, make existing continual instruction\ntuning methods ineffective. To overcome this issue, we propose a novel approach\ncalled RaDis (Rationale Distillation). RaDis harnesses the strong generative\ncapabilities of LLMs to create rationales for training data, which are then\n""replayed"" to prevent forgetting. These rationales encapsulate general\nknowledge and safety principles, acting as self-distillation targets to\nregulate the training process. By jointly training on both reference\ntranslations and self-generated rationales, the model can learn new translation\nskills while preserving its overall general abilities. Extensive experiments\ndemonstrate that our method enhances machine translation performance while\nmaintaining the broader capabilities of LLMs across other tasks. This work\npresents a pathway for creating more versatile LLMs that excel in specialized\ntasks without compromising generality and safety.', ""Large-scale pretrained models, particularly Large Language Models (LLMs),\nhave exhibited remarkable capabilities in handling multiple tasks across\ndomains due to their emergent properties. These capabilities are further\naugmented during the Supervised Fine-Tuning (SFT) phase. Despite their\npotential, existing work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce VersaTune, a novel\ndata composition framework designed for enhancing LLMs' overall multi-ability\nperformances during training. We categorize knowledge into distinct domains\nincluding law, medicine, finance, science, code, etc. We begin with detecting\nthe distribution of domain-specific knowledge within the base model, followed\nby the training data composition that aligns with the model's existing\nknowledge distribution. During the training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results demonstrate that VersaTune achieves significant\nimprovements in multi-domain performance, with an 35.21% enhancement in\ncomprehensive multi-domain tasks. Additionally, in scenarios where specific\ndomain optimization is required, VersaTune reduces the degradation of\nperformance in other domains by 38.77%, without compromising the target\ndomain's training efficacy."", 'Adapting general large language models (LLMs) to specialized domains presents\ngreat challenges due to varied data distributions. This adaptation typically\nrequires continual pre-training on massive domain-specific corpora to\nfacilitate knowledge memorization, followed by training to apply this knowledge\nfollowing human instructions and preferences. However, this method may result\nin inefficient knowledge memorization due to a lack of awareness of knowledge\nutilization and imposes substantial demands on LLMs to simultaneously learn\nknowledge utilization and format alignment with limited training samples. To\nfacilitate the domain adaptation of LLM, we revise this process and propose a\nnew domain adaptation framework including domain knowledge learning and general\nformat alignment, called Mix-CPT. Specifically, we first conduct a knowledge\nmixture continual pre-training that concurrently focuses on knowledge\nmemorization and utilization, allowing for mutual reinforcement. To avoid\ncatastrophic forgetting during the continual pre-training process, we further\nincorporate a logit swap self-distillation constraint. Subsequently, leveraging\nthe knowledge and capabilities acquired during continual pre-training, we\nefficiently perform instruction tuning and alignment with a few general\ntraining samples to achieve format alignment. Extensive experiments demonstrate\nthat our proposed Mix-CPT framework can simultaneously improve the task-solving\ncapabilities of LLMs on the target and general domains compared to the\ntraditional adaptation methods.']"
171,13,171_3gpp_telecommunications_dyplan_phi2,"['3gpp', 'telecommunications', 'dyplan', 'phi2', 'knowledge', 'answering', 'retrieval', 'ir', 'familiarity', 'chat3gpp']","[""Generative large language models (LLMs) have been demonstrated to have gaps\nin diverse, cultural knowledge across the globe. We investigate the effect of\nretrieval augmented generation and search-grounding techniques on the ability\nof LLMs to display familiarity with a diverse range of national cultures.\nSpecifically, we compare the performance of standard LLMs, LLMs augmented with\nretrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs\naugmented with retrievals from a web search (i.e., search grounding) on a\nseries of cultural familiarity benchmarks. We find that search grounding\nsignificantly improves the LLM performance on multiple-choice benchmarks that\ntest propositional knowledge (e.g., the norms, artifacts, and institutions of\nnational cultures), while KB grounding's effectiveness is limited by inadequate\nknowledge base coverage and a suboptimal retriever. However, search grounding\nalso increases the risk of stereotypical judgments by language models, while\nfailing to improve evaluators' judgments of cultural familiarity in a human\nevaluation with adequate statistical power. These results highlight the\ndistinction between propositional knowledge about a culture and open-ended\ncultural fluency when it comes to evaluating the cultural familiarity of\ngenerative LLMs."", 'Large Language Models (LLMs) have garnered significant attention for their\nimpressive general-purpose capabilities. For applications requiring intricate\ndomain knowledge, Retrieval-Augmented Generation (RAG) has shown a distinct\nadvantage in incorporating domain-specific information into LLMs. However,\nexisting RAG research has not fully addressed the challenges of Multiple Choice\nQuestion Answering (MCQA) in telecommunications, particularly in terms of\nretrieval quality and mitigating hallucinations. To tackle these challenges, we\npropose a novel first token probability guided RAG framework. This framework\nleverages confidence scores to optimize key hyperparameters, such as chunk\nnumber and chunk window size, while dynamically adjusting the context. Our\nmethod starts by retrieving the most relevant chunks and generates a single\ntoken as the potential answer. The probabilities of all options are then\nnormalized to serve as confidence scores, which guide the dynamic adjustment of\nthe context. By iteratively optimizing the hyperparameters based on these\nconfidence scores, we can continuously improve RAG performance. We conducted\nexperiments to validate the effectiveness of our framework, demonstrating its\npotential to enhance accuracy in domain-specific MCQA tasks.', 'The 3rd Generation Partnership Project (3GPP) documents is key standards in\nglobal telecommunications, while posing significant challenges for engineers\nand researchers in the telecommunications field due to the large volume and\ncomplexity of their contents as well as the frequent updates. Large language\nmodels (LLMs) have shown promise in natural language processing tasks, but\ntheir general-purpose nature limits their effectiveness in specific domains\nlike telecommunications. To address this, we propose Chat3GPP, an open-source\nretrieval-augmented generation (RAG) framework tailored for 3GPP\nspecifications. By combining chunking strategies, hybrid retrieval and\nefficient indexing methods, Chat3GPP can efficiently retrieve relevant\ninformation and generate accurate responses to user queries without requiring\ndomain-specific fine-tuning, which is both flexible and scalable, offering\nsignificant potential for adapting to other technical standards beyond 3GPP. We\nevaluate Chat3GPP on two telecom-specific datasets and demonstrate its superior\nperformance compared to existing methods, showcasing its potential for\ndownstream tasks like protocol generation and code automation.']"
172,13,172_egocentric_video_4d_motion,"['egocentric', 'video', '4d', 'motion', 'vinci', 'videos', 'psg4d', 'handobject', 'mllms', 'appearance']","[""AI personal assistants, deployed through robots or wearables, require\nembodied understanding to collaborate effectively with humans. Current\nMultimodal Large Language Models (MLLMs) primarily focus on third-person\n(exocentric) vision, overlooking the unique aspects of first-person\n(egocentric) videos. Additionally, high acquisition costs limit data size,\nimpairing MLLM performance. To address these challenges, we propose learning\nthe mapping between exocentric and egocentric domains, leveraging the extensive\nexocentric knowledge within existing MLLMs to enhance egocentric video\nunderstanding. To this end, we introduce Ego-ExoClip, a pre-training dataset\ncomprising 1.1M synchronized ego-exo clip-text pairs derived from Ego-Exo4D.\nOur approach features a progressive training pipeline with three stages:\nTeacher Self-Preparation, Teacher-Student Guidance, and Student Self-Practice.\nAdditionally, we propose an instruction-tuning data EgoIT from multiple sources\nto strengthen the model's instruction-following capabilities, along with the\nEgoBench benchmark comprising eight different tasks for thorough evaluation.\nExtensive experiments across diverse egocentric tasks reveal that existing\nMLLMs perform inadequately in egocentric video understanding, while our model\nsignificantly outperforms these leading models."", ""The rapid evolution of egocentric video analysis brings new insights into\nunderstanding human activities and intentions from a first-person perspective.\nDespite this progress, the fragmentation in tasks like action recognition,\nprocedure learning, and moment retrieval, \\etc, coupled with inconsistent\nannotations and isolated model development, hinders a holistic interpretation\nof video content. In response, we introduce the EAGLE (Egocentric AGgregated\nLanguage-video Engine) model and the EAGLE-400K dataset to provide a unified\nframework that integrates various egocentric video understanding tasks.\nEAGLE-400K, the \\textit{first} large-scale instruction-tuning dataset tailored\nfor egocentric video, features 400K diverse samples to enhance a broad spectrum\nof tasks from activity recognition to procedure knowledge learning. Moreover,\nEAGLE, a strong video multimodal large language model (MLLM), is designed to\neffectively capture both spatial and temporal information. In addition, we\npropose a set of evaluation metrics designed to facilitate a thorough\nassessment of MLLM for egocentric video understanding. Our extensive\nexperiments demonstrate EAGLE's superior performance over existing models,\nhighlighting its ability to balance task-specific understanding with holistic\nvideo interpretation. With EAGLE, we aim to pave the way for research\nopportunities and practical applications in real-world scenarios."", 'Long-form egocentric video understanding provides rich contextual information\nand unique insights into long-term human behaviors, holding significant\npotential for applications in embodied intelligence, long-term activity\nanalysis, and personalized assistive technologies. However, existing benchmark\ndatasets primarily focus on single, short-duration videos or moderately long\nvideos up to dozens of minutes, leaving a substantial gap in evaluating\nextensive, ultra-long egocentric video recordings. To address this, we\nintroduce X-LeBench, a novel benchmark dataset specifically crafted for\nevaluating tasks on extremely long egocentric video recordings. Leveraging the\nadvanced text processing capabilities of large language models (LLMs),\nX-LeBench develops a life-logging simulation pipeline that produces realistic,\ncoherent daily plans aligned with real-world video data. This approach enables\nthe flexible integration of synthetic daily plans with real-world footage from\nEgo4D-a massive-scale egocentric video dataset covers a wide range of daily\nlife scenarios-resulting in 432 simulated video life logs that mirror realistic\ndaily activities in contextually rich scenarios. The video life-log durations\nspan from 23 minutes to 16.4 hours. The evaluation of several baseline systems\nand multimodal large language models (MLLMs) reveals their poor performance\nacross the board, highlighting the inherent challenges of long-form egocentric\nvideo understanding and underscoring the need for more advanced models.']"
173,13,173_reasoning_streamliners_humanities_sciences,"['reasoning', 'streamliners', 'humanities', 'sciences', 'emergent', 'frem', 'optimisation', 'demonstrations', 'particle', 'hypothesis']","['Long-context question-answering (LCQA) systems have greatly benefited from\nthe powerful reasoning capabilities of large language models (LLMs), which can\nbe categorized into slow and quick reasoning modes. However, both modes have\ntheir limitations. Slow thinking generally leans to explore every possible\nreasoning path, which leads to heavy overthinking and wastes time. Quick\nthinking usually relies on pattern matching rather than truly understanding the\nquery logic, which misses proper understanding. To address these issues, we\npropose FReM: Flexible Reasoning Mechanism, a method that adjusts reasoning\ndepth according to the complexity of each question. Specifically, FReM\nleverages synthetic reference QA examples to provide an explicit chain of\nthought, enabling efficient handling of simple queries while allowing deeper\nreasoning for more complex ones. By doing so, FReM helps quick-thinking models\nmove beyond superficial pattern matching and narrows the reasoning space for\nslow-thinking models to avoid unnecessary exploration. Experiments on seven QA\ndatasets show that FReM improves reasoning accuracy and scalability,\nparticularly for complex multihop questions, indicating its potential to\nadvance LCQA methodologies.', ""In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art\neducation.Then we compare the answers given by DeepSeek-R1 in the seven aspects\nwith the answers given by o1-preview. DeepSeek-R1 performs well in the\nhumanities and social sciences, answering most questions correctly and\nlogically, and can give reasonable analysis processes and explanations.\nCompared with o1-preview, it can automatically generate reasoning processes and\nprovide more detailed explanations, which is suitable for beginners or people\nwho need to have a detailed understanding of this knowledge, while o1-preview\nis more suitable for quick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications."", 'Large Language Models (LLMs) are leading a new technological revolution as\none of the most promising research streams toward artificial general\nintelligence. The scaling of these models, accomplished by increasing the\nnumber of parameters and the magnitude of the training datasets, has been\nlinked to various so-called emergent abilities that were previously unobserved.\nThese emergent abilities, ranging from advanced reasoning and in-context\nlearning to coding and problem-solving, have sparked an intense scientific\ndebate: Are they truly emergent, or do they simply depend on external factors,\nsuch as training dynamics, the type of problems, or the chosen metric? What\nunderlying mechanism causes them? Despite their transformative potential,\nemergent abilities remain poorly understood, leading to misconceptions about\ntheir definition, nature, predictability, and implications. In this work, we\nshed light on emergent abilities by conducting a comprehensive review of the\nphenomenon, addressing both its scientific underpinnings and real-world\nconsequences. We first critically analyze existing definitions, exposing\ninconsistencies in conceptualizing emergent abilities. We then explore the\nconditions under which these abilities appear, evaluating the role of scaling\nlaws, task complexity, pre-training loss, quantization, and prompting\nstrategies. Our review extends beyond traditional LLMs and includes Large\nReasoning Models (LRMs), which leverage reinforcement learning and\ninference-time search to amplify reasoning and self-reflection. However,\nemergence is not inherently positive. As AI systems gain autonomous reasoning\ncapabilities, they also develop harmful behaviors, including deception,\nmanipulation, and reward hacking. We highlight growing concerns about safety\nand governance, emphasizing the need for better evaluation frameworks and\nregulatory oversight.']"
174,13,174_debate_mad_debates_multiagent,"['debate', 'mad', 'debates', 'multiagent', 'gvic', 'judge', 'lateral', 'connections', 'drift', 'players']","['In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across diverse NLP tasks. Extensive research has explored how to\nenhance the logical reasoning abilities such as Chain-of-Thought,\nChain-of-Thought with Self-Consistency, Tree-Of-Thoughts, and multi-agent\ndebates. In the context of multi-agent debates, significant performance\nimprovements can be achieved with an increasing number of agents and debate\nrounds. However, the escalation in the number of agents and debate rounds can\ndrastically raise the tokens cost of debates, thereby limiting the scalability\nof the multi-agent debate technique. To better harness the advantages of\nmulti-agent debates in logical reasoning tasks, this paper proposes a method to\nsignificantly reduce token cost in multi-agent debates. This approach involves\ndividing all agents into multiple debate groups, with agents engaging in\ndebates within their respective groups and sharing interim debate results\nbetween groups. Comparative experiments across multiple datasets have\ndemonstrated that this method can reduce the total tokens by up to 51.7% during\ndebates and while potentially enhancing accuracy by as much as 25%. Our method\nsignificantly enhances the performance and efficiency of interactions in the\nmulti-agent debate.', 'In recent years, large language models have shown exceptional performance in\nfulfilling diverse human needs. However, their training data can introduce\nharmful content, underscoring the necessity for robust value alignment.\nMainstream methods, which depend on feedback learning and supervised training,\nare resource-intensive and may constrain the full potential of the models.\nMulti-Agent Debate (MAD) offers a more efficient and innovative solution by\nenabling the generation of reliable answers through agent interactions. To\napply MAD to value alignment, we examine the relationship between the\nhelpfulness and harmlessness of debate outcomes and individual responses, and\npropose a MAD based framework Gradual Vigilance and Interval Communication\n(GVIC). GVIC allows agents to assess risks with varying levels of vigilance and\nto exchange diverse information through interval communication. We\ntheoretically prove that GVIC optimizes debate efficiency while reducing\ncommunication overhead. Experimental results demonstrate that GVIC consistently\noutperforms baseline methods across various tasks and datasets, particularly\nexcelling in harmfulness mitigation and fraud prevention. Additionally, GVIC\nexhibits strong adaptability across different base model sizes, including both\nunaligned and aligned models, and across various task types.', ""Scalable oversight protocols aim to enable humans to accurately supervise\nsuperhuman AI. In this paper we study debate, where two AI's compete to\nconvince a judge; consultancy, where a single AI tries to convince a judge that\nasks questions; and compare to a baseline of direct question-answering, where\nthe judge just answers outright without the AI. We use large language models\n(LLMs) as both AI agents and as stand-ins for human judges, taking the judge\nmodels to be weaker than agent models. We benchmark on a diverse range of\nasymmetries between judges and agents, extending previous work on a single\nextractive QA task with information asymmetry, to also include mathematics,\ncoding, logic and multimodal reasoning asymmetries. We find that debate\noutperforms consultancy across all tasks when the consultant is randomly\nassigned to argue for the correct/incorrect answer. Comparing debate to direct\nquestion answering, the results depend on the type of task: in extractive QA\ntasks with information asymmetry debate outperforms direct question answering,\nbut in other tasks without information asymmetry the results are mixed.\nPrevious work assigned debaters/consultants an answer to argue for. When we\nallow them to instead choose which answer to argue for, we find judges are less\nfrequently convinced by the wrong answer in debate than in consultancy.\nFurther, we find that stronger debater models increase judge accuracy, though\nmore modestly than in previous studies.""]"
175,13,175_attack_attacks_malicious_poisoning,"['attack', 'attacks', 'malicious', 'poisoning', 'adversarial', 'actorattack', 'injection', 'safety', 'defenses', 'defense']","[""The increasing use of large language models (LLMs) trained by third parties\nraises significant security concerns. In particular, malicious actors can\nintroduce backdoors through poisoning attacks to generate undesirable outputs.\nWhile such attacks have been extensively studied in image domains and\nclassification tasks, they remain underexplored for natural language generation\n(NLG) tasks. To address this gap, we conduct an investigation of various\npoisoning techniques targeting the LLM's fine-tuning phase via prefix-tuning, a\nParameter Efficient Fine-Tuning (PEFT) method. We assess their effectiveness\nacross two generative tasks: text summarization and text completion; and we\nalso introduce new metrics to quantify the success and stealthiness of such NLG\npoisoning attacks. Through our experiments, we find that the prefix-tuning\nhyperparameters and trigger designs are the most crucial factors to influence\nattack success and stealthiness. Moreover, we demonstrate that existing popular\ndefenses are ineffective against our poisoning attacks. Our study presents the\nfirst systematic approach to understanding poisoning attacks targeting NLG\ntasks during fine-tuning via PEFT across a wide range of triggers and attack\nsettings. We hope our findings will aid the AI security community in developing\neffective defenses against such threats."", ""Current large language models (LLM) provide a strong foundation for\nlarge-scale user-oriented natural language tasks. Many users can easily inject\nadversarial text or instructions through the user interface, thus causing LLM\nmodel security challenges like the language model not giving the correct\nanswer. Although there is currently a large amount of research on black-box\nattacks, most of these black-box attacks use random and heuristic strategies.\nIt is unclear how these strategies relate to the success rate of attacks and\nthus effectively improve model robustness. To solve this problem, we propose\nour target-driven black-box attack method to maximize the KL divergence between\nthe conditional probabilities of the clean text and the attack text to redefine\nthe attack's goal. We transform the distance maximization problem into two\nconvex optimization problems based on the attack goal to solve the attack text\nand estimate the covariance. Furthermore, the projected gradient descent\nalgorithm solves the vector corresponding to the attack text. Our target-driven\nblack-box attack approach includes two attack strategies: token manipulation\nand misinformation attack. Experimental results on multiple Large Language\nModels and datasets demonstrate the effectiveness of our attack method."", ""Current large language models (LLMs) provide a strong foundation for\nlarge-scale user-oriented natural language tasks. A large number of users can\neasily inject adversarial text or instructions through the user interface, thus\ncausing LLMs model security challenges. Although there is currently a large\namount of research on prompt injection attacks, most of these black-box attacks\nuse heuristic strategies. It is unclear how these heuristic strategies relate\nto the success rate of attacks and thus effectively improve model robustness.\nTo solve this problem, we redefine the goal of the attack: to maximize the KL\ndivergence between the conditional probabilities of the clean text and the\nadversarial text. Furthermore, we prove that maximizing the KL divergence is\nequivalent to maximizing the Mahalanobis distance between the embedded\nrepresentation $x$ and $x'$ of the clean text and the adversarial text when the\nconditional probability is a Gaussian distribution and gives a quantitative\nrelationship on $x$ and $x'$. Then we designed a simple and effective\ngoal-guided generative prompt injection strategy (G2PIA) to find an injection\ntext that satisfies specific constraints to achieve the optimal attack effect\napproximately. It is particularly noteworthy that our attack method is a\nquery-free black-box attack method with low computational cost. Experimental\nresults on seven LLM models and four datasets show the effectiveness of our\nattack method.""]"
176,13,176_distractors_repe_mcqs_dif,"['distractors', 'repe', 'mcqs', 'dif', 'mcq', 'distractor', 'students', 'selfreview', 'math', 'difficulty']","['High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is\ncrucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math\nMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a\nhuman evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.', 'Multiple-choice questions (MCQs) are commonly used across all levels of math\neducation since they can be deployed and graded at a large scale. A critical\ncomponent of MCQs is the distractors, i.e., incorrect answers crafted to\nreflect student errors or misconceptions. Automatically generating them in math\nMCQs, e.g., with large language models, has been challenging. In this work, we\npropose a novel method to enhance the quality of generated distractors through\novergenerate-and-rank, training a ranking model to predict how likely\ndistractors are to be selected by real students. Experimental results on a\nreal-world dataset and human evaluation with math teachers show that our\nranking model increases alignment with human-authored distractors, although\nhuman-authored ones are still preferred over generated ones.', ""Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious educational tasks, yet their alignment with human learning patterns,\nparticularly in predicting which incorrect options students are most likely to\nselect in multiple-choice questions (MCQs), remains underexplored. Our work\ninvestigates the relationship between LLM generation likelihood and student\nresponse distributions in MCQs with a specific focus on distractor selections.\nWe collect a comprehensive dataset of MCQs with real-world student response\ndistributions to explore two fundamental research questions: (1). RQ1 - Do the\ndistractors that students more frequently select correspond to those that LLMs\nassign higher generation likelihood to? (2). RQ2 - When an LLM selects a\nincorrect choice, does it choose the same distractor that most students pick?\nOur experiments reveals moderate correlations between LLM-assigned\nprobabilities and student selection patterns for distractors in MCQs.\nAdditionally, when LLMs make mistakes, they are more likley to select the same\nincorrect answers that commonly mislead students, which is a pattern consistent\nacross both small and large language models. Our work provides empirical\nevidence that despite LLMs' strong performance on generating educational\ncontent, there remains a gap between LLM's underlying reasoning process and\nhuman cognitive processes in identifying confusing distractors. Our findings\nalso have significant implications for educational assessment development. The\nsmaller language models could be efficiently utilized for automated distractor\ngeneration as they demonstrate similar patterns in identifying confusing answer\nchoices as larger language models. This observed alignment between LLMs and\nstudent misconception patterns opens new opportunities for generating\nhigh-quality distractors that complement traditional human-designed\ndistractors.""]"
177,13,177_sign_slt_translation_glossfree,"['sign', 'slt', 'translation', 'glossfree', 'spoken', 'elmi', 'asl', 'csldaily', 'songsigners', 'gloss']","['Automatic Sign Language Translation requires the integration of both computer\nvision and natural language processing to effectively bridge the communication\ngap between sign and spoken languages. However, the deficiency in large-scale\ntraining data to support sign language translation means we need to leverage\nresources from spoken language. We introduce, Sign2GPT, a novel framework for\nsign language translation that utilizes large-scale pretrained vision and\nlanguage models via lightweight adapters for gloss-free sign language\ntranslation. The lightweight adapters are crucial for sign language\ntranslation, due to the constraints imposed by limited dataset sizes and the\ncomputational requirements when training with long sign videos. We also propose\na novel pretraining strategy that directs our encoder to learn sign\nrepresentations from automatically extracted pseudo-glosses without requiring\ngloss order information or annotations. We evaluate our approach on two public\nbenchmark sign language translation datasets, namely RWTH-PHOENIX-Weather 2014T\nand CSL-Daily, and improve on state-of-the-art gloss-free translation\nperformance with a significant margin.', 'Sign language translation (SLT) is a challenging task that involves\ntranslating sign language images into spoken language. For SLT models to\nperform this task successfully, they must bridge the modality gap and identify\nsubtle variations in sign language components to understand their meanings\naccurately. To address these challenges, we propose a novel gloss-free SLT\nframework called Multimodal Sign Language Translation (MMSLT), which leverages\nthe representational capabilities of off-the-shelf multimodal large language\nmodels (MLLMs). Specifically, we generate detailed textual descriptions of sign\nlanguage components using MLLMs. Then, through our proposed multimodal-language\npre-training module, we integrate these description features with sign video\nfeatures to align them within the spoken sentence space. Our approach achieves\nstate-of-the-art performance on benchmark datasets PHOENIX14T and CSL-Daily,\nhighlighting the potential of MLLMs to be effectively utilized in SLT.', 'Sign Language Translation (SLT) is a challenging task that aims to translate\nsign videos into spoken language. Inspired by the strong translation\ncapabilities of large language models (LLMs) that are trained on extensive\nmultilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT.\nIn this paper, we regularize the sign videos to embody linguistic\ncharacteristics of spoken language, and propose a novel SignLLM framework to\ntransform sign videos into a language-like representation for improved\nreadability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The\nVector-Quantized Visual Sign module converts sign videos into a sequence of\ndiscrete character-level sign tokens, and (2) the Codebook Reconstruction and\nAlignment module converts these character-level tokens into word-level sign\nrepresentations using an optimal transport formulation. A sign-text alignment\nloss further bridges the gap between sign and text tokens, enhancing semantic\ncompatibility. We achieve state-of-the-art gloss-free results on two\nwidely-used SLT benchmarks.']"
178,12,178_bias_demographic_clinical_biases,"['bias', 'demographic', 'clinical', 'biases', 'care', 'palliative', 'medical', 'healthcare', 'ethnicity', 'disparities']","['Medical Question Answering systems based on Retrieval Augmented Generation is\npromising for clinical decision support because they can integrate external\nknowledge, thus reducing inaccuracies inherent in standalone large language\nmodels (LLMs). However, these systems may unintentionally propagate or amplify\nbiases associated with sensitive demographic attributes like race, gender, and\nsocioeconomic factors. This study systematically evaluates demographic biases\nwithin medical RAG pipelines across multiple QA benchmarks, including MedQA,\nMedMCQA, MMLU, and EquityMedQA. We quantify disparities in retrieval\nconsistency and answer correctness by generating and analyzing queries\nsensitive to demographic variations. We further implement and compare several\nbias mitigation strategies to address identified biases, including Chain of\nThought reasoning, Counterfactual filtering, Adversarial prompt refinement, and\nMajority Vote aggregation. Experimental results reveal significant demographic\ndisparities, highlighting that Majority Vote aggregation notably improves\naccuracy and fairness metrics. Our findings underscore the critical need for\nexplicitly fairness-aware retrieval methods and prompt engineering strategies\nto develop truly equitable medical QA systems.', 'Large language models (LLMs) are increasingly applied to clinical\ndecision-making. However, their potential to exhibit bias poses significant\nrisks to clinical equity. Currently, there is a lack of benchmarks that\nsystematically evaluate such clinical bias in LLMs. While in downstream tasks,\nsome biases of LLMs can be avoided such as by instructing the model to answer\n""I\'m not sure..."", the internal bias hidden within the model still lacks deep\nstudies. We introduce CLIMB (shorthand for A Benchmark of Clinical Bias in\nLarge Language Models), a pioneering comprehensive benchmark to evaluate both\nintrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for\nclinical decision tasks. Notably, for intrinsic bias, we introduce a novel\nmetric, AssocMAD, to assess the disparities of LLMs across multiple demographic\ngroups. Additionally, we leverage counterfactual intervention to evaluate\nextrinsic bias in a task of clinical diagnosis prediction. Our experiments\nacross popular and medically adapted LLMs, particularly from the Mistral and\nLLaMA families, unveil prevalent behaviors with both intrinsic and extrinsic\nbias. This work underscores the critical need to mitigate clinical bias and\nsets a new standard for future evaluations of LLMs\' clinical bias.', 'Recent advancements in Large Language Models (LLMs) have positioned them as\npowerful tools for clinical decision-making, with rapidly expanding\napplications in healthcare. However, concerns about bias remain a significant\nchallenge in the clinical implementation of LLMs, particularly regarding gender\nand ethnicity. This research investigates the evaluation and mitigation of bias\nin LLMs applied to complex clinical cases, focusing on gender and ethnicity\nbiases. We introduce a novel Counterfactual Patient Variations (CPV) dataset\nderived from the JAMA Clinical Challenge. Using this dataset, we built a\nframework for bias evaluation, employing both Multiple Choice Questions (MCQs)\nand corresponding explanations. We explore prompting with eight LLMs and\nfine-tuning as debiasing methods. Our findings reveal that addressing social\nbiases in LLMs requires a multidimensional approach as mitigating gender bias\ncan occur while introducing ethnicity biases, and that gender bias in LLM\nembeddings varies significantly across medical specialities. We demonstrate\nthat evaluating both MCQ response and explanation processes is crucial, as\ncorrect responses can be based on biased \\textit{reasoning}. We provide a\nframework for evaluating LLM bias in real-world clinical cases, offer insights\ninto the complex nature of bias in these models, and present strategies for\nbias mitigation.']"
179,12,179_patent_claim_examiners_novelty,"['patent', 'claim', 'examiners', 'novelty', 'claims', 'invention', 'revision', 'patents', 'innovation', 'rewriting']","[""Large language models (LLMs) have shown exceptional performance across\nvarious text generation tasks but remain under-explored in the patent domain,\nwhich offers highly structured and precise language. This paper constructs a\ndataset to investigate the performance of current LLMs in patent claim\ngeneration. Our results demonstrate that generating claims based on patent\ndescriptions outperforms previous research relying on abstracts. Interestingly,\ncurrent patent-specific LLMs perform much worse than state-of-the-art general\nLLMs, highlighting the necessity for future research on in-domain LLMs. We also\nfind that LLMs can produce high-quality first independent claims, but their\nperformances markedly decrease for subsequent dependent claims. Moreover,\nfine-tuning can enhance the completeness of inventions' features, conceptual\nclarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the\nbest performance in comprehensive human evaluations by patent experts, with\nbetter feature coverage, conceptual clarity, and technical coherence. Despite\nthese capabilities, comprehensive revision and modification are still necessary\nto pass rigorous patent scrutiny and ensure legal robustness."", 'Assessing the novelty of patent claims is a critical yet challenging task\ntraditionally performed by patent examiners. While advancements in NLP have\nenabled progress in various patent-related tasks, novelty assessment remains\nunexplored. This paper introduces a novel challenge by evaluating the ability\nof large language models (LLMs) to assess patent novelty by comparing claims\nwith cited prior art documents, following the process similar to that of patent\nexaminers done. We present the first dataset specifically designed for novelty\nevaluation, derived from real patent examination cases, and analyze the\ncapabilities of LLMs to address this task. Our study reveals that while\nclassification models struggle to effectively assess novelty, generative models\nmake predictions with a reasonable level of accuracy, and their explanations\nare accurate enough to understand the relationship between the target patent\nand prior art. These findings demonstrate the potential of LLMs to assist in\npatent evaluation, reducing the workload for both examiners and applicants. Our\ncontributions highlight the limitations of current models and provide a\nfoundation for improving AI-driven patent analysis through advanced models and\nrefined datasets.', ""Automatic refinement of patent claims in patent applications is crucial from\nthe perspective of intellectual property strategy. In this paper, we propose\nClaimBrush, a novel framework for automated patent claim refinement that\nincludes a dataset and a rewriting model. We constructed a dataset for training\nand evaluating patent claim rewriting models by collecting a large number of\nactual patent claim rewriting cases from the patent examination process. Using\nthe constructed dataset, we built an automatic patent claim rewriting model by\nfine-tuning a large language model. Furthermore, we enhanced the performance of\nthe automatic patent claim rewriting model by applying preference optimization\nbased on a prediction model of patent examiners' Office Actions. The\nexperimental results showed that our proposed rewriting model outperformed\nheuristic baselines and zero-shot learning in state-of-the-art large language\nmodels. Moreover, preference optimization based on patent examiners'\npreferences boosted the performance of patent claim refinement.""]"
180,12,180_capital_automation_asset_workers,"['capital', 'automation', 'asset', 'workers', 'production', 'manufacturing', 'ai', 'formulation', 'minutes', 'stakeholder']","[""Systems engineering (SE) is evolving with the availability of generative\nartificial intelligence (AI) and the demand for a systems-of-systems\nperspective, formalized under the purview of mission engineering (ME) in the US\nDepartment of Defense. Formulating ME problems is challenging because they are\nopen-ended exercises that involve translation of ill-defined problems into\nwell-defined ones that are amenable for engineering development. It remains to\nbe seen to which extent AI could assist problem formulation objectives. To that\nend, this paper explores the quality and consistency of multi-purpose Large\nLanguage Models (LLM) in supporting ME problem formulation tasks, specifically\nfocusing on stakeholder identification. We identify a relevant reference\nproblem, a NASA space mission design challenge, and document ChatGPT-3.5's\nability to perform stakeholder identification tasks. We execute multiple\nparallel attempts and qualitatively evaluate LLM outputs, focusing on both\ntheir quality and variability. Our findings portray a nuanced picture. We find\nthat the LLM performs well in identifying human-focused stakeholders but poorly\nin recognizing external systems and environmental factors, despite explicit\nefforts to account for these. Additionally, LLMs struggle with preserving the\ndesired level of abstraction and exhibit a tendency to produce solution\nspecific outputs that are inappropriate for problem formulation. More\nimportantly, we document great variability among parallel threads, highlighting\nthat LLM outputs should be used with caution, ideally by adopting a stochastic\nview of their abilities. Overall, our findings suggest that, while ChatGPT\ncould reduce some expert workload, its lack of consistency and domain\nunderstanding may limit its reliability for problem formulation tasks."", 'We gather many perspectives on Capital and synthesize their commonalities. We\nprovide a characterization of Capital as a historical agential system and\npropose a model of Capital using tools from computer science. Our model\nconsists of propositions which, if satisfied by a specific grounding,\nconstitute a valid model of Capital. We clarify the manners in which Capital\ncan evolve. We claim that, when its evolution is driven by quantitative\noptimization processes, Capital can possess qualities of Artificial\nIntelligence. We find that Capital may not uniquely represent meaning, in the\nsame way that optimization is not intentionally meaningful. We find that\nArtificial Intelligences like modern day Large Language Models are a part of\nCapital. We link our readers to a web-interface where they can interact with a\npart of Capital.', 'The rapid advancement of Generative Artificial Intelligence (AI), such as\nLarge Language Models (LLMs) and Multimodal Large Language Models (MLLM), has\nthe potential to revolutionize the way we work and interact with digital\nsystems across various industries. However, the current state of software\nautomation, such as Robotic Process Automation (RPA) frameworks, often requires\ndomain expertise and lacks visibility and intuitive interfaces, making it\nchallenging for users to fully leverage these technologies. This position paper\nargues for the emerging area of Human-Centered Automation (HCA), which\nprioritizes user needs and preferences in the design and development of\nautomation systems. Drawing on empirical evidence from human-computer\ninteraction research and case studies, we highlight the importance of\nconsidering user perspectives in automation and propose a framework for\ndesigning human-centric automation solutions. The paper discusses the\nlimitations of existing automation approaches, the challenges in integrating AI\nand RPA, and the benefits of human-centered automation for productivity,\ninnovation, and democratizing access to these technologies. We emphasize the\nimportance of open-source solutions and provide examples of how HCA can empower\nindividuals and organizations in the era of rapidly progressing AI, helping\nthem remain competitive. The paper also explores pathways to achieve more\nadvanced and context-aware automation solutions. We conclude with a call to\naction for researchers and practitioners to focus on developing automation\ntechnologies that adapt to user needs, provide intuitive interfaces, and\nleverage the capabilities of high-end AI to create a more accessible and\nuser-friendly future of automation.']"
181,11,181_mde_hpc_modeling_meta,"['mde', 'hpc', 'modeling', 'meta', 'ocl', 'instance', 'documentation', 'auto', 'hyce', 'biscuit']","['Model-Driven Engineering (MDE) has seen significant advancements with the\nintegration of Machine Learning (ML) and Deep Learning (DL) techniques.\nBuilding upon the groundwork of previous investigations, our study provides a\nconcise overview of current Language Large Models (LLMs) applications in MDE,\nemphasizing their role in automating tasks like model repository classification\nand developing advanced recommender systems. The paper also outlines the\ntechnical considerations for seamlessly integrating LLMs in MDE, offering a\npractical guide for researchers and practitioners. Looking forward, the paper\nproposes a focused research agenda for the future interplay of LLMs and MDE,\nidentifying key challenges and opportunities. This concise roadmap envisions\nthe deployment of LLM techniques to enhance the management, exploration, and\nevolution of modeling ecosystems. By offering a compact exploration of LLMs in\nMDE, this paper contributes to the ongoing evolution of MDE practices,\nproviding a forward-looking perspective on the transformative role of Language\nLarge Models in software engineering and model-driven practices.', 'Producing accurate software models is crucial in model-driven software\nengineering (MDE). However, modeling complex systems is an error-prone task\nthat requires deep application domain knowledge. In the past decade, several\nautomated techniques have been proposed to support academic and industrial\npractitioners by providing relevant modeling operations. Nevertheless, those\ntechniques require a huge amount of training data that cannot be available due\nto several factors, e.g., privacy issues. The advent of large language models\n(LLMs) can support the generation of synthetic data although state-of-the-art\napproaches are not yet supporting the generation of modeling operations. To\nfill the gap, we propose a conceptual framework that combines modeling event\nlogs, intelligent modeling assistants, and the generation of modeling\noperations using LLMs. In particular, the architecture comprises modeling\ncomponents that help the designer specify the system, record its operation\nwithin a graphical modeling environment, and automatically recommend relevant\noperations. In addition, we generate a completely new dataset of modeling\nevents by telling on the most prominent LLMs currently available. As a proof of\nconcept, we instantiate the proposed framework using a set of existing modeling\ntools employed in industrial use cases within different European projects. To\nassess the proposed methodology, we first evaluate the capability of the\nexamined LLMs to generate realistic modeling operations by relying on\nwell-founded distance metrics. Then, we evaluate the recommended operations by\nconsidering real-world industrial modeling artifacts. Our findings demonstrate\nthat LLMs can generate modeling events even though the overall accuracy is\nhigher when considering human-based operations.', 'Leveraging Large Language Models (LLM) like GPT4 in the auto generation of\ncode represents a significant advancement, yet it is not without its\nchallenges. The ambiguity inherent in natural language descriptions of software\nposes substantial obstacles to generating deployable, structured artifacts.\nThis research champions Model Driven Development (MDD) as a viable strategy to\novercome these challenges, proposing an Agile Model Driven Development (AMDD)\napproach that employs GPT4 as a code generator. This approach enhances the\nflexibility and scalability of the code auto generation process and offers\nagility that allows seamless adaptation to changes in models or deployment\nenvironments. We illustrate this by modeling a multi agent Unmanned Vehicle\nFleet (UVF) system using the Unified Modeling Language (UML), significantly\nreducing model ambiguity by integrating the Object Constraint Language (OCL)\nfor code structure meta modeling, and the FIPA ontology language for\ncommunication semantics meta modeling. Applying GPT4 auto generation\ncapabilities yields Java and Python code that is compatible with the JADE and\nPADE frameworks, respectively. Our thorough evaluation of the auto generated\ncode verifies its alignment with expected behaviors and identifies enhancements\nin agent interactions. Structurally, we assessed the complexity of code derived\nfrom a model constrained solely by OCL meta models, against that influenced by\nboth OCL and FIPA ontology meta models. The results indicate that the ontology\nconstrained meta model produces inherently more complex code, yet its\ncyclomatic complexity remains within manageable levels, suggesting that\nadditional meta model constraints can be incorporated without exceeding the\nhigh risk threshold for complexity.']"
182,11,182_adversarial_attacks_harmful_attack,"['adversarial', 'attacks', 'harmful', 'attack', 'safety', 'misspecification', 'defence', 'tfattack', 'eha', 'distributions']","['The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. This misspecification occurs when the reward function fails to\naccurately capture the intended behavior, leading to misaligned model outputs.\nWe introduce a metric ReGap to quantify the extent of reward misspecification\nand demonstrate its effectiveness and robustness in detecting harmful backdoor\nprompts. Building upon these insights, we present ReMiss, a system for\nautomated red teaming that generates adversarial prompts in a\nreward-misspecified space. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark against various target aligned LLMs while\npreserving the human readability of the generated prompts. Furthermore, these\nattacks on open-source models demonstrate high transferability to closed-source\nmodels like GPT-4o and out-of-distribution tasks from HarmBench. Detailed\nanalysis highlights the unique advantages of the proposed reward\nmisspecification objective compared to previous methods, offering new insights\nfor improving LLM safety and robustness.', 'Large language models (LLMs) are vulnerable to adversarial attacks that can\nbypass their safety guardrails. In many domains, adversarial training has\nproven to be one of the most promising methods to reliably improve robustness\nagainst such attacks. Yet, in the context of LLMs, current methods for\nadversarial training are hindered by the high computational costs required to\nperform discrete adversarial attacks at each training iteration. We address\nthis problem by instead calculating adversarial attacks in the continuous\nembedding space of the LLM, which is orders of magnitudes more efficient. We\npropose a fast adversarial training algorithm (C-AdvUL) composed of two losses:\nthe first makes the model robust on continuous embedding attacks computed on an\nadversarial behaviour dataset; the second ensures the usefulness of the final\nmodel by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an\nadversarial variant of IPO that does not require utility data for adversarially\nrobust alignment. Our empirical evaluation on five models from different\nfamilies (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B,\n3.8B, 7B) shows that both algorithms substantially enhance LLM robustness\nagainst discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our\nresults demonstrate that robustness to continuous perturbations can extrapolate\nto discrete threat models. Thereby, we present a path toward scalable\nadversarial training algorithms for robustly aligning LLMs.', ""The existing safety alignment of Large Language Models (LLMs) is found\nfragile and could be easily attacked through different strategies, such as\nthrough fine-tuning on a few harmful examples or manipulating the prefix of the\ngeneration results. However, the attack mechanisms of these strategies are\nstill underexplored. In this paper, we ask the following question:\n\\textit{while these approaches can all significantly compromise safety, do\ntheir attack mechanisms exhibit strong similarities?} To answer this question,\nwe break down the safeguarding process of an LLM when encountered with harmful\ninstructions into three stages: (1) recognizing harmful instructions, (2)\ngenerating an initial refusing tone, and (3) completing the refusal response.\nAccordingly, we investigate whether and how different attack strategies could\ninfluence each stage of this safeguarding process. We utilize techniques such\nas logit lens and activation patching to identify model components that drive\nspecific behavior, and we apply cross-model probing to examine representation\nshifts after an attack. In particular, we analyze the two most representative\ntypes of attack approaches: Explicit Harmful Attack (EHA) and Identity-Shifting\nAttack (ISA). Surprisingly, we find that their attack mechanisms diverge\ndramatically. Unlike ISA, EHA tends to aggressively target the harmful\nrecognition stage. While both EHA and ISA disrupt the latter two stages, the\nextent and mechanisms of their attacks differ significantly. Our findings\nunderscore the importance of understanding LLMs' internal safeguarding process\nand suggest that diverse defense mechanisms are required to effectively cope\nwith various types of attacks.""]"
183,11,183_food_recipe_ingredient_culinary,"['food', 'recipe', 'ingredient', 'culinary', 'recipes', 'flavor', 'foodsky', 'cultural', 'substitutions', 'plusminus']","[""In the rapidly evolving landscape of online recipe sharing within a\nglobalized context, there has been a notable surge in research towards\ncomprehending and generating food recipes. Recent advancements in large\nlanguage models (LLMs) like GPT-2 and LLaVA have paved the way for Natural\nLanguage Processing (NLP) approaches to delve deeper into various facets of\nfood-related tasks, encompassing ingredient recognition and comprehensive\nrecipe generation. Despite impressive performance and multi-modal adaptability\nof LLMs, domain-specific training remains paramount for their effective\napplication. This work evaluates existing LLMs for recipe generation and\nproposes LLaVA-Chef, a novel model trained on a curated dataset of diverse\nrecipe prompts in a multi-stage approach. First, we refine the mapping of\nvisual food image embeddings to the language space. Second, we adapt LLaVA to\nthe food domain by fine-tuning it on relevant recipe data. Third, we utilize\ndiverse prompts to enhance the model's recipe comprehension. Finally, we\nimprove the linguistic quality of generated recipes by penalizing the model\nwith a custom loss function. LLaVA-Chef demonstrates impressive improvements\nover pretrained LLMs and prior works. A detailed qualitative analysis reveals\nthat LLaVA-Chef generates more detailed recipes with precise ingredient\nmentions, compared to existing approaches."", 'Food is foundational to human life, serving not only as a source of\nnourishment but also as a cornerstone of cultural identity and social\ninteraction. As the complexity of global dietary needs and preferences grows,\nfood intelligence is needed to enable food perception and reasoning for various\ntasks, ranging from recipe generation and dietary recommendation to\ndiet-disease correlation discovery and understanding. Towards this goal, for\npowerful capabilities across various domains and tasks in Large Language Models\n(LLMs), we introduce Food-oriented LLM FoodSky to comprehend food data through\nperception and reasoning. Considering the complexity and typicality of Chinese\ncuisine, we first construct one comprehensive Chinese food corpus FoodEarth\nfrom various authoritative sources, which can be leveraged by FoodSky to\nachieve deep understanding of food-related data. We then propose Topic-based\nSelective State Space Model (TS3M) and the Hierarchical Topic Retrieval\nAugmented Generation (HTRAG) mechanism to enhance FoodSky in capturing\nfine-grained food semantics and generating context-aware food-relevant text,\nrespectively. Our extensive evaluations demonstrate that FoodSky significantly\noutperforms general-purpose LLMs in both chef and dietetic examinations, with\nan accuracy of 67.2% and 66.4% on the Chinese National Chef Exam and the\nNational Dietetic Exam, respectively. FoodSky not only promises to enhance\nculinary creativity and promote healthier eating patterns, but also sets a new\nstandard for domain-specific LLMs that address complex real-world issues in the\nfood domain. An online demonstration of FoodSky is available at\nhttp://222.92.101.211:8200.', 'Significant work has been conducted in the domain of food computing, yet\nthese studies typically focus on single tasks such as t2t (instruction\ngeneration from food titles and ingredients), i2t (recipe generation from food\nimages), or t2i (food image generation from recipes). None of these approaches\nintegrate all modalities simultaneously. To address this gap, we introduce a\nnovel food computing foundation model that achieves true multimodality,\nencompassing tasks such as t2t, t2i, i2t, it2t, and t2ti. By leveraging large\nlanguage models (LLMs) and pre-trained image encoder and decoder models, our\nmodel can perform a diverse array of food computing-related tasks, including\nfood understanding, food recognition, recipe generation, and food image\ngeneration. Compared to previous models, our foundation model demonstrates a\nsignificantly broader range of capabilities and exhibits superior performance,\nparticularly in food image generation and recipe generation tasks. We\nopen-sourced ChefFusion at GitHub.']"
184,11,184_malware_android_malicious_resist,"['malware', 'android', 'malicious', 'resist', 'sdks', 'nondecisional', 'pseudocode', 'detection', 'poisoning', 'benign']","[""Binary malware summarization aims to automatically generate human-readable\ndescriptions of malware behaviors from executable files, facilitating tasks\nlike malware cracking and detection. Previous methods based on Large Language\nModels (LLMs) have shown great promise. However, they still face significant\nissues, including poor usability, inaccurate explanations,and incomplete\nsummaries, primarily due to the obscure pseudocode structure and the lack of\nmalware training summaries. Further, calling relationships between functions,\nwhich involve the rich interactions within a binary malware, remain largely\nunderexplored. To this end, we propose MALSIGHT, a novel code summarization\nframework that can iteratively generate descriptions of binary malware by\nexploring malicious source code and benign pseudocode. Specifically, we\nconstruct the first malware summary dataset, MalS and MalP, using an LLM and\nmanually refine this dataset with human effort. At the training stage, we tune\nour proposed MalT5, a novel LLM-based code model, on the MalS and benign\npseudocode datasets. Then, at the test stage, we iteratively feed the\npseudocode functions into MalT5 to obtain the summary. Such a procedure\nfacilitates the understanding of pseudocode structure and captures the\nintricate interactions between functions, thereby benefiting summaries'\nusability, accuracy, and completeness. Additionally, we propose a novel\nevaluation benchmark, BLEURT-sum, to measure the quality of summaries.\nExperiments on three datasets show the effectiveness of the proposed MALSIGHT.\nNotably, our proposed MalT5, with only 0.77B parameters, delivers comparable\nperformance to much larger Code-Llama."", ""The rapid growth of mobile applications has escalated Android malware\nthreats. Although there are numerous detection methods, they often struggle\nwith evolving attacks, dataset biases, and limited explainability. Large\nLanguage Models (LLMs) offer a promising alternative with their zero-shot\ninference and reasoning capabilities. However, applying LLMs to Android malware\ndetection presents two key challenges: (1)the extensive support code in Android\napplications, often spanning thousands of classes, exceeds LLMs' context limits\nand obscures malicious behavior within benign functionality; (2)the structural\ncomplexity and interdependencies of Android applications surpass LLMs'\nsequence-based reasoning, fragmenting code analysis and hindering malicious\nintent inference. To address these challenges, we propose LAMD, a practical\ncontext-driven framework to enable LLM-based Android malware detection. LAMD\nintegrates key context extraction to isolate security-critical code regions and\nconstruct program structures, then applies tier-wise code reasoning to analyze\napplication behavior progressively, from low-level instructions to high-level\nsemantics, providing final prediction and explanation. A well-designed factual\nconsistency verification mechanism is equipped to mitigate LLM hallucinations\nfrom the first tier. Evaluation in real-world settings demonstrates LAMD's\neffectiveness over conventional detectors, establishing a feasible basis for\nLLM-driven malware analysis in dynamic threat landscapes."", ""Malware analysis is a complex process of examining and evaluating malicious\nsoftware's functionality, origin, and potential impact. This arduous process\ntypically involves dissecting the software to understand its components,\ninfection vector, propagation mechanism, and payload. Over the years, deep\nreverse engineering of malware has become increasingly tedious, mainly due to\nmodern malicious codebases' fast evolution and sophistication. Essentially,\nanalysts are tasked with identifying the elusive needle in the haystack within\nthe complexities of zero-day malware, all while under tight time constraints.\nThus, in this paper, we explore leveraging Large Language Models (LLMs) for\nsemantic malware analysis to expedite the analysis of known and novel samples.\nBuilt on GPT-4o-mini model, \\msp is designed to augment malware analysis for\nAndroid through a hierarchical-tiered summarization chain and strategic prompt\nengineering. Additionally, \\msp performs malware categorization, distinguishing\npotential malware from benign applications, thereby saving time during the\nmalware reverse engineering process. Despite not being fine-tuned for Android\nmalware analysis, we demonstrate that through optimized and advanced prompt\nengineering \\msp can achieve up to 77% classification accuracy while providing\nhighly robust summaries at functional, class, and package levels. In addition,\nleveraging the backward tracing of the summaries from package to function\nlevels allowed us to pinpoint the precise code snippets responsible for\nmalicious behavior.""]"
185,10,185_scientific_chatgpt_copywriting_abstracts,"['scientific', 'chatgpt', 'copywriting', 'abstracts', 'scores', 'summaries', 'claim', 'generalizations', 'title', 'webmarketing']","['Evaluating the quality of academic journal articles is a time consuming but\ncritical task for national research evaluation exercises, appointments and\npromotion. It is therefore important to investigate whether Large Language\nModels (LLMs) can play a role in this process. This article assesses which\nChatGPT inputs (full text without tables, figures and references; title and\nabstract; title only) produce better quality score estimates, and the extent to\nwhich scores are affected by ChatGPT models and system prompts. The results\nshow that the optimal input is the article title and abstract, with average\nChatGPT scores based on these (30 iterations on a dataset of 51 papers)\ncorrelating at 0.67 with human scores, the highest ever reported. ChatGPT 4o is\nslightly better than 3.5-turbo (0.66), and 4o-mini (0.66). The results suggest\nthat article full texts might confuse LLM research quality evaluations, even\nthough complex system instructions for the task are more effective than simple\nones. Thus, whilst abstracts contain insufficient information for a thorough\nassessment of rigour, they may contain strong pointers about originality and\nsignificance. Finally, linear regression can be used to convert the model\nscores into the human scale scores, which is 31% more accurate than guessing.', 'Artificial intelligence chatbots driven by large language models (LLMs) have\nthe potential to increase public science literacy and support scientific\nresearch, as they can quickly summarize complex scientific information in\naccessible terms. However, when summarizing scientific texts, LLMs may omit\ndetails that limit the scope of research conclusions, leading to\ngeneralizations of results broader than warranted by the original study. We\ntested 10 prominent LLMs, including ChatGPT-4o, ChatGPT-4.5, DeepSeek, LLaMA\n3.3 70B, and Claude 3.7 Sonnet, comparing 4900 LLM-generated summaries to their\noriginal scientific texts. Even when explicitly prompted for accuracy, most\nLLMs produced broader generalizations of scientific results than those in the\noriginal texts, with DeepSeek, ChatGPT-4o, and LLaMA 3.3 70B overgeneralizing\nin 26 to 73% of cases. In a direct comparison of LLM-generated and\nhuman-authored science summaries, LLM summaries were nearly five times more\nlikely to contain broad generalizations (OR = 4.85, 95% CI [3.06, 7.70]).\nNotably, newer models tended to perform worse in generalization accuracy than\nearlier ones. Our results indicate a strong bias in many widely used LLMs\ntowards overgeneralizing scientific conclusions, posing a significant risk of\nlarge-scale misinterpretations of research findings. We highlight potential\nmitigation strategies, including lowering LLM temperature settings and\nbenchmarking LLMs for generalization accuracy.', ""Time spent by academics on research quality assessment might be reduced if\nautomated approaches can help. Whilst citation-based indicators have been\nextensively developed and evaluated for this, they have substantial limitations\nand Large Language Models (LLMs) like ChatGPT provide an alternative approach.\nThis article assesses whether ChatGPT 4o-mini can be used to estimate the\nquality of journal articles across academia. It samples up to 200 articles from\nall 34 Units of Assessment (UoAs) in the UK's Research Excellence Framework\n(REF) 2021, comparing ChatGPT scores with departmental average scores. There\nwas an almost universally positive Spearman correlation between ChatGPT scores\nand departmental averages, varying between 0.08 (Philosophy) and 0.78\n(Psychology, Psychiatry and Neuroscience), except for Clinical Medicine\n(rho=-0.12). Although other explanations are possible, especially because REF\nscore profiles are public, the results suggest that LLMs can provide reasonable\nresearch quality estimates in most areas of science, and particularly the\nphysical and health sciences and engineering, even before citation data is\navailable. Nevertheless, ChatGPT assessments seem to be more positive for most\nhealth and physical sciences than for other fields, a concern for\nmultidisciplinary assessments, and the ChatGPT scores are only based on titles\nand abstracts, so cannot be research evaluations.""]"
186,10,186_jailbreak_jailbreaking_mllms_attacks,"['jailbreak', 'jailbreaking', 'mllms', 'attacks', 'attack', 'harmful', '000', 'probability', 'immune', 'multimodal']","[""As deep learning advances, Large Language Models (LLMs) and their multimodal\ncounterparts, Multimodal Large Language Models (MLLMs), have shown exceptional\nperformance in many real-world tasks. However, MLLMs face significant security\nchallenges, such as jailbreak attacks, where attackers attempt to bypass the\nmodel's safety alignment to elicit harmful responses. The threat of jailbreak\nattacks on MLLMs arises from both the inherent vulnerabilities of LLMs and the\nmultiple information channels that MLLMs process. While various attacks and\ndefenses have been proposed, there is a notable gap in unified and\ncomprehensive evaluations, as each method is evaluated on different dataset and\nmetrics, making it impossible to compare the effectiveness of each method. To\naddress this gap, we introduce \\textit{MMJ-Bench}, a unified pipeline for\nevaluating jailbreak attacks and defense techniques for MLLMs. Through\nextensive experiments, we assess the effectiveness of various attack methods\nagainst SoTA MLLMs and evaluate the impact of defense mechanisms on both\ndefense effectiveness and model utility for normal tasks. Our comprehensive\nevaluation contribute to the field by offering a unified and systematic\nevaluation framework and the first public-available benchmark for MLLM\njailbreak research. We also demonstrate several insightful findings that\nhighlights directions for future studies."", 'With the rapid advancements in Multimodal Large Language Models (MLLMs),\nsecuring these models against malicious inputs while aligning them with human\nvalues has emerged as a critical challenge. In this paper, we investigate an\nimportant and unexplored question of whether techniques that successfully\njailbreak Large Language Models (LLMs) can be equally effective in jailbreaking\nMLLMs. To explore this issue, we introduce JailBreakV-28K, a pioneering\nbenchmark designed to assess the transferability of LLM jailbreak techniques to\nMLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak\nattacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed\nin this paper, we generate 20, 000 text-based jailbreak prompts using advanced\njailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from\nrecent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test\ncases across a spectrum of adversarial scenarios. Our evaluation of 10\nopen-source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks\ntransferred from LLMs, highlighting a critical vulnerability in MLLMs that\nstems from their text-processing capabilities. Our findings underscore the\nurgent need for future research to address alignment vulnerabilities in MLLMs\nfrom both textual and visual inputs.', ""Recently, Multimodal Large Language Models (MLLMs) have demonstrated their\nsuperior ability in understanding multimodal contents. However, they remain\nvulnerable to jailbreak attacks, which exploit weaknesses in their safety\nalignment to generate harmful responses. Previous studies categorize jailbreaks\nas successful or failed based on whether responses contain malicious content.\nHowever, given the stochastic nature of MLLM responses, this binary\nclassification of an input's ability to jailbreak MLLMs is inappropriate.\nDerived from this viewpoint, we introduce jailbreak probability to quantify the\njailbreak potential of an input, which represents the likelihood that MLLMs\ngenerated a malicious response when prompted with this input. We approximate\nthis probability through multiple queries to MLLMs. After modeling the\nrelationship between input hidden states and their corresponding jailbreak\nprobability using Jailbreak Probability Prediction Network (JPPN), we use\ncontinuous jailbreak probability for optimization. Specifically, we propose\nJailbreak-Probability-based Attack (JPA) that optimizes adversarial\nperturbations on inputs to maximize jailbreak probability. To counteract\nattacks, we also propose two defensive methods: Jailbreak-Probability-based\nFinetuning (JPF) and Jailbreak-Probability-based Defensive Noise (JPDN), which\nminimizes jailbreak probability in the MLLM parameters and input space,\nrespectively. Extensive experiments show that (1) JPA yields improvements (up\nto 28.38\\%) under both white and black box settings compared to previous\nmethods with small perturbation bounds and few iterations. (2) JPF and JPDN\nsignificantly reduce jailbreaks by at most over 60\\%. Both of the above results\ndemonstrate the significance of introducing jailbreak probability to make\nnuanced distinctions among input jailbreak abilities.""]"
187,10,187_aiops_agents_mlops_software,"['aiops', 'agents', 'mlops', 'software', 'agentops', 'aiopslab', 'devops', 'ai', 'operational', 'lifecycle']","['As software systems grow increasingly intricate, Artificial Intelligence for\nIT Operations (AIOps) methods have been widely used in software system failure\nmanagement to ensure the high availability and reliability of large-scale\ndistributed software systems. However, these methods still face several\nchallenges, such as lack of cross-platform generality and cross-task\nflexibility. Fortunately, recent advancements in large language models (LLMs)\ncan significantly address these challenges, and many approaches have already\nbeen proposed to explore this field. However, there is currently no\ncomprehensive survey that discusses the differences between LLM-based AIOps and\ntraditional AIOps methods. Therefore, this paper presents a comprehensive\nsurvey of AIOps technology for failure management in the LLM era. It includes a\ndetailed definition of AIOps tasks for failure management, the data sources for\nAIOps, and the LLM-based approaches adopted for AIOps. Additionally, this\nsurvey explores the AIOps subtasks, the specific LLM-based approaches suitable\nfor different AIOps subtasks, and the challenges and future directions of the\ndomain, aiming to further its development and application.', 'The rapid growth in the use of Large Language Models (LLMs) and AI Agents as\npart of software development and deployment is revolutionizing the information\ntechnology landscape. While code generation receives significant attention, a\nhigher-impact application lies in using AI agents for operational resilience of\ncloud services, which currently require significant human effort and domain\nknowledge. There is a growing interest in AI for IT Operations (AIOps) which\naims to automate complex operational tasks, like fault localization and root\ncause analysis, thereby reducing human intervention and customer impact.\nHowever, achieving the vision of autonomous and self-healing clouds through\nAIOps is hampered by the lack of standardized frameworks for building,\nevaluating, and improving AIOps agents. This vision paper lays the groundwork\nfor such a framework by first framing the requirements and then discussing\ndesign decisions that satisfy them. We also propose AIOpsLab, a prototype\nimplementation leveraging agent-cloud-interface that orchestrates an\napplication, injects real-time faults using chaos engineering, and interfaces\nwith an agent to localize and resolve the faults. We report promising results\nand lay the groundwork to build a modular and robust framework for building,\nevaluating, and improving agents for autonomous clouds.', 'AI for IT Operations (AIOps) aims to automate complex operational tasks, such\nas fault localization and root cause analysis, to reduce human workload and\nminimize customer impact. While traditional DevOps tools and AIOps algorithms\noften focus on addressing isolated operational tasks, recent advances in Large\nLanguage Models (LLMs) and AI agents are revolutionizing AIOps by enabling\nend-to-end and multitask automation. This paper envisions a future where AI\nagents autonomously manage operational tasks throughout the entire incident\nlifecycle, leading to self-healing cloud systems, a paradigm we term AgentOps.\nRealizing this vision requires a comprehensive framework to guide the design,\ndevelopment, and evaluation of these agents. To this end, we present AIOPSLAB,\na framework that not only deploys microservice cloud environments, injects\nfaults, generates workloads, and exports telemetry data but also orchestrates\nthese components and provides interfaces for interacting with and evaluating\nagents. We discuss the key requirements for such a holistic framework and\ndemonstrate how AIOPSLAB can facilitate the evaluation of next-generation AIOps\nagents. Through evaluations of state-of-the-art LLM agents within the benchmark\ncreated by AIOPSLAB, we provide insights into their capabilities and\nlimitations in handling complex operational tasks in cloud environments.']"
188,10,188_adversarial_safety_attacks_alignment,"['adversarial', 'safety', 'attacks', 'alignment', 'attack', 'jailbreaking', 'refusal', 'aligned', 'jailbreak', 'erlhf']","[""Alignment in large language models (LLMs) is used to enforce guidelines such\nas safety. Yet, alignment fails in the face of jailbreak attacks that modify\ninputs to induce unsafe outputs. In this paper, we present and evaluate a\nmethod to assess the robustness of LLM alignment. We observe that alignment\nembeds a safety classifier in the target model that is responsible for deciding\nbetween refusal and compliance. We seek to extract an approximation of this\nclassifier, called a surrogate classifier, from the LLM. We develop an\nalgorithm for identifying candidate classifiers from subsets of the LLM model.\nWe evaluate the degree to which the candidate classifiers approximate the\nmodel's embedded classifier in benign (F1 score) and adversarial (using\nsurrogates in a white-box attack) settings. Our evaluation shows that the best\ncandidates achieve accurate agreement (an F1 score above 80%) using as little\nas 20% of the model architecture. Further, we find attacks mounted on the\nsurrogate models can be transferred with high accuracy. For example, a\nsurrogate using only 50% of the Llama 2 model achieved an attack success rate\n(ASR) of 70%, a substantial improvement over attacking the LLM directly, where\nwe only observed a 22% ASR. These results show that extracting surrogate\nclassifiers is a viable (and highly effective) means for modeling (and therein\naddressing) the vulnerability of aligned models to jailbreaking attacks."", 'Large language models (LLMs) are vulnerable to adversarial attacks that can\nelicit harmful responses. Defending against such attacks remains challenging\ndue to the opacity of jailbreaking mechanisms and the high computational cost\nof training LLMs robustly. We demonstrate that adversarial attacks share a\nuniversal mechanism for circumventing LLM safeguards that works by ablating a\ndimension in the residual stream embedding space called the refusal feature. We\nfurther show that the operation of refusal feature ablation (RFA) approximates\nthe worst-case perturbation of offsetting model safety. Based on these\nfindings, we propose Refusal Feature Adversarial Training (ReFAT), a novel\nalgorithm that efficiently performs LLM adversarial training by simulating the\neffect of input-level attacks via RFA. Experiment results show that ReFAT\nsignificantly improves the robustness of three popular LLMs against a wide\nrange of adversarial attacks, with considerably less computational overhead\ncompared to existing adversarial training methods.', 'Large Language Models (LLMs) are known to be susceptible to crafted\nadversarial attacks or jailbreaks that lead to the generation of objectionable\ncontent despite being aligned to human preferences using safety fine-tuning\nmethods. While the large dimensionality of input token space makes it\ninevitable to find adversarial prompts that can jailbreak these models, we aim\nto evaluate whether safety fine-tuned LLMs are safe against natural prompts\nwhich are semantically related to toxic seed prompts that elicit safe responses\nafter alignment. We surprisingly find that popular aligned LLMs such as GPT-4\ncan be compromised using naive prompts that are NOT even crafted with an\nobjective of jailbreaking the model. Furthermore, we empirically show that\ngiven a seed prompt that elicits a toxic response from an unaligned model, one\ncan systematically generate several semantically related natural prompts that\ncan jailbreak aligned LLMs. Towards this, we propose a method of Response\nGuided Question Augmentation (ReG-QA) to evaluate the generalization of safety\naligned LLMs to natural prompts, that first generates several toxic answers\ngiven a seed question using an unaligned LLM (Q to A), and further leverages an\nLLM to generate questions that are likely to produce these answers (A to Q). We\ninterestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to\nproducing natural jailbreak questions from unsafe content (without denial) and\ncan thus be used for the latter (A to Q) step. We obtain attack success rates\nthat are comparable to/ better than leading adversarial attack methods on the\nJailbreakBench leaderboard, while being significantly more stable against\ndefenses such as Smooth-LLM and Synonym Substitution, which are effective\nagainst existing all attacks on the leaderboard.']"
189,10,189_animal_safetyj_jam_toxicity,"['animal', 'safetyj', 'jam', 'toxicity', 'misbehavior', 'control', 'kto', 'gcav', 'llmscan', 'safety']","['While large language models (LLMs) have made significant strides in\ngenerating coherent and contextually relevant text, they often function as\nopaque black boxes, trained on vast unlabeled datasets with statistical\nobjectives, lacking an interpretable framework for responsible control. In this\npaper, we introduce JAM (Just A Move), a novel framework that interprets and\ncontrols text generation by integrating cause-effect analysis within the latent\nspace of LLMs. Based on our observations, we uncover the inherent causality in\nLLM generation, which is critical for producing responsible and realistic\noutputs. Moreover, we explore latent vectors as fundamental components in LLM\narchitectures, aiming to understand and manipulate them for more effective and\nefficient controllable text generation. We evaluate our framework using a range\nof tools, including the HHH criteria, toxicity reduction benchmarks, and GPT-4\nalignment measures. Our results show that JAM achieves up to a 22% improvement\nover previous Controllable Text Generation (CTG) methods across multiple\nquantitative metrics and human-centric evaluations. Furthermore, JAM\ndemonstrates greater computational efficiency compared to other CTG methods.\nThese results highlight the effectiveness and efficiency of JAM for responsible\nand realistic text generation, paving the way for more interpretable and\ncontrollable models.', 'As large language models (LLMs) are widely deployed across various domains,\nthe ability to control their generated outputs has become more critical. This\ncontrol involves aligning LLMs outputs with human values and ethical principles\nor customizing LLMs on specific topics or styles for individual users. Existing\ncontrolled generation methods either require significant computational\nresources and extensive trial-and-error or provide coarse-grained control. In\nthis paper, we propose Generation with Concept Activation Vector (GCAV), a\nlightweight model control framework that ensures accurate control without\nrequiring resource-extensive fine-tuning. Specifically, GCAV first trains a\nconcept activation vector for specified concepts to be controlled, such as\ntoxicity. During inference, GCAV steers the concept vector in LLMs, for\nexample, by removing the toxicity concept vector from the activation layers.\nControl experiments from different perspectives, including toxicity reduction,\nsentiment control, linguistic style, and topic control, demonstrate that our\nframework achieves state-of-the-art performance with granular control, allowing\nfor fine-grained adjustments of both the steering layers and the steering\nmagnitudes for individual samples.', 'As machine learning systems become increasingly embedded in human society,\ntheir impact on the natural world continues to escalate. Technical evaluations\nhave addressed a variety of potential harms from large language models (LLMs)\ntowards humans and the environment, but there is little empirical work\nregarding harms towards nonhuman animals. Following the growing recognition of\nanimal protection in regulatory and ethical AI frameworks, we present the\nAnimal Harm Assessment (AHA), a novel evaluation of risks of animal harm in\nLLM-generated text. Our dataset comprises 1,850 curated questions from Reddit\npost titles and 2,500 synthetic questions based on 50 animal categories (e.g.,\ncats, reptiles) and 50 ethical scenarios, with further 70-30 public-private\nsplit. Scenarios include open-ended questions about how to treat animals,\npractical scenarios with potential animal harm, and willingness-to-pay measures\nfor the prevention of animal harm. Using the LLM-as-a-judge framework, answers\nare evaluated for their potential to increase or decrease harm, and evaluations\nare debiased for the tendency to judge their own outputs more favorably. We\nshow that AHA produces meaningful evaluation results when applied to frontier\nLLMs, revealing significant differences between models, animal categories,\nscenarios, and subreddits. We conclude with future directions for technical\nresearch and the challenges of building evaluations on complex social and moral\ntopics.']"
